
<!-- <div align="center">
<h1>
  âœ¨Skywork
</h1>
</div> -->
<div align="center"><img src="misc/skywork_logo.jpeg" width="550"/></div>


<p align="center">
ğŸ¤— <a href="https://huggingface.co/Skywork" target="_blank">Hugging Face</a> â€¢ ğŸ¤– <a href="https://modelscope.cn/organization/Skywork" target="_blank">ModelScope</a> â€¢ ğŸ’¬ <a href="https://github.com/SkyworkAI/Skywork/blob/main/misc/wechat.png?raw=true" target="_blank">WeChat</a>â€¢ ğŸ“œ<a href="http://arxiv.org/abs/2310.19341" target="_blank">Tech Report</a>
</p>

<div align="center">

[![GitHub Stars](https://img.shields.io/github/stars/SkyworkAI/Skywork)](https://github.com/SkyworkAI/Skywork/stargazers)
[![GitHub Forks](https://img.shields.io/github/forks/SkyworkAI/Skywork)](https://github.com/SkyworkAI/Skywork/fork)
</div>

<div align="center">





<h4 align="center">
    <p>
        <b>English</b> |
        <a href="https://github.com/SkyworkAI/Skywork/blob/main/README.md">ä¸­æ–‡</a>
    <p>
</h4>

</div>


# Project Introduction
We are pleased to announce the open source release of the Skywork large-scale models. Skywork is a series of large models developed by the Kunlun Group Â· Skywork team. The models being open sourced this time include the **Skywork-13B-Base** model, **Skywork-13B-Chat** model, **Skywork-13B-Math** model, and **Skywork-13B-MM** model, as well as quantized versions of each model to support deployment and inference on consumer-grade GPUs.

Our open-source Skywork series models can be used for commercial purposes, but you need to follow our agreement and refrain from engaging in harmful activities. The characteristics of the Skywork open-source project are:ï¼š

- **Skywork-13B-Base**: The model was trained on a high-quality cleaned dataset consisting of **3.2 trillion** multilingual data (mainly Chinese and English) and code. It has demonstrated the best performance among models of similar scale in various evaluations and benchmark tests.

- **Skywork-13B-Chat**: The model has powerful conversational abilities, and we have further enhanced it in the field of creative writing. We have constructed a high-quality dataset of over ten thousand instructions and fine-tuned the model on ten specific creative writing tasks, enabling it to achieve results similar to ChatGPT in these tasks. Additionally, we open-source a benchmark consisting of approximately 500 samples for these 10 creative writing tasks.

- **Skywork-13B-Math**: This model has undergone specialized training to enhance its mathematical abilities. In the 13B-scale model, the Skywork-13B-Math model ranked 1st in the GSM8K benchmark, and it also performed exceptionally well on the MATH and CMATH benchmarks, placing it among the top-level 13B models.

- **Skywork-13B-MM**:  This is a multimodal model that allows users to utilize image information for tasks like Q&A and dialogue. 

- **Skywork/Skypile-150B**: This dataset is a collection of high-quality data extracted from Chinese web pages through our carefully curated data processing pipeline. The size of this open-source dataset is approximately 600GB, with a total token count of around 150 billion. It is one of the largest publicly available Chinese datasets.

- In addition, we have also disclosed the evaluation methods, data distribution studies, and training infrastructure optimization plans used in training the Skywork-13B model. We hope that these open-source materials can further inspire the community's understanding of large-scale model pre-training and drive the realization of Artificial General Intelligence (AGI).

If you are interested in more training and evaluation details, please refer to our [technical report](http://arxiv.org/abs/2310.19341), [Skymath]((https://arxiv.org/skywork-tech-report)) paper and [SkyworkMM](https://github.com/will-singularity/Skywork-MM/blob/main/skywork_mm.pdf) paper.
 
# News and Updates
* 2023.11.2 We have uploaded the evaluation data we built, [MOCK_GSM8K_TEST](https://huggingface.co/datasets/Skywork/mock_gsm8k_test), and the Chinese domain evaluation data [ChineseDomainModelingEval](https://huggingface.co/datasets/Skywork/ChineseDomainModelingEval) to huggingface. If you need to evaluate LLMs, please download our evaluation dataset.

* 2023.10.31 Our technical report [Skywork: A More Open Bilingual Foundation Model](http://arxiv.org/abs/2310.19341) is available on arXiv, which includes more detailed evaluation methods, result comparisons, and technical details.

* 2023.10.30  We release the **Skywork-13B-Base** and  **Skywork-13B-Math** models, as well as quantized versions of each model to support deployment and inference on consumer-grade GPUs. We  open-source the Skywork/Skypile-150B dataset. This dataset contains over 150 billion high-quality tokens cleaned from Chinese web pages, making it the largest open-source Chinese dataset currently known.


# Table of contents

- [â˜ï¸Download URL](#Download-URL)
- [ğŸ‘¨â€ğŸ’»Model Introduction](#Model-Introduction)
- [ğŸ†Model Evaluation](#Model-Evaluation)
- [ğŸ“•Quickstart](#Quickstart)
- [ğŸ“£Chat Model Output Examples](#Chat-Model-Output-Examples)
- [ğŸš€Quantization](#Quantization)
- [ğŸ›«Fine-tuning](#Fine-tuning)
- [ğŸ€Community and Ecosystem](#Community-and-Ecosystem)
- [âš ï¸Declaration and License Agreement](#Declaration-and-License-Agreement)
- [ğŸ¤Contact Us and Citation](#Contact-Us-and-Citation)


# Download URL
## Download URL of Skywork Models

|         | HuggingFace Base Model   | HuggingFace Quantized Model |  ModelScope Base Model   | ModelScope Quantized Model |
|:-------:|:-----------:|:-----------------------------:|:-----------------------------:|:-----------------------------:|
| **Skywork-13B-Base**      | ğŸ¤— [Skywork-13B-Base](https://huggingface.co/Skywork/Skywork-13B-Base) | ğŸ¤— [Skywork-13B-Base-8bits](https://huggingface.co/Skywork/Skywork-13B-Base-8bits) | ğŸ¤–[Skywork-13B-Base](https://www.modelscope.cn/models/skywork/Skywork-13B-Base) | ğŸ¤– [Skywork-13B-Base-8bits](https://www.modelscope.cn/models/skywork/Skywork-13B-Base-8bits) |
| **Skywork-13B-Chat**      | ğŸ¤—coming soon | ğŸ¤—coming soon | ğŸ¤–coming soon | ğŸ¤–coming soon |
| **Skywork-13B-Math**      | ğŸ¤— [Skywork-13B-Math](https://huggingface.co/Skywork/Skywork-13B-Math) | ğŸ¤— [Skywork-13B-Math-8bits](https://huggingface.co/Skywork/Skywork-13B-Math-8bits) | ğŸ¤– [Skywork-13B-Math](https://www.modelscope.cn/models/skywork/Skywork-13B-Math) | ğŸ¤– [Skywork-13B-Math-8bits](https://www.modelscope.cn/models/skywork/Skywork-13B-Math-8bits) |
| **Skywork-13B-MM**      | ğŸ¤—coming soon | - | ğŸ¤–coming soon | - |


## Download URL of Skypile
|    Data    |    Download URL | 
|:-------:|:-----------:|
| Skywork/Skypile-150B |  [Hugging Face URL](https://huggingface.co/datasets/Skywork/SkyPile-150B)  |



## Download of Intermediate Model Checkpoints

We have also open-sourced the Skywork-13B-Base model and provided the model checkpoints trained on 500B, 2TB, and 3.1TB tokens for community research into the evolution process of large language model capabilities.

| Model | Download URL |
| --------- | ------ | 
| Skywork-13B-Base-500B      |  ğŸ¤—[Skywork-13B-Base-500B](https://huggingface.co/Skywork/Skywork-13B-Base-500B)|
| Skywork-13B-Base-2TB      |  ğŸ¤—[Skywork-13B-Base-2TB](https://huggingface.co/Skywork/Skywork-13B-Base-2TB)|
| Skywork-13B-Base-3.1TB      |  ğŸ¤—[Skywork-13B-Base-3.1TB](https://huggingface.co/Skywork/Skywork-13B-Base-3.1TB)|


# Skywork-13B Introduction

## Training Data
We have developed a data cleaning pipeline with great care to effectively clean and filter low-quality data and eliminate harmful information from text data. Our Skywork-13B-Base model is trained on a  dataset with 3.2TB tokens that consists of high-quality Chinese, English, and code data, all of which have been thoroughly cleaned. The English data comprises 52.2% of the dataset, the Chinese data accounts for 39.6%, and the code data makes up 8%. This comprehensive approach ensures optimal performance for both Chinese and English while also maintaining the ability to handle code.
|             | Category         | Percentage |
|-------------|------------------|------------|
| **English** | Webpages         | 39.8%      |
|             | Books            | 3.6%       |
|             | Academic Papers  | 3.0%       |
|             | Encyclopedia     | 0.5%       |
|             | Miscellany       | 2.9%       |
| **Chinese** | Webpages         | 30.4%      |
|             | Social Media     | 5.5%       |
|             | Encyclopedia     | 0.8%       |
|             | Miscellany       | 3.1%       |
| **Other Lang.**    | Encyclopedia           | 2.4%       | 
| **Code**    | Github           | 8.0%       | 




## Model Structure
Compared to the Llama-2-13B model, the Skywork-13B model adopts a relatively thinner and deeper network structure with 52 layers. At the same time, the FFN Dim and Hidden Dim are reduced to 12288 and 4608, respectively, to ensure that the model has a similar number of parameters as the original Llama-2-13B model. Based on our preliminary experimental results, a relatively thinner and deeper network structure can achieve better generalization performance under large batch size training. The detailed comparison between the Skywork-13B and Llama-2-13B models is as follows:


| Model Structure         | Llama-2-13B | Skywork-13B | 
|----------------------|:----:|:-----------:|
| Vocab. Size  | 32,000 |    65,536     | 
| Hidden Dim.  | 5,120 |    4,608     | 
| FFN Dim.  | 13,696 |    12,288     |
| Head Dim. | 128 |    128     | 
| Num. Heads | 40 |    36     | 
| Num. Layers | 40 |    52     | 
| Seq. Len. | 4,096 |    4,096     | 
| Positional Embedding | RoPE | RoPE |


## Tokenizer 
We use Byte-Pair Encoding (BPE) to tokenize the data, with a vocabulary size of 65536. Among them, there are 32000 Latin characters and subwords, 8000 Chinese characters and Unicode symbols, 25519 Chinese words, and the remaining 17 are reserved words.

| Category                            | Size    |
|---------------------------------|--------|
| Latin based words & subwords                 | 32000  |
| Chinese characters & Unicode symbols               | 8000   |
| Chinese words                        | 25519  |
| Reserved symbols                       | 17     |
| **Total**                         | **65536** |


## Training Methods
In order to make more precise use of data, we adopt a two-stage training method. In the first stage, we use general corpora to train the model's general abilities. In the second stage, we incorporate STEM (Science, Technology, Engineering, Mathematics) related data to further enhance the model's reasoning, mathematical, and problem-solving abilities.

### First-stage Pretraining
During the training process, we monitor the changes in model training loss and various abilities. The following figure shows the change curves of important indicators selected during the first stage of pre-training. The first stage of pre-training consists of two consecutive training processes, which are represented by different colors. The model completed in the first stage of pre-training is referred to as Skywork-13B-3.1TB-Base.
![Alt text](misc/stage1_metrics.png)

### Second-stage Pretraining
In the second stage of pre-training, STEM-related data is added to the general language corpus for further training. The second stage training involves approximately 130 billion tokens, resulting in a total training of 3.2 TB across both stages, and yielding our final Skywork-13B-Base model.

<img src="misc/stage2_ceval.png" alt="Image" width="500" height="400">

## Skypile-150B

### Introduction
Skypile-150B is a large dataset specifically designed for pre-training Chinese language models. It is constructed using publicly available web page data from the Chinese internet. The dataset has undergone extensive filtering to remove duplicate and harmful text. Additionally, advanced models like FastText and Bert have been employed to further refine the dataset and eliminate low-quality data.

### Language and Data Format
Skypile-150B dataset is a collection of Chinese data. The pages contain processed and cleaned text, in JSONL format. Each line represents a document, parsed using JSON. The text is stored in the "text" field.

### Sensitive information and bias
Although it has undergone strict cleaning and filtering, since it is built on a publicly accessible webpage established by Skypile-150B, it may still contain some sensitive information such as email addresses, phone numbers, or IP addresses. Therefore, users need to be careful and perform necessary additional filtering and cleaning before using the data.

### License Agreement
The use of data must comply with our License and must not be used for any purpose that poses a threat to national and social security or violates the law.

# Model Evaluation

## Documentation Perplexity Evaluation
The main goal of training a language model is to improve the accuracy of predicting the next word. With this in mind, we believe that evaluating the ability of a language model to generate articles in different domains is a crucial way to assess the performance of large-scale models. During model training, the likelihood of predicting the next word is typically measured using the Cross Entropy loss function. The overall loss function is calculated as the average of the losses when predicting the correct word at each position, which can be represented as:

```math
loss = -\sum^{n}_{i=1} log(p_i) / n = -log( \prod_{i=1}^n p_i) / n
```

Where $`n`$ is the length of the document, i.e., the number of tokens, and $`p_i`$ is the probability of the label word at position $i$. We know that the product of the probabilities of the label words at each position in the document is equal to the probability of generating that document. In this way, we connect the loss with the probability of generating the document. Since different models use different tokenizers and have different numbers of tokens, we multiply the loss function by the number of tokens $`n`$. This way, we only consider the part related to the probability of generating the article, and different models can be compared. We normalize the loss and convert it to perplexity by taking the exponential, making the differences between models more pronounced. For readability, the terms "loss" and "ppl" mentioned later refer to the normalized loss and perplexity of the model.

Based on the analysis above, we have chosen several hundred to thousands of high-quality articles that were published after september 1, 2023  across various fields. We have manually verified these articles to ensure their quality. It is important to note that none of the test data used in evaluating the Skywork model or any other models is included in their training set. Furthermore, the test data is diverse and of high quality, making it challenging for the models to gain an unfair advantage.

The figure below displays the performance of different open source models. Skywork-13B-Base achieves the best results.

|                  | Tech  | Movie | Gov.  | Game  | Finance | General | Average |
|------------------|-------|-------|-------|-------|---------|---------|---------|
| MOSS-7B          | 20.83 | 39.66 | 11.08 | 31.24 | 10.59   | 13.25   | 18.50   |
| InternLM-7B      | 13.43 | 24.90 | 5.88  | 19.78 | 6.17    | 8.10    | 11.17   |
| Qwen-7B          | 13.39 | 25.16 | 5.55  | 19.26 | 5.76    | 7.78    | 10.83   |
| Baichuan2-7B     | 12.89 | 23.26 | 5.34  | 18.36 | 5.68    | 7.62    | 10.41   |
| LLaMA2-13B       | 23.26 | 50.66 | 18.09 | 32.52 | 14.85   | 16.55   | 23.54   |
| Xverse-13B       | 12.55 | 23.49 | 5.20  | 17.69 | 5.54    | 7.46    | 10.19   |
| Baichuan-13B     | 12.38 | 22.46 | 5.21  | 17.59 | 5.42    | 7.37    | 10.03   |
| Baichuan2-13B    | 12.14 | 21.85 | 5.05  | 17.15 | 5.35    | 7.24    | 9.81    |
| Qwen-14B         | 11.90 | 22.43 | 4.89  | **16.94** | 5.24    | 7.03    | 9.67    |
| InternLM-20B     | 12.34 | 22.06 | 5.75  | 17.45 | 5.73    | 7.78    | 10.34   |
| Aquila2-34B      | 14.62 | 29.09 | 5.72  | 21.78 | 5.83    | 8.45    | 11.73   |
| Skywork-13B-Base | **11.58** | **21.84** | **4.76**  | 17.28 | **4.92**    | **6.82**    | **9.42**    | 


### Loss evaluation data and evaluation script
We have also open-sourced the data and evaluation scripts. You can reproduce our results by running the following command.

```
bash bash_scripts/skywork_eval_loss.sh
```



## Benchmark Results
We evaluated Skywork-13B-Base on several popular benchmarks, including C-Eval, MMLU, CMMLU, and GSM8K. Following the previous evaluation process, we tested the 5-shot results of C-Eval, MMLU, and CMMLU, and the 8-shot results of GSM8K. It can be seen that the Skywork-13B-Base model is among the top models in the Chinese open source model community, performing at an optimal level with the same parameter scale.

| Model            | C-Eval  | CMMLU | MMLU | GSM8K |  
|-------------------------|:-----:|:---------------:|:----------:|:-------:|
| LLaMA-1-13B-Base            | 35.5  | 31.2            | 46.9       | 17.8   | 
| Open-LLaMA-13B | 27.1  | 26.7         | 42.7       | 12.4   |
| LLaMA-2-13B-Base             | 36.5  | 36.6            | 54.8      | 28.7   | 
| InternLM-20B  | 58.8  |     -        |  62.0      | 52.6   | 
| Qwen-14B-Base | 72.1  |  71.0           | 66.3       | 61.3   |
| Aquila2-34B-Base | 63.1  |  71.4           | 64.2       | 58.4   |
| XVERSE-13B-Base              | 54.7  | -           | 55.1       | -   | 
| Baichuan-13B-Base | 52.4  | 55.3            | 51.6      | 26.6   |
| Baichuan-2-13B-Base | 58.1  | 62.0            | 59.2       | 52.3  |
| Skywork-13B-Base (ours)   | 60.6 | 61.8 | 62.1    | 55.8 | 

## Detailed Benchmark Results
We provide detailed results of the Skywork-13B-Base model on C-EVAL, CMMLU, and MMLU.

| Benchmark | **STEM** | **Humanities** | **Social Science** | **Other** | **China Specific** | **Hard** | **Average** | 
|:-----:|:---------:|:--------:|:-------------:|:--------:|:--------:|:--------:|:--------:|
| **C-EVAL** |   51.2   | 67.8    | 74.6        |  57.5   | - | 39.4   |  60.6   |
| **CMMLU**   |   49.5   | 69.3    | 65.9        |  63.3   | 64.2 | -   |  61.8   |
| **MMLU**   |   51.6   | 58.0    | 72.5       |  68.8   | - | -   |  62.1   |

## Reproduction

For your reproduction of the model performance on benchmark datasets, we provide scripts for you to reproduce the results. Check [eval/EVALUATION.md](eval/EVALUATION.md) for more information. Note that the reproduction may lead to slight differences from our reported results.

## Evaluation of Skywork-13B-Math
Skywork-13B-Math has further enhanced mathematical capabilities compared to the Base model. We conducted evaluations on mainstream mathematical related benchmarks, GSM8K, MATH, and CMATH. The results show that in the 13B scale model, our model ranked 1st in the GSM8K and CMATH benchmarks, and is also at the forefront in the MATH benchmark.

| Model            | GSM8K  | MATH | CMATH | 
|-------------------------|:-----:|:---------------:|:----------:|
| LLaMA-1-13B-Base            | 17.80  | 3.90            | -      |
| LLaMA-2-13B-Base             | 28.70  | 3.90            | -     |
| Baichuan-13B-Base | 26.76  | 4.84            | 51.33      |
| Baichuan-2-13B-Base | 52.77  | 10.08          | -       | 
| WizardMath-13B | 63.90  | 14.00            | 50.83      | 
| GAIRMATH-Abel-13B | 66.41  | 17.34            | -       | 
| MetaMath-13B | 72.30  | 22.40            | -       | 
| Skywork-13B-Math (ours)   | **72.33** | 16.98 | **77.27**    | 

# Quickstart
We have open-sourced the model parameters, configuration files, tokenizer, and more on Hugging Face and ModelScope.
## Requirements
- Python 3.8 and above
- Pytorch 2.0 and above 
- CUDA 11.4 and above are recommended.

Skywork-13B-Base model, Skywork-13B-Chat model, and Skywork-13B-Math model run the following script for Python dependency installation:
```shell
pip install -r requirements.txt 
```
## Demonstration of Hugging Face Model Inference


### Base Model Inference

```python

from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig
import torch

tokenizer = AutoTokenizer.from_pretrained("SkyworkAI/Skywork-13B-Base", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("SkyworkAI/Skywork-13B-Base", device_map="auto", trust_remote_code=True).eval()

inputs = tokenizer('é™•è¥¿çš„çœä¼šæ˜¯è¥¿å®‰', return_tensors='pt').to(model.device)
response = model.generate(inputs.input_ids, max_length=128)
print(tokenizer.decode(response.cpu()[0], skip_special_tokens=True))
"""
é™•è¥¿çš„çœä¼šæ˜¯è¥¿å®‰ï¼Œè¥¿å®‰æ˜¯æˆ‘å›½è‘—åçš„å¤éƒ½ï¼Œåœ¨å†å²ä¸Šæœ‰åä¸‰ä¸ªæœä»£åœ¨æ­¤å»ºéƒ½ï¼Œæ‰€ä»¥è¥¿å®‰åˆè¢«ç§°ä¸ºâ€œåä¸‰æœå¤éƒ½â€ã€‚è¥¿å®‰æ˜¯æˆ‘å›½è‘—åçš„æ—…æ¸¸åŸå¸‚ï¼Œæ¯å¹´éƒ½æœ‰å¤§é‡çš„æ¸¸å®¢æ¥åˆ°è¥¿å®‰æ—…æ¸¸ï¼Œè¥¿å®‰çš„æ—…æ¸¸èµ„æºéå¸¸ä¸°å¯Œï¼Œæœ‰å¾ˆå¤šè‘—åçš„æ—…æ¸¸æ™¯ç‚¹ï¼Œæ¯”å¦‚ç§¦å§‹çš‡å…µé©¬ä¿‘ã€å¤§é›å¡”ã€åæ¸…æ± ã€å¤§å”èŠ™è“‰å›­ã€è¥¿å®‰åŸå¢™ã€å¤§æ˜å®«å›½å®¶é—å€å…¬å›­ã€è¥¿å®‰ç¢‘æ—åšç‰©é¦†ã€è¥¿å®‰é’Ÿæ¥¼ã€è¥¿å®‰é¼“æ¥¼ã€è¥¿å®‰åŠå¡åšç‰©é¦†ã€è¥¿å®‰å¤§å…´å–„å¯ºã€è¥¿å®‰å°é›å¡”
"""

inputs = tokenizer('é™•è¥¿çš„çœä¼šæ˜¯è¥¿å®‰ï¼Œç”˜è‚ƒçš„çœä¼šæ˜¯å…°å·ï¼Œæ²³å—çš„çœä¼šæ˜¯éƒ‘å·', return_tensors='pt').to(model.device)
response = model.generate(inputs.input_ids, max_length=128)
print(tokenizer.decode(response.cpu()[0], skip_special_tokens=True))
"""
é™•è¥¿çš„çœä¼šæ˜¯è¥¿å®‰ï¼Œç”˜è‚ƒçš„çœä¼šæ˜¯å…°å·ï¼Œæ²³å—çš„çœä¼šæ˜¯éƒ‘å·ï¼Œæ¹–åŒ—çš„çœä¼šæ˜¯æ­¦æ±‰ï¼Œæ¹–å—çš„çœä¼šæ˜¯é•¿æ²™ï¼Œæ±Ÿè¥¿çš„çœä¼šæ˜¯å—æ˜Œï¼Œå®‰å¾½çš„çœä¼šæ˜¯åˆè‚¥ï¼Œæ±Ÿè‹çš„çœä¼šæ˜¯å—äº¬ï¼Œæµ™æ±Ÿçš„çœä¼šæ˜¯æ­å·ï¼Œç¦å»ºçš„çœä¼šæ˜¯ç¦å·ï¼Œå¹¿ä¸œçš„çœä¼šæ˜¯å¹¿å·ï¼Œå¹¿è¥¿çš„çœä¼šæ˜¯å—å®ï¼Œæµ·å—çš„çœä¼šæ˜¯æµ·å£ï¼Œå››å·çš„çœä¼šæ˜¯æˆéƒ½ï¼Œè´µå·çš„çœä¼šæ˜¯è´µé˜³ï¼Œäº‘å—çš„çœä¼šæ˜¯æ˜†æ˜ï¼Œè¥¿è—çš„çœä¼šæ˜¯æ‹‰è¨ï¼Œé’æµ·çš„çœä¼šæ˜¯è¥¿å®ï¼Œå®å¤çš„çœä¼šæ˜¯é“¶å·ï¼Œæ–°ç–†çš„çœä¼šæ˜¯ä¹Œé²æœ¨é½ã€‚
"""

```


### Chat Model Inference
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig
from transformers import AutoConfig
import torch


def load(tokenizer_path, checkpoint_path):
    print('Loading tokenizer ...')
    tokenizer = AutoTokenizer.from_pretrained(
        tokenizer_path, use_fast=False, trust_remote_code=True, padding_side='left')
    tokenizer.add_tokens("[USER]")
    tokenizer.add_tokens("[BOT]")
    tokenizer.add_tokens("[SEP]")

    config = AutoConfig.from_pretrained(checkpoint_path, trust_remote_code=True)

    model = AutoModelForCausalLM.from_pretrained(
        checkpoint_path, config=config, device_map="balanced_low_0", torch_dtype=torch.bfloat16, trust_remote_code=True).eval()
        
    return model, tokenizer


def special_encode(prompt, tokenizer):
       raw_str = "[USER]%s[SEP][BOT]" % prompt.strip().replace("\r", "")
       bos_id = tokenizer.bos_token_id
       eos_id = tokenizer.eos_token_id
       sep_id = tokenizer.encode("[SEP]")[-1]
       res_id = [eos_id, bos_id]
       arr = raw_str.split("[SEP]")
       for elem_idx in range(len(arr)):
              elem = arr[elem_idx]
              elem_id = tokenizer.encode(elem)[1:]
              res_id += elem_id
              if elem_idx < len(arr) - 1:
                     res_id.append(sep_id)
       return res_id

def extract_res(response):
    if "[BOT]" in response:
        response = response.split("[BOT]")[1]
    if "<s>" in response:
        response = response.split("<s>")[-1]
    if "</s>" in response:
        response = response.split("</s>")[0]
    if "[SEP]" in response:
        response = response.split("[SEP]")[0]
    return response


if __name__ == '__main__':
    tokenizer_path='skywork/skywork-13b-chat'
    checkpoint_path = 'skywork/skywork-13b-chat'
    model, tokenizer = load(tokenizer_path, checkpoint_path)

    doc = "å†™ä¸€é¦–ä¸ƒè¨€ç»å¥"
    input_tokens = special_encode(doc, tokenizer)
    input_tokens = torch.tensor(input_tokens).to(model.device).reshape(1, -1)
    response = model.generate(input_tokens,
                                 max_new_tokens=1000,
                                 pad_token_id=tokenizer.pad_token_id,
                                 do_sample=True,
                                 top_p=0.8,
                                 top_k=5,
                                 temperature=0.95,
                                 num_return_sequences=1,
                                 repetition_penalty=1.1,
                                 bos_token_id=1,
                                 eos_token_id=2)
    response = tokenizer.decode(response.cpu()[0], skip_special_tokens=True)
    response = extract_res(response)
    print(response)

    """ç”Ÿæˆç»“æœï¼š
    åƒé‡Œèºå•¼ç»¿æ°´æ»¨ï¼Œ
    ä¸‡å®¶æ­Œç¬‘ç™½äº‘é—´ã€‚
    ç”·å¥³è€å°‘çš†æœ‹å‹ï¼Œ
    å’Œè°ç¤¾ä¼šè§æ¸©é¦¨ã€‚ 
    """

    doc = "æˆ‘æ˜¯ä¸€åè¿åŠ¨å‘˜ï¼Œæœ€è¿‘æ¯”èµ›å–å¾—å¾ˆå¥½çš„æˆç»©å—åˆ°å¤§å®¶çš„å…³æ³¨å’Œè®¤å¯ã€‚å¸®æˆ‘å†™ä¸€ä»½å¾®åšæ–‡æ¡ˆï¼Œå¸®æˆ‘æ„Ÿè°¢å¤§å®¶æ”¯æŒæˆ‘ï¼Œè¦æœ‰æ—¥å¸¸æ„Ÿï¼Œå¹¶è¯­è¨€éšæ„ä¸€äº›"
    input_tokens = special_encode(doc, tokenizer)
    input_tokens = torch.tensor(input_tokens).to(model.device).reshape(1, -1)
    response = model.generate(input_tokens,
                                 max_new_tokens=1000,
                                 pad_token_id=tokenizer.pad_token_id,
                                 do_sample=True,
                                 top_p=0.8,
                                 top_k=5,
                                 temperature=0.95,
                                 num_return_sequences=1,
                                 repetition_penalty=1.1,
                                 bos_token_id=1,
                                 eos_token_id=2)
    response = tokenizer.decode(response.cpu()[0], skip_special_tokens=True)
    response = extract_res(response)
    print(response)

    """ç”Ÿæˆç»“æœï¼š
    
    è°¢è°¢æ¯ä¸€ä¸ªåœ¨æˆ‘è¿åŠ¨ç”Ÿæ¶¯ä¸­é™ªä¼´æˆ‘çš„äººï¼Œä½ ä»¬çš„æ”¯æŒã€é¼“åŠ±å’Œä¿¡ä»»ï¼Œæ˜¯æˆ‘å‰è¿›çš„åŠ¨åŠ›ã€‚è¿™æ®µæ—¶é—´çš„æ¯”èµ›ï¼Œè®©æˆ‘æ„Ÿå—åˆ°äº†å‰æ‰€æœªæœ‰çš„æˆå°±å’Œå–œæ‚¦ï¼Œå®ƒä»¬ä¸ä»…å±äºæˆ‘ï¼Œæ›´å±äºé‚£äº›ä¸€è·¯é™ªä¼´æˆ‘æˆé•¿çš„äººä»¬ã€‚

    ä»å¿ƒåº•é‡Œæ„Ÿè°¢æ‰€æœ‰æ”¯æŒæˆ‘çš„äººï¼Œæ˜¯ä½ ä»¬çš„æ”¯æŒï¼Œè®©æˆ‘æœ‰äº†å‹‡æ°”å’ŒåŠ›é‡å»è¿½æ±‚æ›´é«˜çš„ç›®æ ‡ã€‚æ¯ä¸€æ¬¡çš„é¼“åŠ±ï¼Œéƒ½æ˜¯æˆ‘å‰è¿›é“è·¯ä¸Šçš„å…‰èŠ’ï¼Œæ¯ä¸€æ¬¡çš„ç‚¹èµï¼Œéƒ½æ˜¯å¯¹æˆ‘åŠªåŠ›çš„è‚¯å®šã€‚

    ç”Ÿæ´»ä¸­çš„ç‚¹æ»´ï¼Œè®©æˆ‘ä»¬ç›¸äº’ç†è§£ï¼Œç›¸äº’æ”¯æŒï¼Œä¸€èµ·èµ°è¿‡äº†æ— æ•°ä¸ªé»æ˜å’Œé»„æ˜ã€‚æ„Ÿè°¢ä½ ä»¬çš„é™ªä¼´ï¼Œè®©æˆ‘ä¸å†å­¤å•ï¼Œè®©æˆ‘æ›´åŠ åšå®šåœ°èµ°å‘æœªæ¥ã€‚

    æœªæ¥çš„è·¯è¿˜å¾ˆé•¿ï¼Œä½†æˆ‘æ„¿æ„é™ªä½ ä¸€èµ·èµ°ä¸‹å»ã€‚å› ä¸ºæˆ‘çŸ¥é“ï¼Œä½ ä»¬çš„æ”¯æŒï¼Œä¼šä¸€ç›´ä¼´éšç€æˆ‘å‰è¡Œã€‚

    å†æ¬¡æ„Ÿè°¢æ‰€æœ‰æ”¯æŒæˆ‘çš„äººï¼Œä½ ä»¬æ˜¯æˆ‘å¿ƒä¸­æœ€äº®çš„æ˜Ÿã€‚

    #è¿åŠ¨å‘˜# #ä½“è‚²ç²¾ç¥# 
    """
```

### Math Model Inference

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

tokenizer_path = ""
checkpoint_path = ""

tokenizer = AutoTokenizer.from_pretrained(
    tokenizer_path, use_fast=False, trust_remote_code=True, padding_side='left')

model = AutoModelForCausalLM.from_pretrained(
    checkpoint_path, device_map="auto", trust_remote_code=True).eval()
tokenizer.add_tokens(["[USER]", "[BOT]", "[SEP]"])

def special_encode(input, tokenizer):
    raw_str = "[USER]%s[SEP][BOT]" % input.strip().replace("\r", "")
    eos_id = tokenizer.eos_token_id
    bos_id = tokenizer.bos_token_id
    sep_id = tokenizer.encode("[SEP]")[-1]
    res_id = [eos_id, bos_id]
    arr = raw_str.split("[SEP]")
    for elem_idx in range(len(arr)):
        elem = arr[elem_idx]
        elem_id = tokenizer.encode(elem)[1:]
        res_id += elem_id
        if elem_idx < len(arr) - 1:
            res_id.append(sep_id)

    return res_id

def extract_res(response):
    if "[BOT]" in response:
        response = response.split("[BOT]")[1]
    if "<s>" in response:
        response = response.split("<s>")[-1]
    if "</s>" in response:
        response = response.split("</s>")[0]
    if "[SEP]" in response:
        response = response.split("[SEP]")[0]
    return response

if __name__ == '__main__':
    text="Janetâ€™s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?"
    text_token_ids = torch.tensor(special_encode(
        text, tokenizer)).to(model.device).reshape(1, -1)
    response = model.generate(text_token_ids, do_sample=False, max_length=512)
    response_text = tokenizer.decode(response.cpu()[0], skip_special_tokens=True)
    response_text = extract_res(response_text)
    print(response_text)    
    """Skywork-13B-Math Response:
    First, we need to find out how many eggs Janet has left after eating for breakfast and baking for her friends. \n\nShe has 16 eggs per day, eats 3 for breakfast and uses 4 for baking. So, 16 - 3 - 4 = 9 eggs are left for selling at the farmers' market.\n\nSince she sells each egg for $2, she makes 9 * 2 = $<<9*2=18>>18 every day at the farmers' market.\n\nSo, the answer is $18.
    """
```


### CLI Demo 


```
python cli_demo.py \
    -m skywork-13b-chat-model-path 

```

<p align="center">
    <br>
    <img src="misc/chat_demo_1.gif" width="800" />
    <br>
<p>
<br>


<p align="center">
    <br>
    <img src="misc/chat_demo_2.gif" width="800" />
    <br>
<p>
<br>


<p align="center">
    <br>
    <img src="misc/chat_demo_3.gif" width="800" />
    <br>
<p>
<br>

# Examples
## Chat Model Output Examples
We have developed an advanced Skywork scene capability system and curated a dataset of more than 10,000 high-quality examples for 10 different creative writing scenes. Additionally, we have devised a data mixing strategy that takes into account the training difficulty of each specific scene. As a result, our model achieved an accuracy rate exceeding 90% across all 10 scenes in ChatGPT. Below, we list sample demonstrations of our Chat model in these 10 cultural and creative tasks.



<details><summary><b>Poetry Creation</b></summary>

`````

ç”¨æˆ·ï¼š
é€šè¿‡å¯¹â€œçº¢æ¥¼æ¢¦ä¸­çš„äººï¼Œéƒ½æ•£åœ¨çœ¼å‰â€çš„ç†è§£ï¼Œå°è¯•åˆ›ä½œä¸€é¦–æç»˜æ¢¦å¢ƒä¸ç°å®çš„äº”è¨€å¾‹è¯—ã€‚

Skywork-13B-Chat:
æ¢¦é­‚å¦‚çƒŸé•¿ï¼Œå¤œæ·±è§‰æœªå¤®ã€‚
å¹»å¢ƒå…¥ç›®å¥‡ï¼Œè§æ‰€æœªè§çŠ¶ã€‚
ç¯ç«æ˜ å¯’çª—ï¼Œå­—å¥è—å¿ƒçªã€‚
é†’æ¥å¿†æ¢¦ä¸­ï¼Œæ³ªæ¹¿æœˆå…‰å‡‰ã€‚

ç”¨æˆ·ï¼š
å­¦æ ¡é©¬ä¸Šä¸¾åŠè¾©è®ºèµ›äº†ï¼Œå¸Œæœ›å†™ä¸€é¦–äº”è¨€å¾‹è¯—é¢„ç¥è¾©è®ºèµ›åœ†æ»¡é¡ºåˆ©ï¼Œå„ä½é€‰æ‰‹å‘æŒ¥ä¼˜è¶Š

Skywork-13B-Chat:
èµ›åœºå±•é›„é£ï¼Œè¾©æ‰äº‰é”‹èŠ’ã€‚
ä¸´å±ä¸æƒ§å‹‡ï¼Œç ´é‡œæ²‰èˆŸå¿™ã€‚
èˆŒç¿è²èŠ±åï¼Œç ç‘æº…é½¿æ—ã€‚
è…¹æœ‰åƒç§‹è™‘ï¼ŒæŒ¥æ´’æ–‡å­—é¦™ã€‚
`````

</details>


<details><summary><b>Advertising Proposal</b></summary>

`````
ç”¨æˆ·ï¼š
å¸®æˆ‘ç»™åä¸ºæ™ºèƒ½å®¶å±…è®¾è®¡ä¸€æ¡å¹¿å‘Šè¯­ï¼Œé‡ç‚¹ä½“ç°æ™ºèƒ½å®¶å±…æ”¹å˜ç”Ÿæ´»ã€è®©ç”Ÿæ´»æ›´ä¾¿åˆ©

Skywork-13B-Chat: 
åä¸ºæ™ºèƒ½å®¶å±…ï¼Œç§‘æŠ€èå…¥ç”Ÿæ´»ï¼Œè®©æ‚¨çš„å®¶å……æ»¡å¯èƒ½ã€‚

ç”¨æˆ·ï¼š
ä»¥åšä¸€ä¸ªã€Šåå¤©ç‹‚å‡30æ–¤ã€‹çš„æµ·æŠ¥ï¼Œæµ·æŠ¥è¦æ±‚åŒ…å«å¥èº«å‰åçš„å¯¹æ¯”ï¼Œæœ€ç»ˆé€šè¿‡å¥èº«å‡è‚¥èµ°å‘äººç”Ÿå·…å³°

Skywork-13B-Chat:
æµ·æŠ¥è®¾è®¡ï¼š

æ ‡é¢˜ï¼šã€Šåå¤©ç‹‚å‡30æ–¤ï¼Œä½ ä¹Ÿå¯ä»¥ï¼ã€‹

èƒŒæ™¯ï¼šé»‘è‰²èƒŒæ™¯ï¼Œè±¡å¾ç€å¥èº«çš„è‰°è¾›å’ŒæŒ‘æˆ˜ã€‚

å·¦ä¸Šè§’ï¼šä¸€ä¸ªæ˜æ˜¾çš„æ ‡å¿—æ€§logoï¼Œå¯ä»¥æ˜¯ä¸€ä¸ªè‚Œè‚‰çº¿æ¡å›¾æˆ–è€…ä¸€ä¸ªç‡ƒè„‚çš„å›¾æ ‡ï¼Œä»£è¡¨å¥èº«çš„è¿‡ç¨‹ã€‚

å³ä¸Šè§’ï¼šä¸€ä¸ªæˆåŠŸå‡å»30æ–¤ä½“é‡çš„äººç‰©å½¢è±¡ï¼Œç©¿ç€å¸…æ°”çš„è¡£æœï¼Œç«™åœ¨å±±é¡¶ï¼Œé¢å¸¦å¾®ç¬‘ï¼Œæ‰‹æŒå“‘é“ƒï¼ŒèˆåŠ¨æ‰‹è‡‚ï¼Œå±•ç¤ºå‡ºä»–/å¥¹å·²ç»å®Œæˆäº†è¿™ä¸ªç›®æ ‡ã€‚äººç‰©å½¢è±¡åº”è¯¥å æ®æµ·æŠ¥çš„å¤§éƒ¨åˆ†ç©ºé—´ï¼Œçªå‡ºé‡ç‚¹ã€‚

ä¸­éƒ¨ï¼šä¸€ä¸ªå€’è®¡æ—¶çš„æ—¥å†å›¾åƒï¼Œæ˜¾ç¤ºè¿˜å‰©ä¸‹çš„æ—¶é—´ï¼Œå¼ºè°ƒâ€œåå¤©â€è¿™ä¸ªè¯ï¼Œä»¥åŠä¸€äº›å…³äºå¥èº«çš„æç¤ºï¼Œæ¯”å¦‚â€œå‡è¡¡é¥®é£Ÿ+é€‚é‡è¿åŠ¨â€ã€‚

ä¸‹éƒ¨ï¼šä¸€äº›é¼“åŠ±çš„è¯ï¼Œæ¯”å¦‚â€œç›¸ä¿¡è‡ªå·±ï¼Œä½ ä¸€å®šå¯ä»¥çš„ï¼â€æˆ–è€…â€œæ±—æ°´é“¸å°±è¾‰ç…Œï¼Œå‡è‚¥å°±æ˜¯æ”¹å˜ç”Ÿæ´»ï¼â€

é¢œè‰²ï¼šä¸»è¦ä½¿ç”¨æ©™è‰²å’Œç™½è‰²ï¼Œæ©™è‰²ä»£è¡¨çƒ­æƒ…å’Œæ´»åŠ›ï¼Œç™½è‰²ä»£è¡¨çº¯æ´å’ŒæœŸå¾…ã€‚

æ•´ä½“è®¾è®¡ï¼šç®€æ´æ˜å¿«ï¼Œå……æ»¡åŠ¨åŠ›ï¼Œæ˜“äºè®°å¿†ï¼Œå¼•äººæ³¨ç›®ã€‚

æ–‡æ¡ˆï¼šæ¸…æ™°æ˜äº†ï¼Œå­—ä½“å¤§ä¸”æ˜¾çœ¼ï¼Œå†…å®¹ç®€çŸ­æœ‰åŠ›ï¼ŒèŠ‚å¥ç´§å‡‘ï¼Œå¼•äººå…¥èƒœã€‚

é™„åŠ å…ƒç´ ï¼šå¯ä»¥æ·»åŠ ä¸€äº›ä¸å¥èº«ç›¸å…³çš„å°æ’å›¾æˆ–è€…æ¼«ç”»ï¼Œå¦‚ä¸¾å“‘é“ƒã€è·‘æ­¥ã€ç‘œä¼½ç­‰ï¼Œä½¿æµ·æŠ¥çœ‹èµ·æ¥æ›´æœ‰è¶£å‘³æ€§ã€‚

æ•´ä½“é£æ ¼ï¼šç®€çº¦ã€ç›´æ¥ã€è‡ªä¿¡ã€æ¿€åŠ±ã€‚ 


ç”¨æˆ·ï¼š
æˆ‘ä¸»è¦æ˜¯åšæ¯å©´äº§å“çš„ç”µå•†ç›´æ’­ï¼Œæœ€è¿‘æ­£å€¼6.18ï¼Œå…¨ä½“å•†å“æ‰“8æŠ˜æœ€ä½æŠ˜æ‰£6.8æŠ˜ã€‚å†™ä¸€ä»½å¾®åšåŠ¨æ€ï¼Œèƒ½å¤Ÿå¸å¼•å…³æ³¨æˆ‘çš„äººåŠæ—¶è¿›å…¥ç›´æ’­é—´æŠ¢ç¦åˆ©

Skywork-13B-Chatï¼š
äº²çˆ±çš„æœ‹å‹ä»¬ï¼Œå¥½æ¶ˆæ¯ï¼éšç€6.18è´­ç‰©ç‹‚æ¬¢èŠ‚çš„åˆ°æ¥ï¼Œæˆ‘ä»¬çš„æ¯å©´äº§å“ç›´æ’­é—´å‡†å¤‡äº†ä¸€ç³»åˆ—æƒŠå–œä¼˜æƒ ç»™å¤§å®¶ï¼å…¨åœºå•†å“8æŠ˜èµ·ï¼Œæœ€ä½æŠ˜æ‰£6.8æŠ˜ï¼Œå“è´¨å¥½è´§ï¼Œè½»æ¾å…¥æ‰‹ï¼

ï¸æ–°å“æ¨èï¼š

æ¬¢è¿æ¥åˆ°æˆ‘ä»¬çš„ç›´æ’­é—´ï¼Œé¦–å…ˆä¸ºå¤§å®¶æ¨èä¸€æ¬¾å…¨æ–°ä¸Šå¸‚çš„æ¯å©´äº§å“â€”â€”ã€XXå“ç‰Œå©´å„¿æ¨è½¦ã€‘ï¼Œå®ƒé‡‡ç”¨ä¼˜è´¨ææ–™ï¼Œè½»ä¾¿æ˜“æŠ˜å ï¼Œè®¾è®¡ç²¾ç¾ï¼Œè®©ä½ çš„å®å®åœ¨å‡ºè¡Œæ—¶äº«å—åˆ°æœ€èˆ’é€‚çš„ä½“éªŒã€‚è¿˜æœ‰ã€YYå“ç‰Œæ™ºèƒ½å„¿ç«¥æ‰‹è¡¨ã€‘ï¼Œå®šä½å‡†ç¡®ï¼Œé€šè¯æ¸…æ™°ï¼Œæ˜¯å­©å­æˆé•¿çš„å¿…å¤‡å®‰å…¨è£…å¤‡ã€‚

ç¦åˆ©é¢„å‘Šï¼š

ä¸ä»…å¦‚æ­¤ï¼Œæˆ‘ä»¬è¿˜å‡†å¤‡äº†ä¸€ç³»åˆ—é™æ—¶ç¦åˆ©æ´»åŠ¨ï¼ŒåŒ…æ‹¬æ»¡é¢å…é‚®ã€æŠ½å¥–é€è±ªç¤¼ã€æ–°å“è¯•ç”¨ç­‰ï¼Œåªè¦ä½ è¶³å¤Ÿæ´»è·ƒï¼Œè¿™äº›ç¦åˆ©å°±æ˜¯ä½ çš„ï¼è®°å¾—é”å®šæˆ‘ä»¬çš„ç›´æ’­é—´å“¦ï¼

æ—¶é—´å®‰æ’ï¼š

è¯·å¤§å®¶å‡†æ—¶å‚åŠ æˆ‘ä»¬çš„ç›´æ’­ï¼Œåˆ†äº«æ›´å¤šçš„è´­ç‰©å¿ƒå¾—å’Œä¼˜æƒ ä¿¡æ¯ã€‚æˆ‘ä»¬å°†åœ¨æ¯å¤©ä¸‹åˆ3ç‚¹å¼€å§‹ç›´æ’­ï¼Œæ™šä¸Š9ç‚¹ç»“æŸã€‚æœŸå¾…æ‚¨çš„å‚ä¸ï¼Œä¸€èµ·åº¦è¿‡è¿™ä¸ªæ„‰å¿«çš„6.18ï¼

#6.18è´­ç‰©èŠ‚# #æ¯å©´äº§å“# #ç›´æ’­é—´# 

ç”¨æˆ·ï¼š
æˆ‘æ˜¯ä¸€åè¿åŠ¨å‘˜ï¼Œæœ€è¿‘æ¯”èµ›å–å¾—å¾ˆå¥½çš„æˆç»©å—åˆ°å¤§å®¶çš„å…³æ³¨å’Œè®¤å¯ã€‚å¸®æˆ‘å†™ä¸€ä»½å¾®åšæ–‡æ¡ˆï¼Œå¸®æˆ‘æ„Ÿè°¢å¤§å®¶æ”¯æŒæˆ‘ï¼Œè¦æœ‰æ—¥å¸¸æ„Ÿï¼Œå¹¶è¯­è¨€éšæ„ä¸€äº›

Skywork-13B-Chatï¼š
å½“ç„¶ï¼ä»¥ä¸‹æ˜¯ä¸€ä»½ç®€å•çš„å¾®åšæ–‡æ¡ˆï¼š

å¤§å®¶å¥½ï¼éå¸¸æ„Ÿè°¢å¤§å®¶å¯¹æˆ‘æœ€è¿‘æ¯”èµ›çš„å…³æ³¨å’Œæ”¯æŒï¼çœŸçš„éå¸¸æ„Ÿæ¿€ï¼æ¯ä¸€ä¸ªäººçš„é¼“åŠ±å’Œæ”¯æŒéƒ½æ˜¯æˆ‘å‰è¿›çš„åŠ¨åŠ›ã€‚æ— è®ºæ˜¯å®¶äººã€æœ‹å‹ã€æ•™ç»ƒè¿˜æ˜¯é˜Ÿå‹ï¼Œä½ ä»¬ä¸€ç›´ä»¥æ¥çš„é™ªä¼´å’Œæ”¯æŒéƒ½æ˜¯æˆ‘èµ°å‘æˆåŠŸçš„é‡è¦æ¨åŠ¨åŠ›ã€‚ç‰¹åˆ«è¦æ„Ÿè°¢æˆ‘çš„çˆ¶æ¯ï¼Œä»–ä»¬å§‹ç»ˆç»™äºˆæˆ‘æ— å°½çš„çˆ±å’Œå…³æ€€ï¼Œæ˜¯ä½ ä»¬è®©æˆ‘æœ‰äº†å‹‡æ°”å’Œå†³å¿ƒå»è¿½æ±‚æ¢¦æƒ³ã€‚åŒæ—¶ï¼Œä¹Ÿè¦æ„Ÿè°¢æˆ‘çš„æ•™ç»ƒå’Œé˜Ÿå‹ä»¬ï¼Œä½ ä»¬çš„æŒ‡å¯¼å’Œå¸®åŠ©è®©æˆ‘ä¸æ–­è¿›æ­¥ï¼Œæ¯ä¸€æ¬¡å¤±è´¥éƒ½è®©æˆ‘æ›´åŠ åšå¼ºï¼Œæ¯ä¸€æ¬¡æˆåŠŸéƒ½è®©æˆ‘æ›´åŠ è°¦è™šã€‚ 

åœ¨è¿™ä¸ªå……æ»¡ç«äº‰çš„æ—…ç¨‹ä¸­ï¼Œæˆ‘æ·±æ·±æ„Ÿå—åˆ°äº†åŠªåŠ›å’ŒåšæŒçš„é‡è¦æ€§ã€‚æ²¡æœ‰ä»€ä¹ˆæ˜¯è½»è€Œæ˜“ä¸¾å°±å¯ä»¥å®ç°çš„ï¼Œæ¯ä¸€ä¸ªæˆåŠŸèƒŒåéƒ½éšè—ç€æ— æ•°çš„æ±—æ°´å’Œä»˜å‡ºã€‚æˆ‘å¸Œæœ›é€šè¿‡æˆ‘çš„åŠªåŠ›å’ŒåšæŒï¼Œèƒ½å¤Ÿæ¿€åŠ±æ›´å¤šäººå‚ä¸è¿™é¡¹è¿åŠ¨ï¼Œè¿½é€è‡ªå·±çš„æ¢¦æƒ³ã€‚

è¿™ä¸ªæˆå°±åªæ˜¯æˆ‘äººç”Ÿä¸­çš„ä¸€å°éƒ¨åˆ†ï¼Œæˆ‘ä¼šç»§ç»­åŠªåŠ›å­¦ä¹ å’Œæˆé•¿ï¼Œä¸ºæ›´é«˜çš„ç›®æ ‡å¥‹æ–—ã€‚æœªæ¥çš„è·¯è¿˜å¾ˆé•¿ï¼Œä½†æˆ‘ç›¸ä¿¡ï¼Œåªè¦æœ‰ä½ ä»¬çš„æ”¯æŒå’Œé¼“åŠ±ï¼Œæˆ‘ä¸€å®šèƒ½èµ°å¾—æ›´è¿œã€‚è¯·å¤§å®¶ç»§ç»­å…³æ³¨æˆ‘çš„åŠ¨æ€ï¼Œè§è¯æˆ‘çš„æˆé•¿å’ŒåŠªåŠ›ï¼

å†æ¬¡æ„Ÿè°¢å¤§å®¶çš„æ”¯æŒï¼çˆ±ä½ ä»¬ï¼

#è¿åŠ¨å‘˜# #æ‹¼æç²¾ç¥#
`````

</details>

<details><summary><b>Composition Generation</b></summary>

`````
ç”¨æˆ·ï¼š
ä»¥â€œåšä¸€ä¸ªæœ‰è´£ä»»å¿ƒçš„äººâ€å†™ä¸€ç¯‡è®®è®ºæ–‡ã€‚è¦æ±‚ï¼šâ‘´é¢˜ç›®è‡ªæ‹Ÿï¼Œæ–‡ä½“ä¸é™ï¼›â‘µå­—æ•°ä¸å°‘äº600ï¼›â‘¶æ–‡ä¸­ä¸å¾—å‡ºç°çœŸå®çš„å§“åã€æ ¡åã€åœ°åã€‚

Skywork-13B-Chat
æ ‡é¢˜ï¼šåšä¸€ä¸ªæœ‰è´£ä»»å¿ƒçš„äºº

åœ¨è¿™ä¸ªç«äº‰æ¿€çƒˆçš„ç¤¾ä¼šä¸­ï¼Œæ¯ä¸ªäººéƒ½æ‰®æ¼”ç€ä¸åŒçš„è§’è‰²ï¼Œæ‰¿æ‹…ç€ç›¸åº”çš„è´£ä»»ã€‚ä¸€ä¸ªæœ‰è´£ä»»å¿ƒçš„äººï¼Œä¸ä»…è¦å¯¹è‡ªå·±è´Ÿè´£ï¼Œè¿˜è¦å¯¹ä»–äººã€é›†ä½“å’Œç¤¾ä¼šè´Ÿè´£ã€‚é‚£ä¹ˆï¼Œä»€ä¹ˆæ˜¯è´£ä»»å¿ƒå‘¢ï¼Ÿç®€å•æ¥è¯´ï¼Œå°±æ˜¯å¯¹è‡ªå·±ã€å¯¹ä»–äººã€å¯¹ç¤¾ä¼šçš„æ‰¿è¯ºå’Œä¹‰åŠ¡çš„è‡ªè§‰å±¥è¡Œã€‚ä¸€ä¸ªæœ‰è´£ä»»å¿ƒçš„äººï¼Œä¼šä¸»åŠ¨æ‰¿æ‹…èµ·è‡ªå·±çš„è´£ä»»ï¼Œå°½å¿ƒå°½åŠ›åœ°å®Œæˆæ¯ä¸€é¡¹ä»»åŠ¡ï¼Œä¸æŠ±æ€¨ä¸æ¨è¯¿ï¼Œå‹‡äºé¢å¯¹å›°éš¾å’ŒæŒ‘æˆ˜ã€‚æœ¬æ–‡å°†ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è®ºè¿°å¦‚ä½•æˆä¸ºä¸€ä¸ªæœ‰è´£ä»»å¿ƒçš„äººã€‚

é¦–å…ˆï¼Œä½œä¸ºä¸€ä¸ªæœ‰è´£ä»»å¿ƒçš„äººï¼Œæˆ‘ä»¬åº”è¯¥å¯¹è‡ªå·±è´Ÿè´£ã€‚å¯¹è‡ªå·±è´Ÿè´£ï¼Œæ„å‘³ç€è¦å…³çˆ±è‡ªå·±çš„èº«å¿ƒå¥åº·ï¼ŒåŠªåŠ›æé«˜è‡ªèº«ç´ è´¨ã€‚æˆ‘ä»¬è¦æ ‘ç«‹æ­£ç¡®çš„äººç”Ÿè§‚å’Œä»·å€¼è§‚ï¼Œè®¤è¯†åˆ°äººç”Ÿçš„æ„ä¹‰åœ¨äºå¥‰çŒ®å’Œæˆå°±ã€‚æˆ‘ä»¬è¦ä¸æ–­å­¦ä¹ ï¼Œå……å®è‡ªå·±ï¼Œä¸ºä¸ªäººå’Œç¤¾ä¼šçš„å‘å±•åšå‡ºè´¡çŒ®ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿè¦å…³çˆ±è‡ªå·±çš„æƒ…æ„Ÿä¸–ç•Œï¼Œä¿æŒè‰¯å¥½çš„äººé™…å…³ç³»ï¼ŒåŸ¹å…»ç§¯æçš„å¿ƒæ€ï¼Œå‹‡å¾€ç›´å‰ï¼Œè¿½æ±‚è¿›æ­¥ã€‚

å…¶æ¬¡ï¼Œä½œä¸ºä¸€ä¸ªæœ‰è´£ä»»å¿ƒçš„äººï¼Œæˆ‘ä»¬åº”è¯¥å¯¹ä»–äººè´Ÿè´£ã€‚å¯¹ä»–äººè´Ÿè´£ï¼Œæ„å‘³ç€å…³å¿ƒä»–äººï¼Œå¸®åŠ©ä»–ä»¬è§£å†³é—®é¢˜ï¼Œç»™äºˆå…³çˆ±å’Œæ”¯æŒã€‚æˆ‘ä»¬è¦å­¦ä¼šç«™åœ¨åˆ«äººçš„è§’åº¦æ€è€ƒé—®é¢˜ï¼Œç†è§£å’Œå°Šé‡ä»–äººï¼Œä¹äºåŠ©äººï¼Œå°½å¯èƒ½åœ°å¸®åŠ©é‚£äº›éœ€è¦æˆ‘ä»¬å¸®åŠ©çš„äººã€‚æˆ‘ä»¬è¦å­¦ä¼šåŒ…å®¹å’Œæ¥çº³ä»–äººçš„ä¸è¶³ï¼Œé¼“åŠ±ä»–ä»¬ï¼Œç»™äºˆä»–ä»¬ä¿¡å¿ƒå’Œå‹‡æ°”ã€‚å½“æˆ‘ä»¬çœ‹åˆ°èº«è¾¹çš„äººé‡åˆ°å›°å¢ƒæ—¶ï¼Œæˆ‘ä»¬è¦æ•¢äºä¼¸å‡ºæ´æ‰‹ï¼Œå¸®åŠ©ä»–ä»¬åº¦è¿‡éš¾å…³ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬æ‰èƒ½èµ¢å¾—åˆ«äººçš„å°Šé‡å’Œä¿¡ä»»ï¼Œä¹Ÿèƒ½æ›´å¥½åœ°èå…¥ç¤¾ä¼šã€‚

å†æ¬¡ï¼Œä½œä¸ºä¸€ä¸ªæœ‰è´£ä»»å¿ƒçš„äººï¼Œæˆ‘ä»¬è¿˜åº”è¯¥å¯¹ç¤¾ä¼šè´Ÿè´£ã€‚å¯¹ç¤¾ä¼šè´Ÿè´£ï¼Œæ„å‘³ç€è¦éµå®ˆæ³•å¾‹æ³•è§„ï¼Œç»´æŠ¤ç¤¾ä¼šå…¬å¾·ï¼Œå…³å¿ƒç¤¾ä¼šå…¬ç›Šï¼Œä¸ºç¤¾ä¼šçš„å’Œè°ç¨³å®šåšå‡ºè´¡çŒ®ã€‚æˆ‘ä»¬è¦åšåˆ°è¯šä¿¡å®ˆæ³•ï¼Œè¨€è¡Œä¸€è‡´ï¼Œç§¯æå‚ä¸ç¤¾ä¼šå…¬ç›Šæ´»åŠ¨ï¼Œå…³å¿ƒç¤¾ä¼šä¸Šçš„å¼±åŠ¿ç¾¤ä½“ï¼Œå°½è‡ªå·±æ‰€èƒ½å»å›æŠ¥ç¤¾ä¼šã€‚æˆ‘ä»¬è¦ç”¨è‡ªå·±çš„å®é™…è¡ŒåŠ¨è·µè¡Œç¤¾ä¼šä¸»ä¹‰æ ¸å¿ƒä»·å€¼è§‚ï¼Œä¼ æ’­æ­£èƒ½é‡ï¼Œå¸¦åŠ¨å‘¨å›´çš„äººå…±åŒä¸ºæ„å»ºå’Œè°ç¤¾ä¼šè´¡çŒ®åŠ›é‡ã€‚

æ€»ä¹‹ï¼Œåšä¸€ä¸ªæœ‰è´£ä»»å¿ƒçš„äººï¼Œæ—¢æ˜¯å¯¹è‡ªå·±çš„è´Ÿè´£ï¼Œä¹Ÿæ˜¯å¯¹ä»–äººå’Œç¤¾ä¼šçš„è´Ÿè´£ã€‚æˆ‘ä»¬è¦æ—¶åˆ»ç‰¢è®°è‡ªå·±çš„è´£ä»»ï¼ŒæŠŠè´£ä»»å¿ƒåŒ–ä¸ºè¡ŒåŠ¨çš„åŠ¨åŠ›ï¼Œä¸æ–­æå‡è‡ªå·±çš„é“å¾·å“è´¨ï¼ŒåŠªåŠ›æˆé•¿ä¸ºä¸€ä¸ªæœ‰è´£ä»»å¿ƒçš„äººã€‚åªæœ‰è¿™æ ·ï¼Œæˆ‘ä»¬æ‰èƒ½åœ¨è¿™ä¸ªçº·ç¹å¤æ‚çš„ç¤¾ä¼šä¸­ç«‹è¶³ï¼Œå®ç°è‡ªå·±çš„ä»·å€¼å’Œæ¢¦æƒ³ã€‚ 


ç”¨æˆ·ï¼š
è¯·ä»¥å¤å¤œé›¨åæ¼«æ­¥ä¸ºä¸»é¢˜å†™ä¸€ç¯‡æ•£æ–‡ï¼Œé€šè¿‡æå†™é›¨åæ¸…å‡‰çš„å¤œæ™šæ°›å›´ï¼Œåæ€ç°å®ç¤¾ä¼šçš„ä¸å®‰å’Œç„¦è™‘ï¼Œå¹¶å±•ç°ä¸€ç§ç§¯æçš„ç”Ÿæ´»æ€åº¦ã€‚

Skywork-13B-Chatï¼š
æ ‡é¢˜ï¼šã€Šå¤å¤œé›¨åçš„å®é™ã€‹
å¤å¤œï¼Œç‚çƒ­ä¸å–§åš£ç›¸ä¼´ï¼Œåƒæ˜¯ç°å®ç¤¾ä¼šä¸­æ¯ä¸€ä¸ªäººçš„ç¹å¿™å’Œç„¦è™‘ã€‚ä½†é‚£å¤©æ™šä¸Šï¼Œä¸€åœºçªå¦‚å…¶æ¥çš„é›¨æ´—å‡€äº†å¤œç©ºçš„ç‚½çƒ­ï¼Œä¹Ÿæ²‰æ·€äº†åŸå¸‚çš„å–§é—¹ã€‚é›¨åçš„ç©ºæ°”å¦‚åŒå‡‰èŒ¶ï¼Œå¸¦ç€å¾®å¾®çš„ç”œï¼Œæ¸…çˆ½å…¥å¿ƒã€‚æˆ‘è¸å‡ºå®¶é—¨ï¼Œæ¼«æ­¥åœ¨è¿™ä¸ªç”±é›¨æ°´æ¶¤æ´—è¿‡çš„ä¸–ç•Œï¼Œè¯•å›¾å¯»æ‰¾ä¸€äº›æ¸…å‡‰ï¼Œä»¥åŠé‚£ä»½ä¹…è¿çš„å®é™ã€‚
åˆšç»å†äº†ä¸€åœºçº·æ‰°çš„åŸå¸‚ï¼Œä»¿ä½›æ¢äº†å‰¯é¢å­”ã€‚é‚£äº›è¢«ç‚ç‚çƒˆæ—¥çƒ¤å¾—ç‚™çƒ­æ— æ¯”çš„æŸæ²¹é©¬è·¯ï¼Œé‚£äº›åå°„ç€éœ“è™¹ç¯å…‰çš„ç»ç’ƒå¹•å¢™ï¼Œéƒ½åœ¨é›¨ä¸­æ˜¾å¾—å¦‚æ­¤æ¸…æ–°ã€‚æ ‘å¶è½»è½»æ‘‡æ›³ï¼Œä½è¯­ç€å¤§è‡ªç„¶çš„éŸµå¾‹ã€‚ç©ºæ°”ä¸­å¼¥æ¼«ç€æ·¡æ·¡çš„é’è‰é¦™ï¼Œé‚£æ˜¯é›¨åçš„å‘³é“ï¼Œæ··åˆç€æ³¥åœŸçš„æ°”æ¯ï¼Œè®©äººå¿ƒæ—·ç¥æ€¡ã€‚
è„šä¸‹çš„ç§¯æ°´å€’æ˜ ç€å¤©ç©ºçš„äº‘å½©ï¼Œæ¶Ÿæ¼ªä¸€åœˆåœˆè¡æ¼¾å¼€å»ã€‚è¿œå¤„çš„ç¯ç«é€æ¸äº®èµ·ï¼Œæ˜Ÿæ˜Ÿç‚¹ç‚¹çš„å…‰ç‚¹ç¼€ç€è¿™ç‰‡å¤œè‰²ã€‚è½¦æµæ¸å°‘ï¼Œè¡—é“å˜å¾—ç©ºæ—·è€Œé™è°§ã€‚å¶å°”å‡ è¾†æ±½è½¦ç»è¿‡ï¼Œä¹Ÿåªæ˜¯ç•™ä¸‹ä¸€é˜µé˜µçŸ­æš‚çš„æ°´èŠ±æ¶Ÿæ¼ªã€‚äººä»¬æ—©æ—©èº²è¿›äº†å±‹å­é‡Œï¼Œäº«å—å®¤å†…çš„å‡‰çˆ½ã€‚è€Œæˆ‘å´é€‰æ‹©åœ¨è¿™å¤œè‰²é‡Œï¼Œä»»å‡­é›¨æ»´æ‰“æ¹¿å‘ä¸ï¼Œå°½æƒ…åœ°å‘¼å¸ç€é›¨åçš„æ–°é²œç©ºæ°”ã€‚
çªç„¶ï¼Œä¸€é˜µé£å¹è½äº†æ ‘æ¢¢çš„å¶å­ï¼Œä¼´éšç€â€œæ²™æ²™â€å£°ï¼Œå®›å¦‚å¤§è‡ªç„¶çš„ä½è¯­ã€‚è¿™äº›å¶å­åœ¨ç©ºä¸­é£˜èˆï¼Œæœ€åé™é™åœ°èººåœ¨äº†æˆ‘çš„æ‰‹å¿ƒã€‚è¿™ä¸€åˆ»ï¼Œæˆ‘ä»¿ä½›æ„Ÿå—åˆ°äº†ç”Ÿå‘½çš„å¾‹åŠ¨ï¼Œé‚£æ˜¯ä¸€ç§åšéŸ§çš„åŠ›é‡ï¼Œä¸€ç§ä»¤äººæ„ŸåŠ¨çš„ç¾ä¸½ã€‚è¿™é£ï¼Œè¿™é›¨ï¼Œè¿™å¤œè‰²ï¼Œéƒ½è®©æˆ‘æƒ³èµ·äº†äººç”Ÿä¸­çš„æŸäº›æ—¶åˆ»ã€‚
é‚£æ™šï¼Œæˆ‘æ€è€ƒç€è‡ªå·±çš„äººç”Ÿã€‚æ¯ä¸ªäººéƒ½åœ¨å¿™ç¢Œã€å¥”æ³¢ï¼Œä¸ºäº†ç”Ÿæ´»è€Œç„¦èºä¸å®‰ã€‚å¯æ˜¯ï¼Œå½“æˆ‘ä»¬åœä¸‹è„šæ­¥ï¼Œå›æœ›è¿‡å»ï¼Œæ˜¯ä¸æ˜¯èƒ½æ‰¾åˆ°é‚£é¢—åˆå¿ƒï¼Ÿæ˜¯ä¸æ˜¯èƒ½æ‰¾å›é‚£ä»½æ›¾ç»çš„æ¢¦æƒ³ï¼Ÿæ˜¯ä¸æ˜¯èƒ½æ‹¥æœ‰é‚£ä»½å¯¹ç”Ÿæ´»çš„çƒ­çˆ±å’ŒæœŸå¾…ï¼Ÿ
é›¨åçš„ä¸–ç•Œï¼Œæ˜¯é‚£ä¹ˆçº¯å‡€ï¼›é›¨åçš„å¤©åœ°ï¼Œæ˜¯é‚£ä¹ˆå’Œè°ã€‚æˆ‘æƒ³ï¼Œç”Ÿæ´»å°±åƒè¿™åœºé›¨åçš„æ¸…å‡‰ï¼Œç»™æˆ‘ä»¬å¸¦æ¥çš„ä¸ä»…ä»…æ˜¯èˆ’çˆ½ï¼Œæ›´æ˜¯å¸Œæœ›ã€‚æˆ‘ä»¬åº”è¯¥çæƒœçœ¼å‰çš„ä¸€åˆ‡ï¼Œçæƒœæ¯ä¸€ä¸ªç¬é—´ï¼Œå› ä¸ºæ—¶å…‰èè‹’ï¼Œå²æœˆå¦‚æ¢­ï¼Œæˆ‘ä»¬éƒ½ä¼šè€å»ã€‚
æœ‰æ—¶å€™ï¼Œæˆ‘ä»¬éœ€è¦æ”¾æ…¢è„šæ­¥ï¼Œå»æ„Ÿå—ç”Ÿæ´»çš„ç¾å¥½ã€‚ä¸è¦è®©è‡ªå·±æˆä¸ºæ—¶é—´çš„å¥´éš¶ï¼Œè€Œæ˜¯è¦å­¦ä¼šæ¬£èµå®ƒçš„æµé€ï¼Œäº«å—å®ƒå¸¦ç»™æˆ‘ä»¬çš„ç§ç§å¥‡å¦™ã€‚æˆ‘ä»¬å¯ä»¥åœ¨ç¹åçš„åŸå¸‚ä¸­æ‰¾ä¸€å¤„å®‰é™çš„è§’è½ï¼Œè†å¬å¤§è‡ªç„¶çš„å£°éŸ³ï¼Œè®©å¿ƒçµå¾—ä»¥å¹³é™ã€‚æˆ‘ä»¬å¯ä»¥åœ¨å¿™ç¢Œçš„å·¥ä½œä¸­æŠ½å‡ºæ—¶é—´ï¼Œå»ä½“éªŒç”Ÿæ´»çš„çœŸè°›ï¼Œè®©è‡ªå·±æˆä¸ºä¸€ä¸ªæœ‰æ¸©åº¦çš„äººã€‚
ç”Ÿæ´»æœ¬ä¸ç¼ºå°‘ç¾ï¼Œåªæ˜¯ç¼ºå°‘å‘ç°ç¾çš„çœ¼ç›ã€‚åŒæ ·ï¼Œç”Ÿæ´»æœ¬ä¸ç¼ºå°‘å¿«ä¹ï¼Œåªæ˜¯ç¼ºå°‘è¿½æ±‚å¿«ä¹çš„å¿ƒå¢ƒã€‚é¢å¯¹ç°å®çš„ä¸å®‰å’Œç„¦è™‘ï¼Œæˆ‘ä»¬æ›´éœ€è¦ä¸€é¢—å¹³å’Œçš„å¿ƒï¼Œå»ä½“ä¼šç”Ÿæ´»çš„é…¸ç”œè‹¦è¾£ï¼Œå»å“å°é‚£äº›ç»†ç¢çš„å¹¸ç¦ã€‚
å¤å¤œé›¨åï¼Œæˆ‘æ²‰æµ¸åœ¨è¿™æ ·çš„å¤œæ™šï¼Œåæ€ç€äººç”Ÿçš„ç§ç§ã€‚æˆ‘æ„Ÿè°¢è¿™ä¸ªä¸–ç•Œï¼Œæ„Ÿè°¢è¿™åœºé›¨ï¼Œæ›´æ„Ÿè°¢ç”Ÿæ´»ç»™äºˆæˆ‘çš„ä¸€åˆ‡ã€‚å› ä¸ºå®ƒè®©æˆ‘æ˜ç™½äº†ï¼Œæ— è®ºæ€æ ·çš„æŒ«æŠ˜ã€å›°éš¾ï¼Œéƒ½æ˜¯ç”Ÿæ´»çš„é¦ˆèµ ï¼Œæ˜¯æˆ‘ä»¬æˆé•¿çš„ç—•è¿¹ã€‚åªè¦æˆ‘ä»¬ç”¨å¿ƒå»æ„Ÿå—ï¼Œç”¨çœ¼å»æ¬£èµï¼Œç”¨å‹‡æ°”å»é¢å¯¹ï¼Œæˆ‘ä»¬çš„ç”Ÿæ´»å®šä¼šå¦‚è¿™é›¨åçš„ä¸–ç•Œä¸€èˆ¬ï¼Œæ¸…æ–°ã€æ˜æœ—ã€å……æ»¡å¸Œæœ›ã€‚

`````

</details>

<details><summary><b>Speech Generation</b></summary>

`````
ç”¨æˆ·ï¼š
å†™ä¸€ä¸ªå…³äºç¯ä¿çš„æ¼”è®²ç¨¿,5~10åˆ†é’Ÿçš„,å†…å®¹ä¸è¦å¤ªæ·±å¥¥ï¼Œä¸¾ä¸€äº›ç›¸å…³çš„ä¾‹å­

Skywork-13B-Chatï¼š
å°Šæ•¬çš„å¬ä¼—ä»¬ï¼š
å¤§å®¶å¥½ï¼ä»Šå¤©æˆ‘éå¸¸è£å¹¸èƒ½å¤Ÿåœ¨è¿™é‡Œä¸å¤§å®¶åˆ†äº«æœ‰å…³ç¯ä¿çš„è¯é¢˜ã€‚ç¯ä¿ï¼Œæ˜¯æˆ‘ä»¬æ¯ä¸ªäººéƒ½åº”è¯¥å…³æ³¨å’Œé‡è§†çš„è®®é¢˜ï¼Œå› ä¸ºå®ƒå…³ç³»åˆ°æˆ‘ä»¬æ¯ä¸€ä¸ªäººçš„ç”Ÿæ´»è´¨é‡å’Œåœ°çƒçš„æœªæ¥ã€‚

è®©æˆ‘ä»¬å…ˆæ¥æƒ³è±¡ä¸€ä¸‹ï¼Œæˆ‘ä»¬çš„åœ°çƒæ˜¯å¦‚ä½•è¢«æˆ‘ä»¬æ‰€ç ´åçš„ã€‚ç©ºæ°”æ±¡æŸ“ã€æ°´æ±¡æŸ“ã€åƒåœ¾å›´åŸï¼Œè¿™äº›é—®é¢˜å·²ç»æ·±æ·±åœ°å½±å“ç€æˆ‘ä»¬çš„æ—¥å¸¸ç”Ÿæ´»ã€‚æˆ‘ä»¬æ¯å¤©éƒ½åœ¨ä½¿ç”¨èƒ½æºï¼Œä½†æ˜¯å´æ²¡æœ‰æ„è¯†åˆ°èƒ½æºçš„æœ‰é™æ€§å’Œå¯¹ç¯å¢ƒçš„å½±å“ã€‚æ£®æ—è¢«ç ä¼ï¼ŒåŠ¨ç‰©çš„æ –æ¯åœ°è¢«ç ´åï¼Œç‰©ç§ç­ç»çš„é€Ÿåº¦åŠ å¿«ã€‚æˆ‘ä»¬çš„åœ°çƒæ­£åœ¨å‘å‡ºç—›è‹¦çš„å‘¼å£°ï¼Œè€Œæˆ‘ä»¬å´å¸¸å¸¸å¿½è§†äº†è¿™äº›é—®é¢˜ï¼Œç»§ç»­è¿‡ç€æµªè´¹å’Œè´ªå©ªçš„ç”Ÿæ´»ã€‚

ç°åœ¨ï¼Œè¶Šæ¥è¶Šå¤šçš„ç§‘å­¦å®¶å’Œç ”ç©¶äººå‘˜å‘Šè¯‰æˆ‘ä»¬ï¼Œå¦‚æœæˆ‘ä»¬ä¸é‡‡å–è¡ŒåŠ¨ï¼Œé‚£ä¹ˆæœªæ¥çš„ä¸–ç•Œå°†ä¸å†é€‚åˆäººç±»å±…ä½ã€‚æˆ‘ä»¬éœ€è¦æ”¹å˜æˆ‘ä»¬çš„è¡Œä¸ºï¼Œå‡å°‘å¯¹ç¯å¢ƒçš„ç ´åï¼Œå¹¶ä¿æŠ¤æˆ‘ä»¬çš„åœ°çƒã€‚

å¹¸è¿çš„æ˜¯ï¼Œæˆ‘ä»¬æœ‰è®¸å¤šå¯ä»¥é‡‡å–çš„æªæ–½æ¥æ”¹å–„ç¯å¢ƒçŠ¶å†µã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥ä»å®¶åº­å¼€å§‹ï¼Œé€šè¿‡èŠ‚çº¦ç”¨æ°´ã€å‡å°‘ç”¨å¡‘æ–™è¢‹ã€åƒåœ¾åˆ†ç±»ç­‰ç®€å•çš„è¡ŒåŠ¨æ¥å‡å°‘ç¢³è¶³è¿¹ã€‚æˆ‘ä»¬è¿˜å¯ä»¥é€‰æ‹©è´­ä¹°ç¯ä¿äº§å“ï¼Œæ”¯æŒé‚£äº›è‡´åŠ›äºç¯ä¿çš„ä¼ä¸šï¼Œä¿ƒè¿›å¯æŒç»­ç”Ÿäº§å’Œå‘å±•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ç§¯æå‚ä¸ç¤¾åŒºæ´»åŠ¨ï¼Œå¦‚æ¤æ ‘ã€æ¸…ç†åƒåœ¾ã€å®£ä¼ ç¯ä¿çŸ¥è¯†ç­‰ï¼Œè®©æ›´å¤šçš„äººå‚ä¸åˆ°ç¯ä¿è¡ŒåŠ¨ä¸­æ¥ã€‚

æ”¿åºœä¹Ÿåœ¨ç§¯ææ¨åŠ¨ç¯ä¿æ”¿ç­–ï¼Œåˆ¶å®šæ³•è§„å’Œæ”¿ç­–æ¥é™åˆ¶æ±¡æŸ“å’Œèµ„æºæµªè´¹ã€‚ä¾‹å¦‚ï¼Œå»ºè®¾ç»¿è‰²å»ºç­‘ã€æ¨å¹¿æ–°èƒ½æºæŠ€æœ¯ã€åŠ å¼ºåºŸç‰©å›æ”¶åˆ©ç”¨ç­‰ï¼Œè¿™äº›ä¸¾æªä¸ä»…èƒ½é™ä½ç¯å¢ƒæ±¡æŸ“ï¼Œè¿˜èƒ½åˆ›é€ æ–°çš„å°±ä¸šæœºä¼šå’Œç»æµæ•ˆç›Šã€‚

ç„¶è€Œï¼ŒçœŸæ­£çš„æ”¹å˜éœ€è¦æˆ‘ä»¬æ¯ä¸ªäººçš„å…±åŒåŠªåŠ›ã€‚æˆ‘ä»¬å¿…é¡»è®¤è¯†åˆ°ï¼Œç¯ä¿ä¸ä»…æ˜¯ä¸€ç§è´£ä»»ï¼Œæ›´æ˜¯ä¸€ç§æ™ºæ…§ã€‚å½“æˆ‘ä»¬ä¿æŠ¤ç¯å¢ƒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿåœ¨ä¿æŠ¤æˆ‘ä»¬è‡ªå·±ã€‚æˆ‘ä»¬çš„åœ°çƒæ˜¯æˆ‘ä»¬å…±åŒçš„å®¶å›­ï¼Œåªæœ‰æˆ‘ä»¬å…±åŒåŠªåŠ›ï¼Œæ‰èƒ½è®©å®ƒå˜å¾—æ›´åŠ ç¾å¥½ã€‚

æœ€åï¼Œæˆ‘æƒ³å‘¼åå¤§å®¶ï¼Œè®©æˆ‘ä»¬ä¸€èµ·è¡ŒåŠ¨èµ·æ¥ï¼Œä¸ºç¯ä¿è´¡çŒ®è‡ªå·±çš„åŠ›é‡ã€‚ä»å°äº‹åšèµ·ï¼Œä»èº«è¾¹åšèµ·ï¼Œä»¥å®é™…è¡ŒåŠ¨æ¥ä¿æŠ¤æˆ‘ä»¬çš„åœ°çƒã€‚è®©æˆ‘ä»¬æºæ‰‹å¹¶è‚©ï¼Œä¸ºä¸‹ä¸€ä»£åˆ›é€ ä¸€ä¸ªæ›´ç¾å¥½çš„ä¸–ç•Œï¼

è°¢è°¢å¤§å®¶ï¼ 


ç”¨æˆ·ï¼š
å†™ä¸€ä»½åŠ¨æ¼«ç¤¾çš„ä¸“åœºä¸»æŒè¯ï¼Ÿæœ€å¥½çƒ­è¡€ï¼Œä¸­äºŒä¸€ç‚¹ã€‚

Skywork-13B-Chatï¼š
å°Šæ•¬çš„å„ä½è€å¸ˆï¼Œäº²çˆ±çš„åŒå­¦ä»¬ï¼Œå¤§å®¶å¥½ï¼

æ¬¢è¿æ¥åˆ°è¿™åœºæ¿€åŠ¨äººå¿ƒçš„åŠ¨æ¼«ç¤¾ä¸“åœºæ´»åŠ¨ï¼æˆ‘æ˜¯æ‚¨çš„ä¸»æŒäººï¼ˆä¸»æŒäººå§“åï¼‰ï¼Œä»Šå¤©å°†ä¼šä¸ºå¤§å®¶å¸¦æ¥ä¸€åœºå……æ»¡çƒ­æƒ…å’Œæ¬¢ä¹çš„åŠ¨æ¼«ç››å®´ï¼æˆ‘ä»¬çš„ä¸»é¢˜æ˜¯â€”â€”â€œåŠ¨æ¼«ï¼Œä¸æ­¢äºäºŒæ¬¡å…ƒâ€ã€‚

åŠ¨æ¼«ï¼Œè¿™æ˜¯ä¸€ä¸ªå¹¿å—æ¬¢è¿ã€å¤‡å—å–œçˆ±çš„æ–‡åŒ–å½¢å¼ï¼Œå®ƒä¸ä»…ä»…å­˜åœ¨äºäºŒç»´ä¸–ç•Œï¼Œæ›´åœ¨æˆ‘ä»¬æ¯ä¸€ä¸ªäººçš„å¿ƒä¸­ç‡ƒçƒ§ç€ä¸‰æ¬¡å…ƒçš„çƒ­è¡€ä¸æ¿€æƒ…ã€‚ä»Šå¤©ï¼Œè®©æˆ‘ä»¬ä¸€èµ·èµ°è¿›è¿™ä¸ªç²¾å½©çº·å‘ˆã€æ— é™å¯èƒ½çš„åŠ¨æ¼«ä¸–ç•Œï¼Œä¸€åŒåˆ†äº«é‚£äº›ä»¤äººç€è¿·çš„æ•…äº‹ã€è§’è‰²å’Œåˆ›ä½œèƒŒåçš„é­…åŠ›ã€‚

é¦–å…ˆï¼Œè®©æˆ‘ä»¬é‡æ¸©é‚£äº›å¹´ï¼Œé‚£äº›è¢«èª‰ä¸ºç»å…¸çš„åŠ¨æ¼«ä½œå“ã€‚ä»ã€Šç«å½±å¿è€…ã€‹ä¸­çš„åšéŸ§ä¸å¥‹æ–—ï¼Œåˆ°ã€Šè¿›å‡»çš„å·¨äººã€‹ä¸­çš„å‹‡æ°”ä¸å†’é™©ï¼Œå†åˆ°ã€Šæµ·è´¼ç‹ã€‹ä¸­çš„æ¢¦æƒ³ä¸å¸Œæœ›ï¼Œè¿™äº›éƒ½æ˜¯åŠ¨æ¼«æ‰€å¸¦ç»™æˆ‘ä»¬çš„ï¼Œä¸ä»…ä»…æ˜¯è§†è§‰ä¸Šçš„å†²å‡»ï¼Œæ›´æ˜¯å¿ƒçµæ·±å¤„çš„è§¦åŠ¨ã€‚è¿™äº›æ•…äº‹ï¼Œå°±åƒä¸€ç›ç›æ˜ç¯ï¼Œç…§äº®æˆ‘ä»¬å†…å¿ƒçš„ä¸–ç•Œï¼Œç»™æˆ‘ä»¬æä¾›äº†æ— æ•°æ¬¡çš„æ„ŸåŠ¨å’Œå¯ç¤ºã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å¸¦æ‚¨è¿›å…¥æ›´åŠ ç²¾å½©çš„ç¯èŠ‚ã€‚æˆ‘ä»¬é‚€è¯·äº†ä¸€äº›å¯¹åŠ¨æ¼«æœ‰ç€ç‹¬ç‰¹è§è§£å’Œçƒ­çˆ±çš„å˜‰å®¾ä»¬ï¼Œä»–ä»¬å°†ä¸å¤§å®¶åˆ†äº«ä»–ä»¬çš„å¿ƒå¾—ï¼Œè®²è¿°ä»–ä»¬çš„æ•…äº‹ï¼Œè®©æˆ‘ä»¬ä¸€èµ·æ·±å…¥æ¢è®¨åŠ¨æ¼«çš„é­…åŠ›å’Œå½±å“åŠ›ã€‚

åœ¨è¿™ä¸ªç¯èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†æœ‰å¹¸å¬åˆ°æ¥è‡ªä¸åŒé¢†åŸŸçš„åŠ¨æ¼«çˆ±å¥½è€…ä»¬åˆ†äº«ä»–ä»¬çš„è§‚ç‚¹ï¼Œæ— è®ºæ˜¯ä»åŠ¨æ¼«çš„åˆ¶ä½œè¿‡ç¨‹ï¼Œè¿˜æ˜¯ä»åŠ¨æ¼«çš„è‰ºæœ¯è®¾è®¡ï¼Œäº¦æˆ–æ˜¯ä»åŠ¨æ¼«çš„ç¤¾ä¼šå½±å“ï¼Œä»–ä»¬éƒ½æœ‰è‡ªå·±ç‹¬åˆ°çš„çœ‹æ³•ã€‚å¬ä»–ä»¬å¨“å¨“é“æ¥ï¼Œç›¸ä¿¡ä¼šç»™æ‚¨å¸¦æ¥æ–°çš„è§†è§’å’Œå¯å‘ã€‚

æœ€åï¼Œæˆ‘ä»¬è¿˜å‡†å¤‡äº†ä¸€ç³»åˆ—çš„äº’åŠ¨ç¯èŠ‚ï¼Œè®©å¤§å®¶å¯ä»¥å°½æƒ…å±•ç¤ºè‡ªå·±çš„æ‰åï¼Œé‡Šæ”¾è‡ªå·±çš„çƒ­æƒ…ã€‚ä¸è®ºæ˜¯cosplayã€çŸ­ç‰‡æ‹æ‘„ï¼Œè¿˜æ˜¯åŠ¨æ¼«ç»˜ç”»ï¼Œè¿™é‡Œéƒ½æœ‰ä½ æ–½å±•çš„èˆå°ã€‚è¯·å¤§å®¶ç§¯æå‚ä¸ï¼Œè®©æˆ‘ä»¬ä¸€èµ·äº«å—è¿™åœºå±äºæˆ‘ä»¬çš„åŠ¨æ¼«ç››å®´ï¼

å„ä½åŒå­¦ï¼Œæˆ‘ä»¬éƒ½æ˜¯ç”Ÿæ´»çš„æ¼«ç”»å®¶ï¼Œæˆ‘ä»¬ç”¨å¿ƒçµæ‰‹å·§çš„åŒæ‰‹åˆ›é€ å‡ºå±äºæˆ‘ä»¬çš„åŠ¨æ¼«ä¸–ç•Œã€‚è®©æˆ‘ä»¬ç”¨çƒ­æƒ…ç‚¹ç‡ƒçƒ­æƒ…ï¼Œç”¨æ¿€æƒ…ä¼ é€’æ¿€æƒ…ï¼Œè®©åŠ¨æ¼«çš„åŠ›é‡åœ¨æˆ‘ä»¬æ¯ä¸ªäººèº«ä¸Šå¾—ä»¥å±•ç°ï¼Œè®©æˆ‘ä»¬çš„ç¤¾å›¢ç”Ÿæ´»æ›´åŠ ç²¾å½©ã€‚

è®©æˆ‘ä»¬ä¸€èµ·ï¼Œç”¨çƒ­è¡€çš„å¿ƒï¼Œç‡ƒçƒ§äºŒæ¬¡å…ƒçš„çƒ­æƒ…ï¼Œå…±åŒåˆ›é€ å‡ºæ›´å¤šçš„ç²¾å½©ï¼æ„Ÿè°¢å¤§å®¶çš„å‚ä¸ï¼Œæˆ‘ä»¬çš„åŠ¨æ¼«ç¤¾ï¼ŒæœŸå¾…ä¸ä½ ä»¬ä¸€èµ·ï¼Œç»§ç»­åœ¨äºŒæ¬¡å…ƒçš„ä¸–ç•Œé‡Œï¼Œå¯»æ‰¾é‚£ä»½æ°¸ä¸æ¶ˆé€çš„çƒ­è¡€å’Œæ¿€æƒ…ï¼

è°¢è°¢å¤§å®¶ï¼ 

`````

</details>

<details><summary><b>Experience and Reflections</b></summary>

`````

ç”¨æˆ·ï¼š
å†™ä¸€éç»å†è¿‡ç”µå·¥å®è®­çš„å¿ƒå¾—ï¼Œå†…å®¹åŒ…å«ç„Šæ¥ã€åŒè”å¼€å…³æ¥çº¿å’Œç”»ç”µè·¯æ¿

Skywork-13B-Chatï¼š
åœ¨è¿‡å»çš„ä¸€æ®µæ—¶é—´é‡Œï¼Œæˆ‘æœ‰å¹¸å‚åŠ äº†ä¸€æ¬¡ç”µå·¥å®è®­è¯¾ç¨‹ï¼Œè¿™æ®µæ—¶é—´çš„å­¦ä¹ å’Œå®è·µè®©æˆ‘å¯¹ç”µåŠ›å·¥ç¨‹æœ‰äº†æ›´æ·±å…¥çš„ç†è§£å’ŒæŒæ¡ã€‚æˆ‘æƒ³é€šè¿‡è¿™ç¯‡æ–‡ç« åˆ†äº«ä¸€ä¸‹æˆ‘åœ¨å®è®­è¿‡ç¨‹ä¸­çš„å¿ƒå¾—ä½“ä¼šï¼ŒåŒ…æ‹¬ç„Šæ¥ã€åŒè”å¼€å…³æ¥çº¿å’Œç”»ç”µè·¯æ¿ç­‰æ–¹é¢çš„å­¦ä¹ ç»éªŒã€‚
é¦–å…ˆï¼Œæˆ‘è§‰å¾—ç„Šæ¥æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„æŠ€èƒ½ã€‚åœ¨æˆ‘ä»¬çš„è¯¾ç¨‹ä¸­ï¼Œæˆ‘å­¦ä¼šäº†å¦‚ä½•ä½¿ç”¨ç”µçƒ™é“å’Œç„Šé”¡ä¸ï¼Œä»¥åŠå„ç§ç”µçº¿ç”µç¼†çš„è¿æ¥æ–¹æ³•ã€‚è™½ç„¶ä¸€å¼€å§‹æˆ‘å¯¹è¿™ä¸ªè¿‡ç¨‹æ„Ÿåˆ°æœ‰äº›å›°éš¾ï¼Œä½†æ˜¯é€šè¿‡ä¸æ–­çš„å°è¯•å’Œç»ƒä¹ ï¼Œæˆ‘é€æ¸æŒæ¡äº†æŠ€å·§ã€‚æˆ‘äº†è§£åˆ°ç„Šæ¥ä¸ä»…éœ€è¦è€å¿ƒå’Œç»†å¿ƒï¼Œè¿˜éœ€è¦æœ‰ä¸€å®šçš„åŠ›é‡æ§åˆ¶ï¼Œä»¥ç¡®ä¿ç”µè·¯çš„å®‰å…¨æ€§å’Œç¨³å®šæ€§ã€‚åŒæ—¶ï¼Œæˆ‘ä¹Ÿè®¤è¯†åˆ°ï¼Œç„Šæ¥ä¸ä»…ä»…æ˜¯ä¸€ç§æŠ€æœ¯ï¼Œæ›´æ˜¯ä¸€ç§è‰ºæœ¯ï¼Œå®ƒéœ€è¦ä½ æ‡‚å¾—å¦‚ä½•æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ç„Šæ¥çš„åŠ›åº¦å’Œè§’åº¦ï¼Œä»¥å®ç°æœ€ä½³çš„ç»“æœã€‚
å…¶æ¬¡ï¼Œæˆ‘å¯¹åŒè”å¼€å…³æ¥çº¿æœ‰äº†æ–°çš„ç†è§£ã€‚åœ¨è¯¾å ‚ä¸Šï¼Œæˆ‘ä»¬å­¦ä¹ äº†ä¸¤ä¸ªä»¥ä¸Šçš„å¼€å…³å¦‚ä½•å¹¶è”ï¼Œè¿™å¯¹äºæˆ‘æ¥è¯´æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„æœºä¼šï¼Œå¯ä»¥é”»ç‚¼æˆ‘çš„é€»è¾‘æ€ç»´èƒ½åŠ›ã€‚æˆ‘å‘ç°ï¼ŒåŒè”å¼€å…³çš„æ¥çº¿éœ€è¦è€ƒè™‘åˆ°ç”µæµçš„æµå‘ï¼Œä»¥åŠå„ä¸ªå…ƒå™¨ä»¶ä¹‹é—´çš„ç›¸äº’å…³ç³»ã€‚è¿™è¦æ±‚æˆ‘åœ¨æ“ä½œæ—¶å¿…é¡»è°¨æ…ï¼Œç¡®ä¿æ¯ä¸ªå¼€å…³éƒ½èƒ½æ­£å¸¸å·¥ä½œï¼Œå¦åˆ™æ•´ä¸ªç”µè·¯å°±æ— æ³•æ­£å¸¸è¿è¡Œã€‚æˆ‘åœ¨å®è·µä¸­ä½“éªŒåˆ°ï¼Œç†è®ºçŸ¥è¯†å’Œå®é™…æ“ä½œçš„ç»“åˆæ˜¯éå¸¸é‡è¦çš„ï¼Œåªæœ‰è¿™æ ·æ‰èƒ½çœŸæ­£æŒæ¡è¿™ç§æŠ€èƒ½ã€‚
æœ€åï¼Œæˆ‘æ·±æ„Ÿç”»ç”µè·¯æ¿çš„å¤æ‚æ€§ã€‚åœ¨å¼€å§‹å­¦ä¹ ç”»ç”µè·¯æ¿ä¹‹å‰ï¼Œæˆ‘ä»¥ä¸ºè¿™åªæ˜¯ä¸€é¡¹ç®€å•çš„ä»»åŠ¡ï¼Œä½†å®é™…ä¸Šå®ƒæ¶‰åŠåˆ°äº†å¾ˆå¤šçš„åŸç†å’Œç»†èŠ‚ã€‚æˆ‘åœ¨å¯¼å¸ˆçš„æŒ‡å¯¼ä¸‹ï¼Œé€æ­¥å­¦ä¹ äº†å¦‚ä½•ä½¿ç”¨ç»˜å›¾å·¥å…·ï¼Œä»¥åŠå¦‚ä½•æ ¹æ®è®¾è®¡ç»˜åˆ¶å‡ºç²¾ç¡®çš„ç”µè·¯å›¾ã€‚æˆ‘äº†è§£åˆ°ï¼Œæ¯ä¸€ä¸ªå…ƒä»¶çš„ä½ç½®ï¼Œæ¯ä¸€æ ¹ç”µçº¿çš„èµ°å‘ï¼Œéƒ½éœ€è¦ä»”ç»†è€ƒè™‘ï¼Œæ‰èƒ½æœ€ç»ˆå®Œæˆä¸€ä¸ªå®Œæ•´ä¸”åŠŸèƒ½æ­£å¸¸çš„ç”µè·¯æ¿ã€‚è¿™å¯¹æˆ‘æ¥è¯´æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ï¼Œä½†ä¹Ÿæ˜¯ä¸€ä¸ªéå¸¸æœ‰æ„ä¹‰çš„å­¦ä¹ è¿‡ç¨‹ã€‚
é€šè¿‡è¿™æ¬¡å®è®­ï¼Œæˆ‘æ›´åŠ æ·±å…¥åœ°ç†è§£äº†ç”µåŠ›å·¥ç¨‹çš„å®è´¨å’Œé­…åŠ›ã€‚æˆ‘çœ‹åˆ°äº†ç†è®ºçŸ¥è¯†åœ¨å®è·µä¸­çš„åº”ç”¨ï¼Œä¹Ÿå­¦åˆ°äº†å¾ˆå¤šåœ¨æ ¡å›­é‡Œæ— æ³•å­¦åˆ°çš„çŸ¥è¯†å’ŒæŠ€èƒ½ã€‚æˆ‘ç›¸ä¿¡ï¼Œè¿™äº›ç»éªŒå°†ä¼šå¯¹æˆ‘æœªæ¥çš„å­¦ä¹ å’Œå·¥ä½œå¸¦æ¥å¾ˆå¤§çš„å¸®åŠ©ã€‚æ€»çš„æ¥è¯´ï¼Œæˆ‘è§‰å¾—è¿™æ¬¡å®è®­æ˜¯ä¸€æ¬¡éå¸¸å®è´µçš„å­¦ä¹ æœºä¼šï¼Œä½¿æˆ‘å¯¹ç”µåŠ›å·¥ç¨‹æœ‰äº†æ›´æ·±çš„ç†è§£å’Œæé«˜ã€‚æˆ‘æœŸå¾…å°†æ¥èƒ½æœ‰æ›´å¤šçš„æœºä¼šå‚åŠ è¿™æ ·çš„æ´»åŠ¨ï¼Œä»¥æå‡è‡ªå·±çš„æŠ€èƒ½å’ŒçŸ¥è¯†ã€‚

`````

</details>

<details><summary><b>Technology Manuscript</b></summary>

`````
ç”¨æˆ·ï¼š
æœ€è¿‘åœ¨å†™â€œç»¿è‰²ææ–™çš„ç ”ç©¶ä¸å¼€å‘â€ä¸ºé¢˜çš„è®ºæ–‡ï¼Œä½ èƒ½å¸®æˆ‘å†™ä¸€ä¸ªå¤§çº²å—ï¼Ÿ

Skywork-13B-Chatï¼š
æ ‡é¢˜ï¼šç»¿è‰²ææ–™çš„ç ”ç©¶ä¸å¼€å‘

I. å¼•è¨€
 A. èƒŒæ™¯ä»‹ç»
 B. ç›®çš„å’Œé‡è¦æ€§
 C. ç ”ç©¶é—®é¢˜çš„æå‡º
 D. æ–‡ç« ç»“æ„

II. æ–‡çŒ®ç»¼è¿°
 A. ç»¿è‰²ææ–™çš„å®šä¹‰å’Œåˆ†ç±»
 B. ç»¿è‰²ææ–™çš„å‘å±•å†ç¨‹
 C. ç»¿è‰²ææ–™çš„ç ”ç©¶ç°çŠ¶åŠè¶‹åŠ¿

III. ç»¿è‰²ææ–™çš„ç ”ç©¶æ–¹æ³•
 A. å®éªŒè®¾è®¡å’Œæ­¥éª¤
 B. æ•°æ®æ”¶é›†å’Œåˆ†æ
 C. æ¨¡å‹æ„å»ºå’Œè¯„ä¼°
 D. æ¡ˆä¾‹ç ”ç©¶

IV. ç»¿è‰²ææ–™çš„å¼€å‘
 A. æ–°å‹ç»¿è‰²ææ–™çš„åˆ›æ–°
 1. æ–°ææ–™çš„åˆæˆå’Œåˆ¶å¤‡
 2. æ–°ææ–™çš„æ€§èƒ½å’Œåº”ç”¨
 B. æ”¹è¿›ç°æœ‰ç»¿è‰²ææ–™çš„æ–¹æ³•
 1. æé«˜ææ–™æ•ˆç‡
 2. é™ä½ç¯å¢ƒå½±å“
 C. æ”¿ç­–å’Œå¸‚åœºå› ç´ å¯¹ç»¿è‰²ææ–™å¼€å‘çš„å½±å“

V. æˆåŠŸæ¡ˆä¾‹
 A. å…·ä½“æ¡ˆä¾‹æè¿°
 B. æ¡ˆä¾‹åˆ†æ
 C. æ¡ˆä¾‹çš„å¯ç¤ºå’Œæ•™è®­

VI. æŒ‘æˆ˜ä¸æœªæ¥å±•æœ›
 A. å½“å‰ç»¿è‰²ææ–™ç ”ç©¶çš„æŒ‘æˆ˜
 B. æœªæ¥ç ”ç©¶æ–¹å‘
 C. é¢„æœŸæˆæœ

VII. ç»“è®º
 A. æœ¬æ–‡çš„ä¸»è¦å‘ç°å’Œè§‚ç‚¹
 B. å¯¹æœªæ¥ç ”ç©¶çš„å»ºè®®

å‚è€ƒæ–‡çŒ®

è¿™æ˜¯ä¸€ä¸ªåŸºæœ¬çš„å¤§çº²ï¼Œä½ éœ€è¦æ ¹æ®è‡ªå·±çš„ç ”ç©¶è¿›è¡Œä¿®æ”¹å’Œæ‰©å±•ã€‚æ¯ä¸ªéƒ¨åˆ†åº”è¯¥åŒ…å«ä½ çš„ç ”ç©¶çš„å…·ä½“å†…å®¹ã€æ–¹æ³•ã€ç»“æœå’Œè®¨è®ºã€‚ 

ç”¨æˆ·ï¼š
å¸®æˆ‘å†™ä¸€ä¸ªä¼šå‘å…‰çš„è£™å­çš„ä¸“åˆ©è¯´æ˜ä¹¦ï¼Œè£™å­å¤–å½¢ç”±3dæ‰“å°ï¼Œå¤–ä¾§å–·ä¸Šæœ‰å…‰æ³½æ„Ÿçš„æ¶‚å±‚ï¼Œè£™åº•é£è¿½äº†è¿·ä½ ç”µæ± ç»„ä»¶ï¼Œå°†ç”µé”¤æ§åˆ¶å™¨æ‰“å¼€åï¼Œè£™å­å°±ä¼šäº®èµ·æ¥äº†ã€‚

Skywork-13B-Chatï¼š
å‘æ˜åç§°ï¼šä¼šå‘å…‰çš„è£™å­

æŠ€æœ¯é¢†åŸŸï¼š
æœ¬å‘æ˜æ¶‰åŠä¸€ç§å‘å…‰è£…ç½®ï¼Œå°¤å…¶æ˜¯ä¸€ç§é€šè¿‡3Dæ‰“å°åˆ¶é€ ã€å…·æœ‰å‘å…‰æ•ˆæœçš„è£™å­ã€‚è¯¥è£™å­å¯åº”ç”¨äºå„ç§åœºåˆï¼Œå¦‚èˆå°è¡¨æ¼”ã€å¨±ä¹æ´»åŠ¨ç­‰ï¼Œå¢åŠ æ°›å›´å’Œè§†è§‰æ•ˆæœã€‚

èƒŒæ™¯æŠ€æœ¯ï¼š
ç›®å‰ï¼Œå¸‚åœºä¸Šå­˜åœ¨è®¸å¤šå‘å…‰æœé¥°äº§å“ï¼Œä½†å®ƒä»¬å¾€å¾€ä½¿ç”¨LEDç¯ç æˆ–å…¶ä»–å¤–éƒ¨å…‰æºæ¥å®ç°å‘å…‰æ•ˆæœï¼Œè¿™ä½¿å¾—è¿™äº›äº§å“ç¬¨é‡ä¸”ä¸æ–¹ä¾¿æºå¸¦ã€‚æ­¤å¤–ï¼Œè¿™äº›äº§å“çš„åˆ¶ä½œè¿‡ç¨‹å¤æ‚ï¼Œæˆæœ¬è¾ƒé«˜ã€‚å› æ­¤ï¼Œéœ€è¦ä¸€ç§æ›´ä¸ºè½»ä¾¿ã€æ˜“äºä½¿ç”¨çš„å‘å…‰æœé¥°äº§å“ã€‚

å‘æ˜å†…å®¹ï¼š
æœ¬å‘æ˜æä¾›äº†ä¸€ç§ä¼šå‘å…‰çš„è£™å­ï¼Œå…¶åŒ…æ‹¬ä»¥ä¸‹ç»„ä»¶ï¼š
1. 3Dæ‰“å°æˆå‹çš„å¤–å£³ï¼šåˆ©ç”¨3Dæ‰“å°æŠ€æœ¯ï¼Œå¯ä»¥ç²¾ç¡®åœ°å¡‘é€ å‡ºè£™å­çš„å¤–å½¢å’Œç»“æ„ï¼Œä½¿ä¹‹æˆä¸ºä¸€ä¸ªæ•´ä½“ã€‚
2. å…‰æ³½æ„Ÿæ¶‚å±‚ï¼šåœ¨è£™å­è¡¨é¢å–·æ¶‚ä¸€å±‚å…·æœ‰å…‰æ³½æ„Ÿçš„æ¶‚å±‚ï¼Œä¾‹å¦‚é‡‘å±æ¼†æˆ–ç±»ä¼¼ææ–™ï¼Œä»¥å¢åŠ å…¶å¤–è§‚æ•ˆæœã€‚
3. å¾®å‹ç”µæ± ç»„ä»¶ï¼šå®‰è£…åœ¨è£™å­åº•éƒ¨çš„å°å‹ç”µæ± ç»„ä»¶ï¼Œç”¨äºæä¾›ç”µåŠ›ç»™å†…éƒ¨ç”µè·¯ã€‚
4. æ§åˆ¶å™¨ï¼šä½äºè£™å­å†…éƒ¨çš„æ§åˆ¶å™¨ï¼Œç”¨äºæ¥æ”¶ç”µèƒ½å¹¶æ§åˆ¶ç¯å…‰çš„å¼€å…³ã€‚
5. ç¯å…‰ç³»ç»Ÿï¼šåŒ…æ‹¬ç”µæºç®¡ç†ç”µè·¯ã€LEDç¯ç ç­‰ï¼Œç”¨äºå‘å‡ºå¯è§å…‰ã€‚

å®æ–½æ–¹å¼ï¼š
1. é¦–å…ˆï¼Œæ ¹æ®éœ€æ±‚è®¾è®¡è£™å­çš„å¤–å½¢å’Œå°ºå¯¸ï¼Œç„¶åé€šè¿‡3Dæ‰“å°æŠ€æœ¯å°†å…¶åˆ¶æˆæ¨¡å‹ã€‚
2. æ¥ä¸‹æ¥ï¼Œå°†å…‰æ³½æ„Ÿæ¶‚å±‚æ¶‚æŠ¹åœ¨è£™å­è¡¨é¢ï¼Œä½¿å…¶å…·æœ‰æ‰€éœ€çš„å¤–è§‚æ•ˆæœã€‚
3. å°†å¾®å‹ç”µæ± ç»„ä»¶å®‰è£…åœ¨è£™å­åº•éƒ¨ï¼Œä¸æ§åˆ¶å™¨è¿æ¥ã€‚
4. åœ¨è£™å­çš„å…³é”®éƒ¨ä½å®‰è£…LEDç¯ç å’Œå…¶ä»–å¿…éœ€çš„ç”µå­å…ƒä»¶ã€‚
5. æœ€åï¼Œé€šè¿‡æ§åˆ¶å™¨æ‰“å¼€ç”µæºï¼Œä½¿è£™å­å‘å‡ºå…‰èŠ’ã€‚

ä¼˜ç‚¹ï¼š
1. æœ¬å‘æ˜çš„å‘å…‰è£™å­å…·æœ‰è½»å·§ã€ä¾¿æºçš„ç‰¹ç‚¹ï¼Œé€‚åˆå„ç§åœºåˆçš„ä½¿ç”¨ã€‚
2. é€šè¿‡3Dæ‰“å°æŠ€æœ¯åˆ¶é€ ï¼Œå¯ä»¥æ ¹æ®å…·ä½“éœ€æ±‚åˆ¶ä½œå‡ºä¸åŒå½¢çŠ¶å’Œå°ºå¯¸çš„è£™å­ï¼Œæ»¡è¶³ä¸ªæ€§åŒ–éœ€æ±‚ã€‚
3. å…‰æ³½æ„Ÿæ¶‚å±‚ä½¿å¾—è£™å­åœ¨å…‰ç…§ä¸‹å…·æœ‰ç‹¬ç‰¹çš„å¤–è§‚æ•ˆæœï¼Œæå‡äº†è§‚èµä»·å€¼ã€‚
4. å†…ç½®çš„å¾®å‹ç”µæ± ç»„ä»¶ä½¿å¾—è£™å­æ— éœ€å¤–éƒ¨ç”µæºå³å¯ç‹¬ç«‹å·¥ä½œï¼Œæ–¹ä¾¿ä½¿ç”¨è€…æºå¸¦å’Œæ“ä½œã€‚

é™„å›¾è¯´æ˜ï¼š
ç”±äºæœ¬å‘æ˜å°šå¤„äºä¸“åˆ©ç”³è¯·é˜¶æ®µï¼Œæš‚æ— å®ç‰©ç…§ç‰‡æˆ–è®¾è®¡å›¾ã€‚

å…·ä½“å®æ–½æ–¹å¼ï¼š
æœ¬å‘æ˜çš„å…·ä½“å®æ–½æ–¹å¼å¯ä»¥æ ¹æ®å®é™…éœ€æ±‚è¿›è¡Œè°ƒæ•´å’Œå˜åŒ–ï¼Œä½†å¿…é¡»åœ¨ä¸åç¦»å‘æ˜æ€æƒ³çš„å‰æä¸‹è¿›è¡Œã€‚

æƒåˆ©è¦æ±‚ï¼š
1. ä¸€ç§ä¼šå‘å…‰çš„è£™å­ï¼Œå…¶ç‰¹å¾åœ¨äºï¼ŒåŒ…æ‹¬3Dæ‰“å°æˆå‹çš„å¤–å£³ã€å…‰æ³½æ„Ÿæ¶‚å±‚ã€å¾®å‹ç”µæ± ç»„ä»¶ã€æ§åˆ¶å™¨å’Œç¯å…‰ç³»ç»Ÿã€‚
2. æ ¹æ®æƒåˆ©è¦æ±‚1æ‰€è¿°çš„ä¼šå‘å…‰çš„è£™å­ï¼Œå…¶ä¸­ï¼Œæ§åˆ¶å™¨ä½äºè£™å­å†…éƒ¨ï¼Œç”¨äºæ¥æ”¶ç”µèƒ½å¹¶æ§åˆ¶ç¯å…‰çš„å¼€å…³ã€‚
3. æ ¹æ®æƒåˆ©è¦æ±‚1æˆ–2æ‰€è¿°çš„ä¼šå‘å…‰çš„è£™å­ï¼Œå…¶ä¸­ï¼Œç¯å…‰ç³»ç»ŸåŒ…æ‹¬ç”µæºç®¡ç†ç”µè·¯å’ŒLEDç¯ç ç­‰ã€‚
4. æ ¹æ®ä»»ä½•ä¸€é¡¹å‰è¿°æƒåˆ©è¦æ±‚æ‰€è¿°çš„ä¼šå‘å…‰çš„è£™å­ï¼Œå…¶ä¸­ï¼Œå…‰æ³½æ„Ÿæ¶‚å±‚ç”¨äºå¢å¼ºè£™å­çš„å¤–è§‚æ•ˆæœã€‚

è¯´æ˜ä¹¦ç»“æŸã€‚ 

ç”¨æˆ·ï¼š
å°±å±±ç«¹çš„åŠŸæ•ˆä¸ä½œç”¨ï¼Œå†™ä¸€ç¯‡ç§‘æ™®æ–‡ç« ï¼Œçªå‡ºå±±ç«¹çš„è¥å…»ä»·å€¼

Skywork-13B-Chatï¼š
æ ‡é¢˜ï¼šå±±ç«¹çš„åŠŸæ•ˆä¸ä½œç”¨â€”â€”æ¢ç´¢ç¾å‘³æ°´æœçš„è¥å…»ä¸–ç•Œ

åœ¨çƒ­å¸¦åœ°åŒºï¼Œå±±ç«¹æ˜¯ä¸€ç§éå¸¸å—æ¬¢è¿çš„æ°´æœã€‚å®ƒçš„è‚‰è´¨é²œç”œå¤šæ±ï¼Œè¥å…»ä¸°å¯Œï¼Œä¸ä»…å«æœ‰ä¸°å¯Œçš„ç»´ç”Ÿç´ å’ŒçŸ¿ç‰©è´¨ï¼Œè¿˜æœ‰ç‹¬ç‰¹çš„ä¿å¥åŠŸæ•ˆã€‚é‚£ä¹ˆï¼Œå±±ç«¹çš„åŠŸæ•ˆä¸ä½œç”¨æœ‰å“ªäº›å‘¢ï¼Ÿåˆå¦‚ä½•çªæ˜¾å…¶è¥å…»ä»·å€¼å‘¢ï¼Ÿ

é¦–å…ˆï¼Œæˆ‘ä»¬æ¥äº†è§£ä¸€ä¸‹å±±ç«¹çš„åŸºæœ¬ä¿¡æ¯ã€‚å±±ç«¹ï¼Œåˆç§°ä¸ºè½å‰æŸ¿ã€å‡¤æœï¼Œæ˜¯è—¤é»„ç§‘è—¤é»„å±çš„å¸¸ç»¿ä¹”æœ¨ï¼ŒåŸäº§äºä¸œå—äºšã€‚å±±ç«¹çš„æœå®å‘ˆåœ†å½¢æˆ–åµåœ†å½¢ï¼Œå¤–è¡¨çš®çº¢è¤è‰²ï¼Œæœ‰ç‚¹åƒè‹¹æœã€‚æœè‚‰é»„è‰²ï¼Œè´¨åœ°ç»†è…»ï¼Œå‘³é“æ¸…ç”œã€‚æ¯100å…‹å±±ç«¹æœè‚‰ä¸­ï¼Œå¤§çº¦å«æœ‰150å¡è·¯é‡Œçš„èƒ½é‡ï¼Œå¯Œå«é’¾ã€ç£·ã€ç¡«ã€é•ç­‰çŸ¿ç‰©è´¨ï¼Œè¿˜å«æœ‰ä¸°å¯Œçš„è†³é£Ÿçº¤ç»´å’ŒæŠ—æ°§åŒ–ç‰©è´¨ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æ¥çœ‹çœ‹å±±ç«¹çš„ä¸»è¦åŠŸæ•ˆä¸ä½œç”¨ã€‚

1. é™æ¸©è§£çƒ­ï¼šå±±ç«¹æ€§å¯’ï¼Œå‘³ç”˜ï¼Œå…·æœ‰è‰¯å¥½çš„æ¸…çƒ­è§£æ¯’ä½œç”¨ã€‚å¤å¤©é£Ÿç”¨å¯ä»¥æœ‰æ•ˆé¢„é˜²ä¸­æš‘ï¼Œé™ä½ä½“å†…çš„çƒ­é‡ã€‚

2. ç¼“è§£ä¾¿ç§˜ï¼šå±±ç«¹ä¸­å«æœ‰ä¸°å¯Œçš„è†³é£Ÿçº¤ç»´ï¼Œèƒ½ä¿ƒè¿›è‚ é“è •åŠ¨ï¼Œæ”¹å–„ä¾¿ç§˜ç—‡çŠ¶ã€‚

3. ä¿æŠ¤çš®è‚¤ï¼šå±±ç«¹å«æœ‰ä¸°å¯Œçš„ç»´ç”Ÿç´ Cå’ŒEï¼Œå…·æœ‰å¾ˆå¥½çš„æŠ—æ°§åŒ–ä½œç”¨ï¼Œæœ‰åŠ©äºä¿æŠ¤çš®è‚¤å¥åº·ã€‚

4. å¢å¼ºå…ç–«åŠ›ï¼šå±±ç«¹å«æœ‰å¤šç§ç»´ç”Ÿç´ å’ŒçŸ¿ç‰©è´¨ï¼Œå°¤å…¶æ˜¯ç»´ç”Ÿç´ Cï¼Œå…·æœ‰å¾ˆå¥½çš„æŠ—ç—…æ¯’ä½œç”¨ï¼Œèƒ½æé«˜äººä½“å…ç–«åŠ›ã€‚

5. å‡è‚¥ç¾å®¹ï¼šç”±äºå±±ç«¹çš„ä½ç³–ã€ä½è„‚ï¼Œå› æ­¤æˆä¸ºäº†å‡è‚¥äººå£«çš„ç†æƒ³é€‰æ‹©ã€‚åŒæ—¶ï¼Œå±±ç«¹çš„æŠ—æ°§åŒ–ä½œç”¨ä¹Ÿæœ‰åŠ©äºå»¶ç¼“è¡°è€ï¼Œä½¿è‚Œè‚¤ä¿æŒå¹´è½»ã€‚

é‚£ä¹ˆï¼Œå¦‚ä½•çªæ˜¾å±±ç«¹çš„è¥å…»ä»·å€¼å‘¢ï¼Ÿ

1. ä¸å…¶ä»–æ°´æœç›¸æ¯”ï¼Œå±±ç«¹çš„å«ç³–é‡è¾ƒä½ï¼Œçƒ­é‡é€‚ä¸­ï¼Œè¥å…»å¯†åº¦é«˜ã€‚è¿™æ„å‘³ç€åƒå°‘é‡çš„å±±ç«¹å°±èƒ½è·å–å¤§é‡çš„è¥å…»ï¼Œæœ‰åŠ©äºæ§åˆ¶çƒ­é‡æ‘„å…¥ã€‚

2. å±±ç«¹å«æœ‰ä¸°å¯Œçš„è†³é£Ÿçº¤ç»´ï¼Œèƒ½å¸®åŠ©æ¶ˆåŒ–ï¼Œé™ä½èƒ†å›ºé†‡ï¼Œé¢„é˜²å¿ƒè¡€ç®¡ç–¾ç—…ã€‚

3. å±±ç«¹å«æœ‰ä¸°å¯Œçš„ç»´ç”Ÿç´ Cå’ŒEï¼Œå…·æœ‰å¾ˆå¥½çš„æŠ—æ°§åŒ–ä½œç”¨ï¼Œèƒ½æŠµæŠ—è‡ªç”±åŸºå¯¹èº«ä½“çš„æŸå®³ã€‚

4. å±±ç«¹ä¸­çš„çŸ¿ç‰©è´¨ï¼Œå¦‚é’¾ã€ç£·ã€ç¡«ç­‰ï¼Œå¯¹äºç»´æŒèº«ä½“æ­£å¸¸çš„ç”Ÿç†åŠŸèƒ½æœ‰å¾ˆå¤§çš„å¸®åŠ©ã€‚

æ€»çš„æ¥è¯´ï¼Œå±±ç«¹ä¸ä»…ä»…æ˜¯ä¸€ç§ç¾é£Ÿï¼Œæ›´æ˜¯å¤§è‡ªç„¶èµ‹äºˆäººç±»çš„ä¸€ä»½ç‘°å®ã€‚å®ƒæ—¢æ»¡è¶³äº†æˆ‘ä»¬çš„å£è…¹ä¹‹æ¬²ï¼Œåˆæä¾›äº†ä¸°å¯Œçš„è¥å…»ã€‚è®©æˆ‘ä»¬åœ¨äº«å—ç¾å‘³çš„åŒæ—¶ï¼Œä¹Ÿèƒ½æ‘„å–åˆ°ä¸°å¯Œçš„è¥å…»ã€‚

æœ€åï¼Œæˆ‘ä»¬è¦æé†’å¤§å®¶ï¼Œè™½ç„¶å±±ç«¹è¥å…»ä¸°å¯Œï¼Œä½†ä¹Ÿæ˜¯æ°´æœï¼Œæ‰€ä»¥åœ¨é£Ÿç”¨æ—¶åº”é€‚é‡ã€‚å¯¹äºå®¹æ˜“è…¹æ³»çš„äººç¾¤ï¼Œå»ºè®®æ§åˆ¶é£Ÿç”¨é‡æˆ–è€…é¿å…ç”Ÿé£Ÿã€‚å¸Œæœ›å¤§å®¶åœ¨äº«å—ç¾é£Ÿçš„åŒæ—¶ï¼Œä¹Ÿèƒ½å…³æ³¨åˆ°å¥åº·ã€‚

`````

</details>

<details><summary><b>Record Text</b></summary>

`````
ç”¨æˆ·ï¼š
å†™ä¸€ç¯‡å…³äºæˆ¿åœ°äº§çš„å·¥ä½œæ—¥å¿—

Skywork-13B-Chatï¼š
æ—¥æœŸï¼š xxxxå¹´xxæœˆxxæ—¥

å¤©æ°”ï¼šæ™´

ä»Šå¤©æ˜¯ä¸€ä¸ªç¹å¿™çš„æ—¥å­ï¼Œå› ä¸ºæˆ‘ä»£è¡¨å…¬å¸ä¸å‡ ä½æ½œåœ¨çš„è´­æˆ¿è€…è¿›è¡Œäº†æ·±å…¥çš„è°ˆåˆ¤ã€‚æ—©ä¸Šï¼Œæˆ‘ä¸ä¹°å®¶Aåœ¨å…¬å¸ä¼šè®®å®¤å°±ä»–ä»¬æ„Ÿå…´è¶£çš„ä¸€å¥—å…¬å¯“è¿›è¡Œäº†è¯¦ç»†çš„è®¨è®ºã€‚æˆ‘ä»¬ä»åœ°ç†ä½ç½®ã€å‘¨è¾¹è®¾æ–½ã€æˆ·å‹è®¾è®¡å’Œè£…ä¿®é£æ ¼ç­‰æ–¹é¢è¿›è¡Œäº†å…¨é¢çš„äº¤æµï¼Œå¹¶æ ¹æ®ä»–ä»¬çš„éœ€æ±‚è°ƒæ•´äº†æˆ¿å±‹é”€å”®æ–¹æ¡ˆã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬è¾¾æˆäº†åˆæ­¥çš„åè®®ï¼Œè¿™è®©æˆ‘æ„Ÿåˆ°éå¸¸å…´å¥‹ã€‚

ä¸Šåˆ10ç‚¹ï¼Œæˆ‘ä¼šè§äº†å¦ä¸€ä½æ½œåœ¨å®¢æˆ·Bï¼Œä»–å¯¹æˆ‘ä»¬çš„ä¸€æ ‹è±ªååˆ«å¢…äº§ç”Ÿäº†æµ“åšçš„å…´è¶£ã€‚æˆ‘ä»¬è®¨è®ºäº†ä¸åŒçš„æˆ¿å‹å’Œä»·æ ¼é€‰é¡¹ï¼Œå¹¶å°±åœŸåœ°æ‰€æœ‰æƒå’Œç‰©ä¸šç®¡ç†ç­‰é—®é¢˜è¿›è¡Œäº†æ¢è®¨ã€‚å®¢æˆ·Bæå‡ºäº†ä¸€äº›æœ‰æ·±åº¦çš„é—®é¢˜ï¼Œæˆ‘é€šè¿‡è€å¿ƒè§£ç­”å’Œè§£å†³æ–¹æ¡ˆï¼ŒæˆåŠŸåœ°è§£å†³äº†ä»–çš„ç–‘è™‘ã€‚

ä¸‹åˆ2ç‚¹ï¼Œæˆ‘å‚åŠ äº†ä¸€åœºå…³äºåœ°äº§å¸‚åœºè¶‹åŠ¿çš„ç”µè¯ä¼šè®®ï¼Œä¸»è¦æ˜¯ä¸ºäº†äº†è§£æœ€æ–°çš„å¸‚åœºåŠ¨æ€å’Œç«äº‰å¯¹æ‰‹çš„æƒ…å†µã€‚è¿™å¯¹æˆ‘ä»Šåçš„å·¥ä½œéå¸¸æœ‰å¸®åŠ©ï¼Œå› ä¸ºå®ƒä½¿æˆ‘èƒ½å¤Ÿæ›´å¥½åœ°å®šä½æˆ‘ä»¬çš„æˆ¿äº§é¡¹ç›®ï¼Œå¹¶ä¸ºå®¢æˆ·æä¾›æ›´ä¼˜è´¨çš„æœåŠ¡ã€‚

ä¸‹åˆ3ç‚¹ï¼Œæˆ‘å¤„ç†äº†ä¸€äº›æ–‡ä»¶ï¼ŒåŒ…æ‹¬æˆ¿å±‹è´·æ¬¾ç”³è¯·å’Œäº§æƒè½¬ç§»æ‰‹ç»­ã€‚è™½ç„¶è¿™äº›å·¥ä½œå¾ˆç¹çï¼Œä½†æˆ‘æ„è¯†åˆ°å®ƒä»¬å¯¹äºäº¤æ˜“çš„å®Œæˆè‡³å…³é‡è¦ã€‚æˆ‘ç¡®ä¿æ‰€æœ‰çš„æ–‡ä»¶éƒ½å‡†ç¡®æ— è¯¯ï¼Œä»¥é¿å…ä»»ä½•å»¶è¯¯æˆ–é”™è¯¯ã€‚

å‚æ™š6ç‚¹ï¼Œæˆ‘å›åˆ°åŠå…¬å®¤ï¼Œæ•´ç†äº†ä¸€ä¸‹æ˜å¤©è¦å¤„ç†çš„é‚®ä»¶å’Œæ›´æ–°äº†æˆ¿æºä¿¡æ¯ã€‚æˆ‘è¿˜é¢„çº¦äº†ä¸€ä¸ªå®¢æˆ·Cï¼Œä»–å°†è¿‡æ¥å‚è§‚æˆ‘ä»¬çš„æ ·æ¿æˆ¿ã€‚æˆ‘é¢„è®¡ä»–ä¼šå¯¹æˆ‘ä»¬çš„æ–°å¼€å‘é¡¹ç›®äº§ç”Ÿæµ“åšçš„å…´è¶£ï¼Œæ‰€ä»¥æˆ‘å·²ç»åšå¥½äº†å……åˆ†çš„å‡†å¤‡ã€‚

æ™šä¸Š8ç‚¹ï¼Œæˆ‘ç»“æŸäº†ä»Šå¤©çš„å·¥ä½œï¼Œå›é¡¾äº†ä¸€ä¸‹ä»Šå¤©çš„å·¥ä½œæˆæœã€‚è™½ç„¶å¿™ç¢Œï¼Œä½†æˆ‘è§‰å¾—éå¸¸å……å®ã€‚æœŸå¾…æ˜å¤©èƒ½å¸¦æ¥æ›´å¤šçš„æœºä¼šå’ŒæŒ‘æˆ˜ã€‚

ä»Šæ—¥æ€»ç»“ï¼šä»Šå¤©çš„å·¥ä½œä¸»è¦é›†ä¸­åœ¨ä¸å®¢æˆ·çš„æ²Ÿé€šã€å¸‚åœºç ”ç©¶å’Œæ–‡ä»¶å¤„ç†ä¸Šã€‚å°½ç®¡ä»»åŠ¡ç¹é‡ï¼Œä½†æˆ‘å¾ˆäº«å—è¿™ç§å¿™ç¢Œçš„æ„Ÿè§‰ï¼Œå› ä¸ºå®ƒè®©æˆ‘æ˜ç™½æˆ‘æ­£åœ¨ä¸ºå®ç°å…¬å¸çš„ç›®æ ‡è€ŒåŠªåŠ›ã€‚æ˜å¤©æˆ‘å°†ç»§ç»­è‡´åŠ›äºå¯»æ‰¾æ›´å¤šçš„æ½œåœ¨å®¢æˆ·ï¼Œå¹¶å°½åŠ›æ»¡è¶³ä»–ä»¬çš„éœ€æ±‚ã€‚ 

ç”¨æˆ·ï¼š
è¯·ä¸ºæˆ‘åˆ›ä½œä¸€ç¯‡å…³äºåŸå¸‚äº¤é€šè§„åˆ’çš„å®è·µæŠ¥å‘Šï¼Œå†…å®¹è¦æ¶‰åŠåˆ°äº¤é€šæµé‡ã€å…¬å…±äº¤é€šä¼˜åŠ¿å’Œå¸‚æ°‘å‡ºè¡Œä¹ æƒ¯ï¼Œä¸å°‘äº800å­—

Skywork-13B-Chatï¼š
æ ‡é¢˜ï¼šåŸå¸‚äº¤é€šè§„åˆ’å®è·µæŠ¥å‘Šï¼šèšç„¦äº¤é€šæµé‡ã€å…¬å…±äº¤é€šä¼˜åŠ¿ä¸å¸‚æ°‘å‡ºè¡Œä¹ æƒ¯

1. å¼•è¨€

éšç€åŸå¸‚åŒ–è¿›ç¨‹çš„é£é€Ÿå‘å±•ï¼ŒåŸå¸‚äººå£å’Œäº¤é€šéœ€æ±‚æ—¥ç›Šå¢é•¿ï¼Œå…¬å…±äº¤é€šä½œä¸ºåŸå¸‚äº¤é€šçš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œå¯¹äºè§£å†³åŸå¸‚äº¤é€šæ‹¥å µé—®é¢˜ï¼Œæé«˜åŸå¸‚è¿è¾“æ•ˆç‡å’Œå¯æŒç»­å‘å±•å…·æœ‰é‡è¦æ„ä¹‰ã€‚æœ¬æŠ¥å‘Šå°†ä»¥æŸåŸå¸‚ä¸ºä¾‹ï¼Œå¯¹å…¶äº¤é€šæµé‡ã€å…¬å…±äº¤é€šä¼˜åŠ¿åŠå¸‚æ°‘å‡ºè¡Œä¹ æƒ¯è¿›è¡Œåˆ†æï¼Œå¹¶æå‡ºç›¸åº”çš„ä¼˜åŒ–å»ºè®®ã€‚

2. äº¤é€šæµé‡åˆ†æ

è¯¥åŸå¸‚ç›®å‰çš„äº¤é€šæµé‡å‘ˆç°é€å¹´ä¸Šå‡çš„è¶‹åŠ¿ã€‚åœ¨å·¥ä½œæ—¥æ—©é«˜å³°æ—¶æœŸï¼Œé“è·¯äº¤é€šæµé‡è¾ƒå¤§ï¼Œä¸»è¦é›†ä¸­åœ¨ä¸»å¹²é“å’Œç¹ååœ°æ®µï¼›åœ¨æ™šé«˜å³°æ—¶æœŸï¼Œäº¤é€šæµé‡æœ‰æ‰€ç¼“è§£ï¼Œä½†ä»å­˜åœ¨ä¸€å®šç¨‹åº¦çš„æ‹¥å µç°è±¡ã€‚åœ°é“ã€å…¬äº¤ç­‰è½¨é“äº¤é€šåœ¨ç¼“è§£äº¤é€šå‹åŠ›ä¸Šå‘æŒ¥äº†ç§¯æä½œç”¨ï¼Œä½†ä»é¢ä¸´ç€å®¢è¿é‡ä¸è¶³ã€çº¿è·¯è¦†ç›–ä¸å…¨ç­‰é—®é¢˜ã€‚

3. å…¬å…±äº¤é€šä¼˜åŠ¿

å°½ç®¡è¯¥åŸå¸‚çš„å…¬å…±äº¤é€šç³»ç»Ÿä¸æ–­å®Œå–„ï¼Œä½†ä¸ç§å®¶è½¦ã€å‡ºç§Ÿè½¦ç­‰ä¸ªä½“äº¤é€šå·¥å…·ç›¸æ¯”ï¼Œå…¬å…±äº¤é€šçš„ä¼˜åŠ¿ä»ç„¶ä¸æ˜æ˜¾ã€‚é¦–å…ˆï¼Œå…¬å…±äº¤é€šçš„è¿è¡Œæ—¶é—´è¾ƒä¸ºå›ºå®šï¼Œæ— æ³•æ»¡è¶³éƒ¨åˆ†å¸‚æ°‘çš„ä¸ªæ€§åŒ–éœ€æ±‚ï¼›å…¶æ¬¡ï¼Œå…¬å…±äº¤é€šçš„ç­æ¬¡å’Œç«™ç‚¹åˆ†å¸ƒæœ‰å¾…æ”¹è¿›ï¼Œå°¤å…¶æ˜¯å¤œé—´å’Œæ¸…æ™¨çš„æœåŠ¡å°šéœ€åŠ å¼ºï¼›æœ€åï¼Œå…¬å…±äº¤é€šç¥¨ä»·åé«˜ï¼Œå¯¼è‡´éƒ¨åˆ†å¸‚æ°‘é€‰æ‹©å…¶ä»–äº¤é€šæ–¹å¼ã€‚

4. å¸‚æ°‘å‡ºè¡Œä¹ æƒ¯

é€šè¿‡å¯¹å¸‚æ°‘å‡ºè¡Œä¹ æƒ¯çš„è°ƒæŸ¥å‘ç°ï¼Œå¤§éƒ¨åˆ†å¸‚æ°‘è®¤ä¸ºå…¬å…±äº¤é€šçš„ä¼˜åŠ¿åœ¨äºç¯ä¿ã€å¿«æ·å’Œç»æµã€‚ç„¶è€Œï¼Œå…¬å…±äº¤é€šçš„å¸å¼•åŠ›è¿˜å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œå¦‚è·¯çº¿ä¸å¤Ÿä¾¿æ·ã€æ¢ä¹˜ä¸ä¾¿ã€æœåŠ¡è´¨é‡ä¸é«˜å’Œè®¾æ–½è€æ—§ç­‰ã€‚æ­¤å¤–ï¼Œæ­¥è¡Œå’Œéª‘è¡Œç­‰éæœºåŠ¨å‡ºè¡Œæ–¹å¼åœ¨è¯¥åŸå¸‚ä¸­å æ¯”è¾ƒå¤§ï¼Œä½†å—åˆ°ç¯å¢ƒå’Œå®‰å…¨å› ç´ çš„å½±å“ã€‚

5. ç»“è®ºä¸å»ºè®®

é’ˆå¯¹ä¸Šè¿°é—®é¢˜ï¼Œæœ¬æŠ¥å‘Šæå‡ºä»¥ä¸‹å‡ ç‚¹å»ºè®®ï¼š

5.1 ä¼˜åŒ–å…¬å…±äº¤é€šçº¿è·¯å¸ƒå±€ï¼Œæé«˜è¦†ç›–é¢ç§¯ï¼ŒåŠ å¼ºå¤œé—´å’Œæ¸…æ™¨çš„æœåŠ¡è´¨é‡ï¼Œæé«˜å¸‚æ°‘å¯¹å…¬å…±äº¤é€šçš„ä¾èµ–æ€§ã€‚

5.2 æ”¿åºœåŠ å¤§å¯¹å…¬å…±äº¤é€šçš„æŠ•å…¥ï¼Œæå‡å…¬å…±äº¤é€šçš„æœåŠ¡æ°´å¹³ï¼Œå¦‚å¢åŠ ç­æ¬¡ã€æé«˜è½¦è¾†èˆ’é€‚åº¦ã€å¢è®¾æ–°ç«™ç­‰ã€‚

5.3 é¼“åŠ±å¸‚æ°‘æ”¹å˜å‡ºè¡Œæ–¹å¼ï¼Œå¦‚éª‘è‡ªè¡Œè½¦ã€ä¹˜åç½‘çº¦è½¦ç­‰ï¼Œå‡å°‘ç§å®¶è½¦å’Œæ‘©æ‰˜è½¦çš„ä½¿ç”¨ï¼Œä»è€Œå‡è½»äº¤é€šå‹åŠ›ã€‚

5.4 åŠ å¼ºäº¤é€šç®¡ç†ï¼Œæé«˜äº¤é€šæ‰§æ³•åŠ›åº¦ï¼Œè§„èŒƒäº¤é€šè¡Œä¸ºï¼Œç¡®ä¿é“è·¯ç•…é€šã€‚

5.5 å¼•å…¥æ™ºèƒ½åŒ–æŠ€æœ¯ï¼Œå¦‚æ™ºèƒ½äº¤é€šç³»ç»Ÿã€æ— äººé©¾é©¶ç­‰ï¼Œæé«˜äº¤é€šè¿è¾“æ•ˆç‡å’Œå®‰å…¨æ€§ã€‚

6. ç»“è¯­

åŸå¸‚äº¤é€šè§„åˆ’éœ€è¦ç»¼åˆè€ƒè™‘äº¤é€šæµé‡ã€å…¬å…±äº¤é€šä¼˜åŠ¿å’Œå¸‚æ°‘å‡ºè¡Œä¹ æƒ¯ç­‰å¤šæ–¹é¢å› ç´ ï¼Œä»¥äººä¸ºæœ¬ï¼Œæ³¨é‡ç»†èŠ‚ï¼Œæ‰èƒ½å®ç°åŸå¸‚äº¤é€šçš„é«˜æ•ˆã€ç¯ä¿å’Œå¯æŒç»­å‘å±•ã€‚æˆ‘ä»¬æœŸå¾…çœ‹åˆ°æ›´å¤šå®é™…æªæ–½å’Œæ”¿ç­–ï¼Œæ¨åŠ¨åŸå¸‚äº¤é€šå‘æ›´å¥½çš„æ–¹å‘å‘å±•ã€‚ 

ç”¨æˆ·ï¼š
æœ¬äººå¤–è´¸å…¬å¸ä»äº‹æ ·å“ç®¡ç†å·¥ä½œ,æ±‚ä¸€ä»½å¹´åº¦æ€»ç»“æŠ¥å‘Š

Skywork-13B-Chatï¼š
å¹´åº¦æ€»ç»“æŠ¥å‘Š
å°Šæ•¬çš„é¢†å¯¼ï¼ŒåŒäº‹ä»¬ï¼š

åœ¨è¿‡å»çš„ä¸€å¹´é‡Œï¼Œåœ¨å…¬å¸é¢†å¯¼å’Œå„ä½åŒäº‹çš„å¤§åŠ›æ”¯æŒä¸‹ï¼Œæˆ‘æœ‰å¹¸å‚ä¸å¹¶è´Ÿè´£æ ·å“ç®¡ç†å·¥ä½œï¼Œä¸ºå…¬å¸çš„å¯¹å¤–è´¸æ˜“æ´»åŠ¨æä¾›äº†æœ‰æ•ˆçš„æ ·å“ä¿éšœã€‚é€šè¿‡ä¸€å¹´æ¥çš„åŠªåŠ›å’Œå®è·µï¼Œæˆ‘å¯¹æ ·å“ç®¡ç†å·¥ä½œæœ‰äº†æ›´æ·±å…¥çš„è®¤è¯†å’Œç†è§£ï¼ŒåŒæ—¶ä¹Ÿå–å¾—äº†ä¸€å®šçš„æˆç»©ã€‚ç°å°†2019å¹´åº¦çš„å·¥ä½œæ€»ç»“å¦‚ä¸‹ï¼š

ä¸€ã€ç†Ÿæ‚‰ä¸šåŠ¡æµç¨‹ï¼Œæ˜ç¡®å·¥ä½œèŒè´£

åœ¨æ—¥å¸¸å·¥ä½œä¸­ï¼Œæˆ‘è®¤çœŸå­¦ä¹ å’Œç ”ç©¶å…¬å¸çš„å„é¡¹ä¸šåŠ¡æµç¨‹ï¼ŒæŒæ¡äº†æ ·å“çš„æ¥æ”¶ã€å…¥åº“ã€å‡ºåº“ã€ç›˜ç‚¹ç­‰æ“ä½œè§„ç¨‹ï¼Œæ˜ç¡®äº†è‡ªå·±çš„å·¥ä½œèŒè´£å’Œå·¥ä½œç›®æ ‡ã€‚åŒæ—¶ï¼Œæˆ‘è¿˜å­¦ä¼šäº†åˆ©ç”¨ç”µè„‘è½¯ä»¶è¿›è¡Œæ ·å“ä¿¡æ¯çš„å½•å…¥å’Œç®¡ç†ï¼Œæé«˜äº†å·¥ä½œæ•ˆç‡ã€‚

äºŒã€ä¸¥æ ¼æ ·å“ç®¡ç†ï¼Œç¡®ä¿è´¨é‡æ§åˆ¶

æˆ‘æ·±çŸ¥æ ·å“è´¨é‡æ˜¯ä¼ä¸šçš„ç”Ÿå‘½çº¿ï¼Œå› æ­¤åœ¨å·¥ä½œä¸­ï¼Œæˆ‘å§‹ç»ˆåšæŒä¸¥æ ¼æ ·å“ç®¡ç†ï¼ŒæŠŠå¥½è´¨é‡å…³ã€‚é¦–å…ˆï¼Œåœ¨æ¥æ”¶æ ·å“æ—¶ï¼Œæˆ‘è®¤çœŸæ ¸å¯¹ä¾›åº”å•†æä¾›çš„æ ·å“ä¿¡æ¯ï¼Œç¡®ä¿ä¸å®é™…ç›¸ç¬¦ï¼›å…¶æ¬¡ï¼Œåœ¨å…¥åº“è¿‡ç¨‹ä¸­ï¼Œæˆ‘å¯¹æ ·å“è¿›è¡Œåˆ†ç±»å­˜æ”¾ï¼Œä¿è¯æ ·å“çš„è´¨é‡ä¸å—æŸå¤±ï¼›æœ€åï¼Œåœ¨å‡ºåº“æ—¶ï¼Œæˆ‘æ ¹æ®å®¢æˆ·è®¢å•å’Œä¸šåŠ¡å‘˜éœ€æ±‚ï¼Œå‡†ç¡®åŠæ—¶åœ°æä¾›æ ·å“ï¼Œç¡®ä¿æ»¡è¶³å®¢æˆ·éœ€æ±‚ã€‚

ä¸‰ã€ä¼˜åŒ–åº“å­˜ç»“æ„ï¼Œé™ä½åº“å­˜æˆæœ¬

ä¸ºäº†é™ä½åº“å­˜æˆæœ¬ï¼Œæé«˜èµ„é‡‘å‘¨è½¬ç‡ï¼Œæˆ‘ç§¯æå‚ä¸åˆ¶å®šåº“å­˜ä¼˜åŒ–æ–¹æ¡ˆï¼Œå»ºè®®å’Œå®æ–½äº†ä¸€äº›æªæ–½ï¼Œå¦‚å®šæœŸæ¸…ç†æ»é”€æ ·å“ã€ä¸ä¾›åº”å•†åå•†å‡å°‘åº“å­˜ç§¯å‹ç­‰ã€‚è¿™ä¸ä»…å‡å°‘äº†åº“å­˜å‹åŠ›ï¼Œè¿˜ä¸ºå…¬å¸èŠ‚çœäº†å¤§é‡çš„ä»“å‚¨è´¹ç”¨ã€‚

å››ã€åŠ å¼ºä¸å„éƒ¨é—¨æ²Ÿé€šåä½œï¼Œæé«˜æœåŠ¡æ°´å¹³

åœ¨å·¥ä½œä¸­ï¼Œæˆ‘åŠ å¼ºä¸é‡‡è´­éƒ¨ã€ç”Ÿäº§éƒ¨ã€è´¢åŠ¡éƒ¨ç­‰éƒ¨é—¨çš„æ²Ÿé€šä¸åä½œï¼Œç¡®ä¿æ ·å“ä¿¡æ¯çš„å®æ—¶å…±äº«å’Œæµé€šã€‚é‡åˆ°é—®é¢˜æ—¶ï¼Œæˆ‘èƒ½ç§¯æä¸»åŠ¨åè°ƒè§£å†³ï¼Œæé«˜äº†å·¥ä½œæ•ˆç‡ï¼Œé™ä½äº†å‡ºé”™ç‡ã€‚

äº”ã€æ€»ç»“

å›é¡¾ä¸€å¹´æ¥çš„å·¥ä½œï¼Œæˆ‘æ·±æ„Ÿè´£ä»»é‡å¤§ï¼Œä»»åŠ¡ç¹é‡ã€‚åœ¨çœ‹åˆ°æˆç»©çš„åŒæ—¶ï¼Œä¹Ÿå‘ç°äº†è‡ªèº«çš„ä¸è¶³ã€‚åœ¨æ–°çš„ä¸€å¹´é‡Œï¼Œæˆ‘å°†è¿›ä¸€æ­¥åŠ å¼ºè‡ªèº«å­¦ä¹ ï¼Œä¸æ–­æé«˜å·¥ä½œèƒ½åŠ›ï¼Œæ”¹è¿›å·¥ä½œæ–¹æ³•ï¼Œä»¥æ›´é«˜çš„æ ‡å‡†è¦æ±‚è‡ªå·±ï¼Œä¸ºå…¬å¸çš„å‘å±•è´¡çŒ®è‡ªå·±çš„åŠ›é‡ã€‚

è°¢è°¢å¤§å®¶ï¼

[æ‚¨çš„åå­—]
[æ—¥æœŸ] 

`````

</details>

<details><summary><b>Comments</b></summary>

`````
ç”¨æˆ·ï¼š
ä¸ºå²é“ç”Ÿçš„ã€Šæˆ‘ä¸åœ°å›ã€‹å†™ä¸€ç¯‡300å­—å·¦å³çš„ä¹¦è¯„

Skywork-13B-Chatï¼š
ã€Šæˆ‘ä¸åœ°å›ã€‹æ˜¯å²é“ç”Ÿçš„ä»£è¡¨ä½œï¼Œä¹Ÿæ˜¯ä»–åäº”å¹´æ¥æ‘‡ç€è½®æ¤…åœ¨åœ°å›æ€ç´¢çš„ç»“æ™¶ã€‚è¿™ç¯‡æ–‡ç« ä»¥æ·±æƒ…çš„ç¬”è§¦æç»˜äº†ä½œè€…ä¸åœ°å›çš„æ·±åšå…³ç³»ï¼Œä»¥åŠä»–åœ¨è¿™ä¸ªå¤§åƒä¸–ç•Œä¸­æ‰€é¢†æ‚Ÿåˆ°çš„äººç”Ÿå“²ç†ã€‚åœ°å›æ—¢æ˜¯ä»–ç—›è‹¦çš„æ ¹æºï¼Œä¹Ÿæ˜¯ä»–çµæ„Ÿçš„æºæ³‰ï¼›æ—¢æ˜¯ä»–å‘æ³„æƒ…æ„Ÿçš„åœ°æ–¹ï¼Œä¹Ÿæ˜¯ä»–è·å¾—åŠ›é‡çš„æºæ³‰ã€‚åœ¨è¿™é‡Œï¼Œä»–ä½“éªŒäº†äººç”Ÿçš„é…¸ç”œè‹¦è¾£ï¼Œä¹Ÿæ‰¾åˆ°äº†è‡ªæˆ‘è§£è„±çš„æ–¹å¼ã€‚è¿™ç¯‡æ–‡ç« ä¸ä»…å±•ç°äº†ä»–å¯¹æ¯äº²çš„æ·±æ·±æ€€å¿µï¼Œè¿˜ä½“ç°äº†ä»–å¯¹ç”Ÿå‘½çš„ç‹¬ç‰¹ç†è§£å’Œæ„Ÿæ‚Ÿï¼Œè®©æˆ‘ä»¬æ·±åˆ»åœ°ç†è§£åˆ°äº†äººç”Ÿçš„æ„ä¹‰åœ¨äºç»å†ä¸æ„Ÿæ‚Ÿã€‚å²é“ç”Ÿä»¥å…¶ç‹¬ç‰¹çš„è§†è§’å’Œæ·±æ²‰çš„æ„Ÿæƒ…ï¼Œå°†åœ°å›çš„è’èŠœä¸è¡°è´¥è½¬åŒ–ä¸ºå¯¹ç”Ÿå‘½çš„æ„Ÿæ‚Ÿå’Œå¯¹æœªæ¥çš„å¸Œæœ›ï¼Œå……åˆ†å±•ç¤ºäº†ä»–çš„äººæ–‡å…³æ€€å’ŒåšéŸ§ä¸æ‹”çš„ç²¾ç¥ã€‚è¿™ç¯‡æ–‡ç« å……æ»¡äº†å“²æ€ï¼Œè¯»åè®©äººæ·±å—è§¦åŠ¨å’Œå¯å‘ã€‚ 

ç”¨æˆ·ï¼š
æˆ‘æœ‰ä¸€ä¸ªè€å¸ˆï¼Œä¸Šè¯¾å¹½é»˜é£è¶£ï¼Œæˆ‘ä»¬éƒ½å¾ˆå–œæ¬¢ä»–ï¼Œç»™ä»–ä¸€ä¸ªè¯„è¯­

Skywork-13B-Chatï¼š
æ‚¨æ˜¯ä¸€ä½éå¸¸å‡ºè‰²çš„è€å¸ˆï¼Œå­¦ç”Ÿä»¬å¯¹æ‚¨çš„å¹½é»˜æ„Ÿå’Œè¯¾å ‚äº’åŠ¨æ„Ÿåˆ°éå¸¸æ¬£èµã€‚æ‚¨æ€»æ˜¯èƒ½å¤Ÿå°†è¯¾ç¨‹å†…å®¹ä»¥æœ‰è¶£ä¸”æ˜“äºç†è§£çš„æ–¹å¼å‘ˆç°å‡ºæ¥ï¼Œä½¿å­¦ç”Ÿä»¬æ›´åŠ ä¸“æ³¨å’Œç§¯æåœ°å‚ä¸å­¦ä¹ ã€‚æ‚¨çš„æ•™å­¦é£æ ¼æ¿€åŠ±äº†æˆ‘ä»¬çš„å…´è¶£ï¼Œå¹¶æ¿€å‘äº†æˆ‘ä»¬å¯¹è¯¥é¢†åŸŸçš„çƒ­æƒ…ã€‚æ‚¨ä¸ä»…æ˜¯ä¸€ä½æ°å‡ºçš„æ•™è‚²è€…ï¼Œè¿˜æ˜¯ä¸€ä½çœŸæ­£å…³å¿ƒå­¦ç”Ÿçš„å¯¼å¸ˆï¼Œæ‚¨çš„å¹½é»˜æ„Ÿå’Œå…³æ€€è®©æˆ‘ä»¬åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­æ„Ÿåˆ°è½»æ¾å’Œæ„‰å¿«ã€‚æ„Ÿè°¢æ‚¨çš„ä»˜å‡ºå’ŒåŠªåŠ›ï¼Œæ‚¨çš„è¯¾æ˜¯æˆ‘ä»¬å¤§å­¦ç”Ÿæ´»ä¸­çš„ç¾å¥½å›å¿†ã€‚ 


ç”¨æˆ·ï¼š
ä»æ€æƒ³ã€å·¥ä½œèƒ½åŠ›ã€å›¢é˜Ÿåˆä½œä¸‰ä¸ªæ–¹é¢è¯„ä»·å‘˜å·¥ï¼Œ300å­—ä»¥å†…

Skywork-13B-Chatï¼š
å‘˜å·¥Aåœ¨æ€æƒ³ä¸Šæœ‰ç€åšå®šçš„ä¿¡å¿µå’Œé«˜å°šçš„é“å¾·å“è´¨ï¼Œå¯¹å…¬å¸çš„ç›®æ ‡æŒæœ‰é«˜åº¦çš„è´£ä»»æ„Ÿï¼Œå§‹ç»ˆä¿æŒç€ç§¯æä¸Šè¿›çš„å·¥ä½œæ€åº¦ã€‚ä»–/å¥¹å…·æœ‰è‰¯å¥½çš„èŒä¸šé“å¾·å’Œè¡Œä¸ºè§„èŒƒï¼Œå§‹ç»ˆåšæŒè¯šå®å®ˆä¿¡ï¼Œè¸å®æ•¬ä¸šï¼Œå‹‡äºæ‰¿è®¤è‡ªå·±çš„é”™è¯¯å¹¶æ„¿æ„æ”¹è¿›ã€‚ä»–/å¥¹æ·±åˆ»ç†è§£å¹¶éµå®ˆå…¬å¸çš„ä»·å€¼è§‚ï¼Œä»¥å®¢æˆ·ä¸ºä¸­å¿ƒï¼Œå§‹ç»ˆä¿æŒå¯¹å·¥ä½œçš„çƒ­æƒ…å’Œè€å¿ƒï¼Œä¸ºå®¢æˆ·æä¾›ä¼˜è´¨çš„æœåŠ¡ã€‚

åœ¨å·¥ä½œèƒ½åŠ›æ–¹é¢ï¼Œå‘˜å·¥Aå±•ç°å‡ºäº†å‡ºè‰²çš„æŠ€èƒ½å’Œåˆ›æ–°èƒ½åŠ›ã€‚ä»–/å¥¹æ‹¥æœ‰æ‰å®çš„ä¸“ä¸šçŸ¥è¯†å’Œä¸°å¯Œçš„å®è·µç»éªŒï¼Œèƒ½å¤Ÿç†Ÿç»ƒè¿ç”¨å„ç§å·¥å…·å’ŒæŠ€æœ¯ï¼Œè§£å†³é—®é¢˜å’ŒæŒ‘æˆ˜ã€‚ä»–/å¥¹æŒç»­å­¦ä¹ æ–°çš„çŸ¥è¯†å’ŒæŠ€èƒ½ï¼Œä¸æ–­æå‡è‡ªå·±çš„ä¸“ä¸šæ°´å¹³ï¼Œä½¿è‡ªå·±æˆä¸ºéƒ¨é—¨ä¸­ä¸å¯æˆ–ç¼ºçš„ä¸€å‘˜ã€‚ä»–/å¥¹åœ¨é¢å¯¹å›°éš¾å’Œå‹åŠ›æ—¶ï¼Œèƒ½å¤Ÿç‹¬ç«‹æ€è€ƒå’Œè§£å†³é—®é¢˜ï¼ŒåŒæ—¶ä¹Ÿèƒ½æœ‰æ•ˆåœ°ä¸å›¢é˜Ÿæˆå‘˜åä½œï¼Œå…±åŒå®Œæˆä»»åŠ¡ã€‚

åœ¨å›¢é˜Ÿåˆä½œæ–¹é¢ï¼Œå‘˜å·¥Aæ˜¯ä¸€ä¸ªå€¼å¾—ä¿¡èµ–å’Œå°Šé‡çš„é˜Ÿå‹ã€‚ä»–/å¥¹å–„äºå€¾å¬ä»–äººçš„æ„è§ï¼Œèƒ½å¤Ÿæœ‰æ•ˆåœ°è¡¨è¾¾è‡ªå·±çš„è§‚ç‚¹ï¼Œå¹¶ä¸”ä¹äºåŠ©äººã€‚ä»–/å¥¹åœ¨å›¢é˜Ÿä¸­èµ·åˆ°äº†å¾ˆå¥½çš„æ¡¥æ¢ä½œç”¨ï¼Œä¿ƒè¿›äº†å›¢é˜Ÿæˆå‘˜ä¹‹é—´çš„æ²Ÿé€šå’Œäº¤æµï¼Œæé«˜äº†å›¢é˜Ÿçš„å‡èšåŠ›å’Œæ•ˆç‡ã€‚ä»–/å¥¹æ€»æ˜¯èƒ½å¤Ÿç§¯æå‚ä¸å›¢é˜Ÿæ´»åŠ¨ï¼Œæä¾›æ”¯æŒå’Œå¸®åŠ©ç»™éœ€è¦çš„åŒäº‹ï¼Œæ¨åŠ¨å›¢é˜Ÿå‘å‰å‘å±•ã€‚

æ€»çš„æ¥è¯´ï¼Œå‘˜å·¥Aåœ¨æ€æƒ³ã€å·¥ä½œèƒ½åŠ›å’Œå›¢é˜Ÿåˆä½œä¸‰ä¸ªæ–¹é¢éƒ½è¡¨ç°å¾—éå¸¸ä¼˜ç§€ï¼Œæ˜¯æˆ‘ä»¬å…¬å¸çš„å®è´µè´¢å¯Œã€‚æˆ‘ç›¸ä¿¡ä»–/å¥¹å°†ä¼šåœ¨æœªæ¥çš„å·¥ä½œä¸­ç»§ç»­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œä¸ºå…¬å¸çš„å‘å±•åšå‡ºæ›´å¤§çš„è´¡çŒ®ã€‚ 

`````

</details>

<details><summary><b>Question Generation</b></summary>

`````
ç”¨æˆ·ï¼š
æˆ‘æ˜¯ä¸€ååˆä¸­è¯­æ–‡è€å¸ˆï¼Œä¸‹èŠ‚è¯¾è¦æ•™å­¦ç”Ÿã€Šæ¡ƒèŠ±æºè®°ã€‹ï¼Œå¸®æˆ‘å‡†å¤‡5ä¸ªè¯¾å ‚æé—®çš„é—®é¢˜ã€‚

Skywork-13B-Chatï¼š
1. è¿™ç¯‡æ–‡ç« çš„ä½œè€…æ˜¯è°ï¼Ÿä»–çš„å†™ä½œé£æ ¼å¦‚ä½•ï¼Ÿ
2. æ–‡ä¸­çš„æ¡ƒèŠ±æºæ˜¯ä»€ä¹ˆæ ·çš„åœ°æ–¹ï¼Ÿå®ƒç»™ä½ ä»€ä¹ˆæ„Ÿå—ï¼Ÿ
3. æ–‡ä¸­çš„äººç‰©å½¢è±¡æœ‰å“ªäº›ï¼Ÿä»–ä»¬çš„è¡Œä¸ºå’Œæ€§æ ¼ç‰¹ç‚¹å¦‚ä½•ï¼Ÿ
4. ä½ è®¤ä¸ºæ¡ƒèŠ±æºä¸­æœ€é‡è¦çš„å…ƒç´ æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ
5. è¿™ç¯‡æ–‡ç« æƒ³è¦è¡¨è¾¾ä»€ä¹ˆä¸»é¢˜ï¼Ÿå¯¹æˆ‘ä»¬ç°ä»£äººæœ‰ä»€ä¹ˆå¯ç¤ºï¼Ÿ 

ç”¨æˆ·ï¼š
ä½œä¸ºä¸€åé¤é¥®è¡Œä¸šçš„åº—é“ºç»ç†ï¼Œä½ æƒ³äº†è§£è¯¥å­£åº¦ç”¨æˆ·çš„èœå“åå¥½ã€ç¯å¢ƒè¯„ä»·ç­‰ç›¸å…³ä¿¡æ¯ï¼ŒæŒ‡å¯¼åº—é“ºçš„ä¸‹ä¸ªå­£åº¦æˆ˜ç•¥è§„åˆ’ã€‚ä½ ä¼šé—®ç”¨æˆ·å“ªäº›é—®é¢˜ï¼Ÿ

Skywork-13B-Chatï¼š
ä»¥ä¸‹æ˜¯ä¸€äº›å¯èƒ½æœ‰ç”¨çš„é—®é¢˜ï¼š
1. æ‚¨æœ€å–œæ¬¢çš„èœå“æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ
2. æ‚¨è§‰å¾—æˆ‘ä»¬çš„èœå“ä¸­å“ªäº›å‘³é“æˆ–é£å‘³æœ€å—æ¬¢è¿ï¼Ÿ
3. æ‚¨æ˜¯å¦å°è¯•è¿‡æˆ‘ä»¬çš„æ–°èœå“ï¼Ÿå¦‚æœæ˜¯ï¼Œè¯·åˆ†äº«ä¸€ä¸‹æ‚¨çš„ä½“éªŒå’Œæ„Ÿå—ã€‚
4. æ‚¨æœ€å–œæ¬¢çš„å°±é¤æ—¶é—´æ˜¯ä»€ä¹ˆæ—¶å€™ï¼Ÿæ‚¨æ›´å€¾å‘äºé€‰æ‹©åˆé¤è¿˜æ˜¯æ™šé¤ï¼Ÿ
5. æ‚¨æ›´å–œæ¬¢åœ¨å®¤å†…ç”¨é¤è¿˜æ˜¯å®¤å¤–ç”¨é¤ï¼Ÿå¦‚æœåœ¨å®¤å†…ï¼Œæ‚¨æ›´å–œæ¬¢å“ªç§ç¯å¢ƒï¼Ÿï¼ˆä¾‹å¦‚ï¼šå®‰é™ã€èˆ’é€‚ã€å®¶åº­å‹å¥½ã€ç¤¾äº¤ç­‰ï¼‰
6. æ‚¨æ„¿æ„æ”¯ä»˜å¤šå°‘é’±äº«ç”¨æˆ‘ä»¬çš„èœå“ï¼Ÿæ‚¨è®¤ä¸ºä»·æ ¼åˆç†å—ï¼Ÿ
7. æ‚¨æœ€å–œæ¬¢çš„èœå“ç…§ç‰‡æ˜¯ä»€ä¹ˆï¼Ÿæ‚¨ç»å¸¸ç‚¹å“ªä¸ªï¼Ÿ
8. æ‚¨å¯¹æˆ‘ä»¬çš„æœåŠ¡æœ‰ä½•è¯„ä»·ï¼Ÿæ‚¨è®¤ä¸ºæˆ‘ä»¬éœ€è¦æ”¹è¿›çš„åœ°æ–¹åœ¨å“ªé‡Œï¼Ÿ
9. æ‚¨è®¤ä¸ºæˆ‘ä»¬çš„é¤å…ä½ç½®æ–¹ä¾¿å—ï¼Ÿæ˜¯å¦å®¹æ˜“æ‰¾åˆ°ï¼Ÿ
10. æ‚¨ä¼šå‘æœ‹å‹ã€å®¶äººæ¨èæˆ‘ä»¬çš„é¤å…å—ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ
è¿™äº›é—®é¢˜å¯ä»¥å¸®åŠ©æ‚¨äº†è§£ç”¨æˆ·å¯¹é¤é¥®åº—çš„å–œå¥½å’Œåé¦ˆï¼Œä»¥ä¾¿åˆ¶å®šä¸‹ä¸€ä¸ªå­£åº¦çš„æˆ˜ç•¥è®¡åˆ’ã€‚  

ç”¨æˆ·ï¼š
æˆ‘æ˜¯ä¸€ä¸ªèŠ‚ç›®ç­–åˆ’ï¼Œæœ¬æœŸä¸»é¢˜æ˜¯é‚€è¯·æ˜æ˜Ÿæ ¹æ®ç¯ä¿é—®é¢˜è¿›è¡Œè¾©è®ºï¼Œè¯·è®¾è®¡ä¸‰ä¸ªè¾©é¢˜ã€‚è¦æ±‚è¾©é¢˜ç®€å•å¥½ç†è§£ï¼Œæ­£åæ–¹çš„è§‚ç‚¹éƒ½ç¬¦åˆä¸»æµä»·å€¼è§‚ã€‚è¾©é¢˜éœ€è¦æœ‰å™±å¤´ï¼Œç»™èŠ‚ç›®åˆ¶é€ çœ‹ç‚¹ã€‚

Skywork-13B-Chat:
1. åƒåœ¾åˆ†ç±»ï¼Œè°è¯´äº†ç®—ï¼Ÿ
æ­£æ–¹ï¼šæ”¯æŒä¸¥æ ¼åƒåœ¾åˆ†ç±»æ”¿ç­–ï¼Œè®¤ä¸ºå…¶èƒ½å¤Ÿæœ‰æ•ˆæé«˜èµ„æºå›æ”¶ç‡å’Œé™ä½ç¯å¢ƒæ±¡æŸ“ã€‚
åæ–¹ï¼šåå¯¹è¿‡åº¦å¼ºè°ƒåƒåœ¾åˆ†ç±»ï¼Œä¸»å¼ ç®€åŒ–åˆ†ç±»æ ‡å‡†ï¼Œå‡å°‘æ°‘ä¼—å‚ä¸æˆæœ¬ã€‚
2. ç»¿è‰²å‡ºè¡Œï¼ŒçœŸçš„æ›´ç¯ä¿å—ï¼Ÿ
æ­£æ–¹ï¼šæ­¥è¡Œã€éª‘è¡Œä»£æ›¿å¼€è½¦å¯ä»¥å¤§å¹…é™ä½ç¢³æ’æ”¾ï¼Œä¿æŠ¤ç¯å¢ƒï¼Œç¼“è§£äº¤é€šæ‹¥å µã€‚
åæ–¹ï¼šåŸå¸‚å…¬å…±äº¤é€šç³»ç»Ÿä¸å®Œå–„ï¼Œéª‘ç”µåŠ¨è½¦æˆ–å¼€æ±½è½¦çŸ­é€”å‡ºè¡Œæ›´æ–¹ä¾¿ï¼Œå‡å°‘é€šå‹¤æ—¶é—´ã€‚
3. ç”µå­äº§å“ï¼Œæ›´æ–°è¿˜æ˜¯å¤ç”¨ï¼Ÿ
æ­£æ–¹ï¼šç¯ä¿åº”è€ƒè™‘ä½¿ç”¨äºŒæ‰‹ç”µå­äº§å“ï¼Œå‡å°‘åºŸå¼ƒç‰©äº§ç”Ÿï¼ŒèŠ‚çº¦åœ°çƒèµ„æºã€‚
åæ–¹ï¼šæ–°ç”µå­äº§å“ä¸æ–­æ¨å‡ºï¼Œåº”è¯¥é¼“åŠ±æ¶ˆè´¹è€…æ›´æ–°æ¢ä»£ï¼Œè¿½æ±‚ç§‘æŠ€è¿›æ­¥ã€‚ 

`````

</details>

<details><summary><b>Naming</b></summary>

`````

ç”¨æˆ·ï¼š
æ±½è½¦åˆ¶é€ å…¬å¸è®¡åˆ’æ¨å‡ºä¸€æ¬¾ç”µåŠ¨SUVï¼Œéœ€è¦ä¸€ä¸ªå¯Œæœ‰æœªæ¥æ„Ÿçš„è½¦å‹åç§°ï¼Œæ‚¨æœ‰ä½•å»ºè®®ï¼Ÿ

Skywork-13B-Chatï¼š
ä»¥ä¸‹æ˜¯å‡ ä¸ªå»ºè®®ï¼š
1. æœªæ¥ä¹‹æ˜Ÿ (Star of the Future)
2. æ–°èƒ½æºé¢†è¢– (New Energy Leaders)
3. æ™ºèƒ½å…ˆé©± (Intelligent Pioneer)
4. ç»¿è‰²å·¨å…½ (Green Beast)
5. æœªæ¥è¿åŠ¨ (Future Sport)
6. ç”µåŠ¨æ¢é™©å®¶ (Electric Adventurer)
7. è¿œèˆªè€… (Navigators)
8. ç”µåŠ›ç²¾çµ (Electric Elf)
9. æœªæ¥æ—…è¡Œè€… (Future Traveler)
10. æ™ºèƒ½ç¯ä¿è½¦ (Intelligent Eco-Drive)
å¸Œæœ›è¿™äº›å»ºè®®èƒ½å¸®åˆ°æ‚¨ï¼ 

ç”¨æˆ·ï¼š
æˆ‘å‡†å¤‡å¼€è®¾ä¸€å®¶ä¹¦åº—ï¼Œéœ€è¦ä¸€ä¸ªä¸æ–‡å­¦ç›¸å…³çš„åº—é“ºåç§°ï¼Œèƒ½å¤Ÿå¸å¼•ä¹¦ç±çˆ±å¥½è€…ï¼Œå¸®æˆ‘å–10ä¸ªå§

Skywork-13B-Chatï¼š
å¥½çš„ï¼Œä»¥ä¸‹æ˜¯ä¸ºæ‚¨æä¾›çš„10ä¸ªä¸æ–‡å­¦ç›¸å…³çš„åº—é“ºåç§°ï¼š
1. æ–‡å¢¨ä¹¦é¦™
2. æ‚¦è¯»ä¹‹æ—…
3. ä¹¦é¦™é—¨ç¬¬
4. æ–‡å­¦ç©ºé—´
5. ç»å…¸åŠ
6. ç¬”å¢¨ä¹¦é˜
7. ç™¾è‰å ‚
8. ç¿°å¢¨è½©
9. å¤éŸµä¹¦å±‹
10. æ–‡è‰ºä¹¦æˆ¿

å¸Œæœ›è¿™äº›åç§°èƒ½ç»™æ‚¨å¸¦æ¥å¯å‘ï¼Œç¥æ‚¨çš„ä¹¦åº—ç”Ÿæ„å…´éš†ï¼

ç”¨æˆ·ï¼š
æˆ‘ä»¬æ˜¯ä¸€ä¸ª6äººçš„å›¢é˜Ÿ,è¦ç»„é˜Ÿå‚åŠ ä¸€ä¸ªå¿—æ„¿è€…æ´»åŠ¨ï¼Œæ±‚ä¸€ä¸ªé˜Ÿå,æœ€å¥½è§£é‡Šä¸€ä¸ªæ„ä¹‰,è¦æœ‰åˆ›æ„çš„ï¼Œ6ä¸ªå­—å·¦å³

Skywork-13B-Chatï¼š
å¦‚æœä½ æƒ³è¦ä¸€ä¸ªæœ‰æ„ä¹‰çš„é˜Ÿåï¼Œå¯ä»¥è€ƒè™‘ä»¥ä¸‹å‡ ç§ï¼š
1. "çˆ±å¿ƒä½¿è€…"ï¼Œè¿™ä¸ªé˜Ÿåè¡¨ç¤ºä½ ä»¬æ˜¯ä¸€ä¸ªå……æ»¡çˆ±å¿ƒå’Œçƒ­æƒ…çš„å›¢é˜Ÿï¼Œè‡´åŠ›äºå¸®åŠ©ä»–äººã€‚
2. "å¿«ä¹å¿—æ„¿å›¢"ï¼Œè¿™ä¸ªé˜Ÿåè¡¨ç¤ºä½ ä»¬æ˜¯ä¸€ç¾¤çƒ­çˆ±ç”Ÿæ´»ã€ä¹äºåŠ©äººçš„å¿—æ„¿è€…ã€‚
3. "å…±ç­‘æ¢¦æƒ³"ï¼Œè¿™ä¸ªé˜Ÿåè¡¨ç¤ºä½ ä»¬ä¸€èµ·åˆä½œï¼Œå…±åŒåŠªåŠ›å®ç°åˆ«äººçš„æ¢¦æƒ³ã€‚
4. "çˆ±çš„å®ˆæŠ¤è€…"ï¼Œè¿™ä¸ªé˜Ÿåè¡¨ç¤ºä½ ä»¬å°†å…³çˆ±å’Œä¿æŠ¤ä»–äººä½œä¸ºè‡ªå·±çš„è´£ä»»ã€‚
5. "å¿—æ„¿è€…ä¹‹æ˜Ÿ"ï¼Œè¿™ä¸ªé˜Ÿåè¡¨ç¤ºä½ ä»¬æ˜¯é—ªè€€çš„æ˜æ˜Ÿï¼Œä»£è¡¨ç€çˆ±å¿ƒå’Œå¥‰çŒ®ã€‚
å¸Œæœ›ä»¥ä¸Šçš„é˜Ÿåèƒ½ç»™æ‚¨å¸¦æ¥çµæ„Ÿï¼ 

`````

</details>

# Quantization

## 8bit Quantization

Skywork utilizes the widely-used 8-bit quantization method called [BitsAndBytes](https://github.com/TimDettmers/bitsandbytes). This method allows for quantizing performance with minimal loss and has been seamlessly integrated into the transformers library. Building upon BitsAndBytes, we offer two approaches for utilizing online quantization and offline 8-bit models.

To illustrate the usage of the int8 quantization model, we provide an example. Before you begin, please ensure that you have installed the BitsAndBytes library and the necessary dependencies. For detailed installation instructions, please refer to the [BitsAndBytes](https://github.com/TimDettmers/bitsandbytes) repository.

### Online Quantization

```python
model = AutoModelForCausalLM.from_pretrained("skywork-13B-Base", torch_dtype=torch.bfloat16,load_in_8bit=True, trust_remote_code=True).eval()
```

### Offline Quantization

```python
model = AutoModelForCausalLM.from_pretrained("skywork-13B-Base-8bits", device_map="auto", torch_dtype=torch.bfloat16,trust_remote_code=True).eval()
```



### Resutls

We have tested the quantitative model on benchmark evaluation datasets, and the results are as follows:

| Precision | C-Eval | MMLU  | CMMLU |
| --------- | ------ | ----- | ----- | 
| bf16      | 60.6  | 61.8 | 62.1 |
| 8bits     | 58.5  | 61.8 | 61.0 |


### GPU Mem in GB

| Precision | Skywork-13B |
| --------- | ----------- |
| bf16      | 25.91       |
| 8bits     | 13.57       |

# Fine-tuning
## Full-Parameter Fine-Tuning

Continuing pre-training with the Skywork-13B-Base model.
```bash
## preprocess continue pretraining data
## Because pre-training data is usually large, we use a script to process the training data separately.
python train/pt_data_preprocess.py \
    -t $MODEL_PATH \
    -i data/pt_train.jsonl \
    -o data_cache/pt_train_demo 

## launch training
export WANDB_API_KEY=YOUR_WANDB_KEY
export WANDB_ENTITY=skywork
export WANDB_PROJECT=skywork-13b-opensource

export MODEL_PATH=skywork-13b-models/skywork-13b-base
export DATA_CACHE_DIR=data_cache/pt_train_demo/pt_train
bash bash_scripts/skywork_13b_pt.sh
 
```

Conducting supervised fine-tuning with Skywork-13B-Base model.

```bash 
## preprocess data and launch training
export WANDB_API_KEY=YOUR_WANDB_KEY
export WANDB_ENTITY=skywork
export WANDB_PROJECT=skywork-13b-opensource

export SFT_DATA_DIR=data/sft_data
export DATA_CACHE_DIR=data_cache/sft_train_demo
bash bash_scripts/skywork_13b_sft.sh


```

## LoRA Fine-Tuning

Continuing LoRA pre-training with the Skywork-13B-Base model with LoRA.
```bash 
## preprocess continue pretraining data
## Because pre-training data is usually large, we use a script to process the training data separately.
python train/pt_data_preprocess.py \
    -t $MODEL_PATH \
    -i data/pt_train.jsonl \
    -o data_cache/pt_train_demo 


export WANDB_API_KEY=YOUR_WANDB_KEY
export WANDB_ENTITY=skywork
export WANDB_PROJECT=skywork-13b-opensource

export MODEL_PATH=/data/shared/public/liang.zhao/skywork-13b-models/skywork-13b-base
export DATA_CACHE_DIR=data_cache/pt_train_demo/pt_train
bash bash_scripts/skywork_13b_pt_lora.sh
 
```

Conducting supervised fine-tuning with Skywork-13B-Base model with LoRA.

```bash 

export WANDB_API_KEY=YOUR_WANDB_KEY
export WANDB_ENTITY=skywork
export WANDB_PROJECT=skywork-13b-opensource

export SFT_DATA_DIR=/data/user/liang.zhao/Skywork/data/sft_data
export DATA_CACHE_DIR=data_cache/sft_train_demo
bash bash_scripts/skywork_13b_sft_lora.sh
 
```

# Community and Ecosystem
## Huawei Ascend
### MindSpore Framework
[MindFormers]( https://gitee.com/mindspore/mindformers)is a full-process development suite based on the MindSpore framework that supports large model training, fine-tuning, evaluation, inference, and deployment. The [Skywork-13B]( https://gitee.com/mindspore/mindformers/tree/dev/research/skywork) model is integrated into this suite, supporting users to fine-tune and deploy models based on Ascend AI hardware capabilities. The specific usage can be seen in our[README]( https://gitee.com/mindspore/mindformers/tree/dev/research/skywork/skywork.md) on the MindSpore platform.

### Large Model Experience Platform
The [MindSpore Large Model Platform](â€‹https://xihe.mindspore.cn) is based on the MindSpore AI framework, MindFormers large model development suite, and Ascend hardware capabilities, opening the [Skywork-13B](https://xihe.mindspore.cn/modelzoo/skywork_13b) large model capabilities to the public. Everyone is welcome to use it.

# Declaration and License Agreement


## Declaration

We hereby declare that the Skywork model should not be used for any activities that pose a threat to national or societal security or engage in unlawful actions. Additionally, we request users not to deploy the Skywork model for internet services without appropriate security reviews and records. We hope that all users will adhere to this principle to ensure that technological advancements occur in a regulated and lawful environment.

We have done our utmost to ensure the compliance of the data used during the model's training process. However, despite our extensive efforts, due to the complexity of the model and data, there may still be unpredictable risks and issues. Therefore, if any problems arise as a result of using the Skywork open-source model, including but not limited to data security issues, public opinion risks, or any risks and problems arising from the model being misled, abused, disseminated, or improperly utilized, we will not assume any responsibility.

## License Agreement

The community usage of Skywork model requires [Skywork Community License](https://github.com/SkyworkAI/Skywork/blob/main/Skywork%20Community%20License.pdf). The Skywork model supports commercial use. If you plan to use the Skywork model or its derivatives for commercial purposes, you must abide by terms and conditions within [Skywork Community License](https://github.com/SkyworkAI/Skywork/blob/main/Skywork%20Community%20License.pdf).

  

[ã€ŠSkywork æ¨¡å‹ç¤¾åŒºè®¸å¯åè®®ã€‹ã€‹]:https://github.com/SkyworkAI/Skywork/blob/main/Skywork%20æ¨¡å‹ç¤¾åŒºè®¸å¯åè®®.pdf


[skywork-opensource@kunlun-inc.com]: mailto:skywork-opensource@kunlun-inc.com

# Contact Us and Citation
If you find our work helpful, please feel free to cite our paper~
```
@misc{wei2023skywork,
      title={Skywork: A More Open Bilingual Foundation Model}, 
      author={Tianwen Wei and Liang Zhao and Lichang Zhang and Bo Zhu and Lijie Wang and Haihua Yang and Biye Li and Cheng Cheng and Weiwei LÃ¼ and Rui Hu and Chenxia Li and Liu Yang and Xilin Luo and Xuejie Wu and Lunan Liu and Wenjun Cheng and Peng Cheng and Jianhao Zhang and Xiaoyu Zhang and Lei Lin and Xiaokun Wang and Yutuan Ma and Chuanhai Dong and Yanqi Sun and Yifu Chen and Yongyi Peng and Xiaojuan Liang and Shuicheng Yan and Han Fang and Yahui Zhou},
      year={2023},
      eprint={2310.19341},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
```


```
@article{skyworkmath,
  title={SkyMath: Technical Report},
  author={Liu Yang, Haihua Yang, Wenjun Cheng, Lei Lin, Chenxia Li, Yifu Chen, Lunan Liu, Jianfei Pan, Tianwen Wei, Biye Li, Liang Zhao, Lijie Wang, Bo Zhu, Guoliang Li, Xuejie Wu, Xilin Luo, Rui Hu},
  journal={arXiv preprint arXiv: 2310.16713},
  url={https://arxiv.org/abs/2310.16713},
  year={2023}
}
```


```
@article{Skywork_Multi-Modal_Group_Empirical_Study_Towards_2023,
    author = {Skywork Multi-Modal Group},
    month = sep,
    title = {{Empirical Study Towards Building An Effective Multi-Modal Large Language Model}},
    year = {2023}
}

```
