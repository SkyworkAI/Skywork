<!-- <div align="center">
<h1>
  âœ¨Skywork
</h1>
</div> -->
<div align="center"><img src="misc/skywork_logo.jpeg" width="550"/></div>

<p align="center">
ğŸ¤— <a href="https://huggingface.co/Skywork" target="_blank">Hugging Face</a> â€¢ ğŸ¤– <a href="https://modelscope.cn/organization/Skywork" target="_blank">ModelScope</a> â€¢ ğŸ’¬ <a href="https://github.com/SkyworkAI/Skywork/blob/main/misc/wechat.png?raw=true" target="_blank">WeChat</a>â€¢ ğŸ“œ<a href="https://arxiv.org/" target="_blank">Tech Report</a>â€¢ ğŸ§®<a href="https://arxiv.org/" target="_blank">Skymath Paper</a>
</p>


<div align="center">


[ğŸ‰å¤©å·¥åœ¨çº¿å¯¹è¯å¹³å°å·²æ­£å¼å‘å…¬ä¼—å¼€æ”¾](https://sso.tiangong.cn/?redirect=https://model-platform.tiangong.cn/overview&client_id=200005)

</div>



<div align="center">


[![GitHub Stars](https://img.shields.io/github/stars/SkyworkAI/Skywork)](https://github.com/SkyworkAI/Skywork/stargazers)
[![GitHub Forks](https://img.shields.io/github/forks/SkyworkAI/Skywork)](https://github.com/SkyworkAI/Skywork/fork)
</div>



# æ¨¡å‹ä»‹ç»ï¼ˆIntroductionï¼‰
**Skywork-13B-Base**æ¨¡å‹åœ¨é«˜è´¨é‡æ¸…æ´—è¿‡æ»¤çš„3.2ä¸‡äº¿ä¸ªå¤šè¯­è¨€ï¼ˆä¸»è¦æ˜¯ä¸­æ–‡å’Œè‹±æ–‡ï¼‰å’Œä»£ç æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå®ƒåœ¨å¤šç§è¯„æµ‹å’Œå„ç§åŸºå‡†æµ‹è¯•ä¸Šéƒ½å±•ç°äº†åŒç­‰è§„æ¨¡æ¨¡å‹çš„æœ€ä½³æ•ˆæœã€‚

**Skywork-13B-Base**: The model was trained on a high-quality cleaned dataset consisting of 3.2 trillion multilingual data (mainly Chinese and English) and code. It has demonstrated the best performance among models of similar scale in various evaluations and benchmark tests.

å¦‚æœæ‚¨å¸Œæœ›äº†è§£æ›´å¤šçš„ä¿¡æ¯ï¼Œå¦‚è®­ç»ƒæ–¹æ¡ˆï¼Œè¯„ä¼°æ–¹æ³•ï¼Œè¯·å‚è€ƒæˆ‘ä»¬çš„[æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/skywork-tech-report)å’Œ[Skywork-Math](https://arxiv.org/skywork-tech-report)è®ºæ–‡ã€‚

If you are interested in more training and evaluation details, please refer to our [technical report](https://arxiv.org/skywork-tech-report) and [Skywork-Math]((https://arxiv.org/skywork-tech-report)) paper.


## è®­ç»ƒæ•°æ®ï¼ˆTraining Dataï¼‰
æˆ‘ä»¬ç²¾å¿ƒæ­å»ºäº†æ•°æ®æ¸…æ´—æµç¨‹å¯¹æ–‡æœ¬ä¸­çš„ä½è´¨é‡æ•°æ®ã€æœ‰å®³ä¿¡æ¯ã€æ•æ„Ÿä¿¡æ¯è¿›è¡Œæ¸…æ´—è¿‡æ»¤ã€‚æˆ‘ä»¬çš„Skywork-13B-Baseæ¨¡å‹æ˜¯åœ¨æ¸…æ´—åçš„3.2TBé«˜è´¨é‡ä¸­ã€è‹±ã€ä»£ç æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå…¶ä¸­è‹±æ–‡å æ¯”52.2%ï¼Œä¸­æ–‡å æ¯”39.6%ï¼Œä»£ç å æ¯”8%ï¼Œåœ¨å…¼é¡¾ä¸­æ–‡å’Œè‹±æ–‡ä¸Šçš„è¡¨ç°çš„åŒæ—¶ï¼Œä»£ç èƒ½åŠ›ä¹Ÿèƒ½æœ‰ä¿è¯ã€‚

We have developed a data cleaning pipeline with great care to effectively clean and filter low-quality data and eliminate harmful information from text data. Our Skywork-13B-Base model is trained on a  dataset with 3.2TB tokens that consists of high-quality Chinese, English, and code data, all of which have been thoroughly cleaned. The English data comprises 52.2% of the dataset, the Chinese data accounts for 39.6%, and the code data makes up 8%. This comprehensive approach ensures optimal performance for both Chinese and English while also maintaining the ability to handle code.

|             | Category         | Percentage |
|-------------|------------------|------------|
| **English** | Webpages         | 39.8%      |
|             | Books            | 3.6%       |
|             | Academic Papers  | 3.0%       |
|             | Encyclopedia     | 0.5%       |
|             | Miscellany       | 2.9%       |
| **Chinese** | Webpages         | 30.4%      |
|             | Social Media     | 5.5%       |
|             | Encyclopedia     | 0.8%       |
|             | Miscellany       | 3.1%       |
| **Other Lang.**    | Encyclopedia           | 2.4%       | 
| **Code**    | Github           | 8.0%       | 





## æ¨¡å‹ç»“æ„ï¼ˆModel Structureï¼‰
ä¸Llama-2-13Bæ¨¡å‹å¯¹æ¯”ï¼Œå¤©å·¥Skywork-13Bæ¨¡å‹é‡‡ç”¨ç›¸å¯¹æ›´åŠ ç˜¦é•¿çš„ç½‘ç»œç»“æ„ï¼Œå±‚æ•°ä¸º52å±‚ï¼ŒåŒæ—¶å°†FFN Dimå’ŒHidden Dimç¼©å°åˆ°12288å’Œ4608ï¼Œä»è€Œä¿è¯æ¨¡å‹å‚æ•°é‡å’ŒåŸå§‹Llama-2-13Bæ¨¡å‹ç›¸å½“ã€‚æ ¹æ®æˆ‘ä»¬å‰æœŸå®éªŒå¯¹æ¯”ï¼Œç›¸å¯¹ç˜¦é•¿çš„ç½‘ç»œç»“æ„åœ¨å¤§Batch Sizeè®­ç»ƒä¸‹å¯ä»¥å–å¾—æ›´å¥½çš„æ³›åŒ–æ•ˆæœã€‚Skywork-13Bå’ŒLlama-2-13Bæ¨¡å‹çš„å¯¹æ¯”å¦‚ä¸‹ï¼š

Compared to the Llama2-13B model, the Skywork-13B model adopts a relatively thinner and deeper network structure with 52 layers. At the same time, the FFN Dim and Hidden Dim are reduced to 12288 and 4608, respectively, to ensure that the model has a similar number of parameters as the original Llama-13B model. Based on our preliminary experimental results, a relatively thinner and deeper network structure can achieve better generalization performance under large batch size training. The detailed comparison between the Skywork-13B and Llama-2-13B models is as follows:

| Model Structure         | Llama2-13B | Skywork-13B | 
|----------------------|:----:|:-----------:|
| Vocab. Size  | 32,000 |    65,536     | 
| Hidden Dim.  | 5,120 |    4,608     | 
| FFN Dim.  | 13,696 |    12,288     |
| Head Dim. | 128 |    128     | 
| Num. Heads | 40 |    36     | 
| Num. Layers | 40 |    52     | 
| Seq. Len. | 4,096 |    4,096     | 
| Positional Embedding | RoPE | RoPE |


## åˆ†è¯å™¨ï¼ˆTokenizerï¼‰
æˆ‘ä»¬ä½¿ç”¨Byte-Pair Encodingï¼ˆBPEï¼‰å¯¹æ•°æ®è¿›è¡Œåˆ†è¯ï¼Œè¯è¡¨å¤§å°ä¸º65536ï¼Œå…¶ä¸­æ‹‰ä¸å­—ç¬¦å’Œå­è¯ä¸º32000ä¸ªï¼Œæ±‰å­—å’ŒUnicodeç¬¦å·8000ä¸ªï¼Œæ±‰è¯­è¯è¯­25519ä¸ªï¼Œå‰©ä¸‹çš„17ä¸ªä¸ºä¿ç•™å­—ã€‚

We use Byte-Pair Encoding (BPE) to tokenize the data, with a vocabulary size of 65536. Among them, there are 32000 Latin characters and subwords, 8000 Chinese characters and Unicode symbols, 25519 Chinese words, and the remaining 17 are reserved words.


| Category                            | Size    |
|---------------------------------|--------|
| Latin based words & subwords                 | 32000  |
| Chinese characters & Unicode symbols               | 8000   |
| Chinese words                        | 25519  |
| Reserved symbols                       | 17     |
| **Total**                         | **65536** |


# æ¨¡å‹è¯„ä¼°ï¼ˆEvaluationï¼‰
## é¢†åŸŸæ•°æ®å›°æƒ‘åº¦è¯„ä¼°ï¼ˆPerplexity Evaluaitonï¼‰
è¯­è¨€æ¨¡å‹è®­ç»ƒçš„æœ¬è´¨ä¸Šæ˜¯è®©é¢„æµ‹ä¸‹ä¸€ä¸ªè¯æ›´å‡†ç¡®ã€‚åŸºäºè¿™ä¸ªè®¤çŸ¥ï¼Œæˆ‘ä»¬è®¤ä¸ºè¯„ä¼°åŸºç¡€å¤§æ¨¡å‹ä¸€ä¸ªé‡è¦çš„æ–¹å¼æ˜¯è¯„ä¼°åœ¨å„å¤§é¢†åŸŸä¸Šè¯­è¨€æ¨¡å‹ç”Ÿæˆæ–‡ç« çš„æ¦‚ç‡ã€‚åœ¨æ¨¡å‹è®­ç»ƒä¸­é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡ä¸€èˆ¬ä½¿ç”¨Cross EntropyæŸå¤±å‡½æ•°ï¼Œæ•´ä½“çš„æŸå¤±å‡½æ•°ä¸ºæ¯ä¸ªä½ç½®é¢„æµ‹çœŸå®è¯æŸå¤±çš„å¹³å‡ï¼Œåˆ™æœ‰ï¼š

$$loss = \sum^{n}_{i=1} log(p_i) / n = log( \prod_{i=1}^n p_i) / n$$

å…¶ä¸­$n$æ˜¯æ–‡æ¡£çš„é•¿åº¦ï¼Œå³tokenæ•°ï¼Œ$p_i$æ˜¯ä½ç½®iä¸ŠçœŸå®è¯çš„æ¦‚ç‡ï¼Œæˆ‘ä»¬çŸ¥é“æ–‡æ¡£ä¸­æ¯ä¸€ä¸ªä½ç½®ä¸ŠçœŸå®è¯çš„æ¦‚ç‡çš„è”ä¹˜åˆ™ä¸ºç”Ÿæˆè¯¥æ–‡æ¡£çš„æ¦‚ç‡ï¼Œå¦‚æ­¤æˆ‘ä»¬å°±å°†losså’Œç”Ÿæˆæ–‡ç« çš„æ¦‚ç‡è”ç³»åœ¨äº†ä¸€èµ·ã€‚è€Œä¸åŒæ¨¡å‹å› ä¸ºä½¿ç”¨çš„åˆ†è¯å™¨ä¸åŒï¼Œå…·æœ‰ä¸åŒçš„tokenæ•°ï¼Œå› æ­¤å¯¹æŸå¤±å‡½æ•°ä¹˜ä»¥tokenæ•°ç›®$n$ï¼Œè¿™æ ·å°±ä»…è€ƒè™‘ç”Ÿæˆæ–‡ç« çš„æ¦‚ç‡éƒ¨åˆ†ï¼Œä¸åŒæ¨¡å‹ä¹Ÿå¯ä»¥è¿›è¡Œæ¯”è¾ƒã€‚æˆ‘ä»¬å°†æ ‡å‡†åŒ–ålosså–æŒ‡æ•°è½¬æ¢æˆperplexityï¼Œä½¿å¾—æ¨¡å‹çš„å·®å¼‚æ›´åŠ å¯è¯»ã€‚ä¸ºäº†é˜…è¯»æ–¹ä¾¿åç»­æåˆ°çš„losså’Œpplä¸ºæ¨¡å‹æ ‡å‡†åŒ–åçš„losså’Œperplexityã€‚

åŸºäºä¸Šè¿°åˆ†æï¼Œæˆ‘ä»¬å¯¹å¯¹å¤šä¸ªé¢†åŸŸç­›é€‰å‡º2023å¹´10æœˆä»½æ–°å‘å¸ƒçš„å‡ ç™¾åˆ°ä¸Šåƒç¯‡é«˜è´¨é‡æ–‡ç« ï¼Œå¹¶äººå·¥è¿›è¡Œäº†æ ¸å¯¹ã€‚ä¿è¯æ‰€æœ‰çš„æµ‹è¯•æ•°æ®ä¸åœ¨å¤©å·¥æ¨¡å‹ä»¥åŠå…¶ä»–æ‰€æœ‰æ¨¡å‹çš„è®­ç»ƒé›†ä¸­ï¼Œå¹¶ä¸”æµ‹è¯•æ•°æ®çš„æ¥æºä¹Ÿè¶³å¤Ÿå¹¿æ³›ï¼Œè´¨é‡ä¹Ÿé«˜ã€‚æˆ‘ä»¬å¯ä»¥é€‰å–å½“å‰æœ€æ–°çš„æ–‡ç« è¯„æµ‹ä¸åŒæ¨¡å‹çš„pplï¼Œæ¨¡å‹å¾ˆéš¾ä½œå¼Šã€‚
ä¸‹å›¾åˆ—å‡ºäº†ä¸åŒå¼€æºæ¨¡å‹ï¼Œå¤©å·¥Skywork-13B-Baseå–å¾—æœ€ä¼˜æ•ˆæœï¼Œè¯æ˜äº†æˆ‘ä»¬çš„Baseæ¨¡å‹çš„åŸºç¡€èƒ½åŠ›å¤„äºå›½å†…å¼€æºæ¨¡å‹ä¸­æ–‡æœ€å¼ºæ°´å¹³ã€‚

We have chosen several hundred to thousands of high-quality articles that were published in October 2023 across various fields. We have manually verified these articles to ensure their quality. It is important to note that none of the test data used in evaluating the Skywork model or any other models is included in their training set. Furthermore, the test data is diverse and of high quality, making it challenging for the models to gain an unfair advantage.

The figure below displays the performance of different open source models. Skywork-13B-Base achieves the best results.

|                  | Tech  | Movie | Gov.  | Game  | Finance | General | Average |
|------------------|-------|-------|-------|-------|---------|---------|---------|
| MOSS-7B          | 20.83 | 39.66 | 11.08 | 31.24 | 10.59   | 13.25   | 18.50   |
| InternLM-7B      | 13.43 | 24.90 | 5.88  | 19.78 | 6.17    | 8.10    | 11.17   |
| Qwen-7B          | 13.39 | 25.16 | 5.55  | 19.26 | 5.76    | 7.78    | 10.83   |
| Baichuan2-7B     | 12.89 | 23.26 | 5.34  | 18.36 | 5.68    | 7.62    | 10.41   |
| LLaMA2-13B       | 23.26 | 50.66 | 18.09 | 32.52 | 14.85   | 16.55   | 23.54   |
| Xverse-13B       | 12.55 | 23.49 | 5.20  | 17.69 | 5.54    | 7.46    | 10.19   |
| Baichuan-13B     | 12.38 | 22.46 | 5.21  | 17.59 | 5.42    | 7.37    | 10.03   |
| Baichuan2-13B    | 12.14 | 21.85 | 5.05  | 17.15 | 5.35    | 7.24    | 9.81    |
| Qwen-14B         | 11.90 | 22.43 | 4.89  | **16.94** | 5.24    | 7.03    | 9.67    |
| InternLM-20B     | 12.34 | 22.06 | 5.75  | 17.45 | 5.73    | 7.78    | 10.34   |
| Aquila2-34B      | 14.62 | 29.09 | 5.72  | 21.78 | 5.83    | 8.45    | 11.73   |
| Skywork-13B-Base | **11.58** | **21.84** | **4.76**  | 17.28 | **4.92**    | **6.82**    | **9.42**    | 

### è¯„æµ‹æ•°æ®å’Œè¯„æµ‹è„šæœ¬ï¼ˆLoss Evaluationï¼‰
æˆ‘ä»¬å°†è¯„æµ‹æ•°æ®å’Œè¯„æµ‹è„šæœ¬ä¹Ÿè¿›è¡Œäº†å¼€æºï¼Œä¸‹è½½githubä¸Šçš„ä»£ç è¿è¡Œä¸‹é¢å‘½ä»¤åˆ™å¯ä»¥å¤ç°æˆ‘ä»¬çš„ç»“æœã€‚

We have also open-sourced the data and evaluation scripts. You can reproduce our results by running the following command.

```
bash bash_scripts/skywork_eval_loss.sh
```

## Benchmarkè¯„ä¼°ï¼ˆBenchmark Resultsï¼‰
æˆ‘ä»¬è¯„ä¼°äº†å„å¤§æƒå¨è¯„æµ‹benchmarkä¸Šçš„ç»“æœä½œä¸ºå‚è€ƒï¼ŒåŒ…æ‹¬C-Evalï¼ŒMMLUï¼ŒCMMLUï¼ŒGSM8Kã€‚éµå¾ªä¹‹å‰çš„è¯„ä¼°æµç¨‹ï¼ŒC-Evalã€MMLUã€CMMLUæµ‹è¯•5-shotç»“æœï¼ŒGSM8Kæµ‹è¯•8-shotç»“æœã€‚å¯ä»¥çœ‹åˆ°Skywork-13B-Baseæ¨¡å‹åœ¨ä¸­æ–‡å¼€æºæ¨¡å‹ä¸­å¤„äºå‰åˆ—ï¼Œåœ¨åŒç­‰å‚æ•°è§„æ¨¡ä¸‹ä¸ºæœ€ä¼˜æ°´å¹³ã€‚

We evaluated Skywork-13B-Base on several popular benchmarks, including C-Eval, MMLU, CMMLU, and GSM8K. Following the previous evaluation process, we tested the 5-shot results of C-Eval, MMLU, and CMMLU, and the 8-shot results of GSM8K. It can be seen that the Skywork-13B-Base model is among the top models in the Chinese open source model community, performing at an optimal level with the same parameter scale.

| Model            | C-Eval  | CMMLU | MMLU | GSM8K |  
|-------------------------|:-----:|:---------------:|:----------:|:-------:|
| LLaMA-1-13B-Base            | 35.5  | 31.2            | 46.9       | 17.8   | 
| Open-LLaMA-13B | 27.1  | 26.7         | 42.7       | 12.4   |
| LLaMA-2-13B-Base             | 36.5  | 36.6            | 54.8      | 28.7   | 
| InternLM-20B  | 58.8  |     -        |  62.0      | 52.6   | 
| Qwen-14B-Base | 72.1  |  71.0           | 66.3       | 61.3   |
| Aquila2-34B-Base | 63.1  |  71.4           | 64.2       | 58.4   |
| XVERSE-13B-Base              | 54.7  | -           | 55.1       | -   | 
| Baichuan-13B-Base | 52.4  | 55.3            | 51.6      | 26.6   |
| Baichuan-2-13B-Base | 58.1  | 62.0            | 59.2       | 52.3  |
| Skywork-13B-Base (ours)   | 59.5 | 61.6 | 61.6    | 55.8 | 

## Benchmarkè¯„ä¼°è¯¦ç»†ç»“æœ
æˆ‘ä»¬ç»™å‡º**Skywork-13B-Base**æ¨¡å‹åœ¨C-Evalï¼ŒCMMLUï¼ŒMMLUä¸Šæ¨¡å‹çš„è¯¦ç»†ç»“æœã€‚

We provide detailed results of the Skywork-13B-Base model on C-EVAL, CMMLU, and MMLU.

| Benchmark | **STEM** | **Humanities** | **Social Science** | **Other** | **China Specific** | **Hard** | **Average** | 
|:-----:|:---------:|:--------:|:-------------:|:--------:|:--------:|:--------:|:--------:|
| **C-EVAL** |   51.5   | 65.1    | 73.9        |  55.1   | - | 39.9   |  59.5   |
| **CMMLU**   |   49.8   | 68.9    | 65.6        |  62.8   | 63.7 | -   |  61.6   |
| **MMLU**   |   50.6   | 57.8    | 71.9       |  68.3   | - | -   |  61.6   |


# å¿«é€Ÿå¼€å§‹ï¼ˆQuickstartï¼‰
æˆ‘ä»¬å°†æ¨¡å‹å‚æ•°ã€é…ç½®æ–‡ä»¶ã€tokenizerç­‰åœ¨huggingfaceå’Œmodelscopeä¸Šè¿›è¡Œäº†å¼€æºã€‚

We have open-sourced the model parameters, configuration files, tokenizer, and more on Huggingface and Modelscope.

## ä¾èµ–å®‰è£…ï¼ˆRequirementsï¼‰
- Python 3.8åŠä»¥ä¸Šç‰ˆæœ¬
- Pytorch 2.0åŠä»¥ä¸Šç‰ˆæœ¬
- CUDAå»ºè®®ä½¿ç”¨11.4ä»¥ä¸Šç‰ˆæœ¬ã€‚

Skywork-13B-Baseæ¨¡å‹ï¼ŒSkywork-13B-Chatæ¨¡å‹å’ŒSkywork-13B-Mathæ¨¡å‹è¿è¡Œä¸‹é¢çš„è„šæœ¬è¿›è¡ŒPythonä¾èµ–å®‰è£…ã€‚

- Python 3.8 and above
- Pytorch 2.0 and above 
- CUDA 11.4 and above are recommended.

Skywork-13B-Base model, Skywork-13B-Chat model, and Skywork-13B-Math model run the following script for Python dependency installation:

```shell
pip install -r requirements.txt 
```
## Huggingfaceæ¨¡å‹æµ‹è¯•ï¼ˆDemonstrationï¼‰


### Base æ¨¡å‹æ¨ç†ï¼ˆBase Model Inferenceï¼‰

```python

>>> from transformers import AutoModelForCausalLM, AutoTokenizer
>>> from transformers.generation import GenerationConfig
>>> import torch

>>> tokenizer = AutoTokenizer.from_pretrained("SkyworkAI/Skywork-13B-Base", trust_remote_code=True)
>>> model = AutoModelForCausalLM.from_pretrained("SkyworkAI/Skywork-13B-Base", device_map="auto", trust_remote_code=True).eval()

>>> inputs = tokenizer('é™•è¥¿çš„çœä¼šæ˜¯è¥¿å®‰', return_tensors='pt').to(model.device)
>>> response = model.generate(inputs.input_ids, max_length=128)
>>> print(tokenizer.decode(response.cpu()[0], skip_special_tokens=True))
é™•è¥¿çš„çœä¼šæ˜¯è¥¿å®‰ï¼Œè¥¿å®‰æ˜¯æˆ‘å›½è‘—åçš„å¤éƒ½ï¼Œåœ¨å†å²ä¸Šæœ‰åä¸‰ä¸ªæœä»£åœ¨æ­¤å»ºéƒ½ï¼Œæ‰€ä»¥è¥¿å®‰åˆè¢«ç§°ä¸ºâ€œåä¸‰æœå¤éƒ½â€ã€‚è¥¿å®‰æ˜¯æˆ‘å›½è‘—åçš„æ—…æ¸¸åŸå¸‚ï¼Œæ¯å¹´éƒ½æœ‰å¤§é‡çš„æ¸¸å®¢æ¥åˆ°è¥¿å®‰æ—…æ¸¸ï¼Œè¥¿å®‰çš„æ—…æ¸¸èµ„æºéå¸¸ä¸°å¯Œï¼Œæœ‰å¾ˆå¤šè‘—åçš„æ—…æ¸¸æ™¯ç‚¹ï¼Œæ¯”å¦‚ç§¦å§‹çš‡å…µé©¬ä¿‘ã€å¤§é›å¡”ã€åæ¸…æ± ã€å¤§å”èŠ™è“‰å›­ã€è¥¿å®‰åŸå¢™ã€å¤§æ˜å®«å›½å®¶é—å€å…¬å›­ã€è¥¿å®‰ç¢‘æ—åšç‰©é¦†ã€è¥¿å®‰é’Ÿæ¥¼ã€è¥¿å®‰é¼“æ¥¼ã€è¥¿å®‰åŠå¡åšç‰©é¦†ã€è¥¿å®‰å¤§å…´å–„å¯ºã€è¥¿å®‰å°é›å¡”


>>> inputs = tokenizer('é™•è¥¿çš„çœä¼šæ˜¯è¥¿å®‰ï¼Œç”˜è‚ƒçš„çœä¼šæ˜¯å…°å·ï¼Œæ²³å—çš„çœä¼šæ˜¯éƒ‘å·', return_tensors='pt').to(model.device)
>>> response = model.generate(inputs.input_ids, max_length=128)
>>> print(tokenizer.decode(response.cpu()[0], skip_special_tokens=True))
é™•è¥¿çš„çœä¼šæ˜¯è¥¿å®‰ï¼Œç”˜è‚ƒçš„çœä¼šæ˜¯å…°å·ï¼Œæ²³å—çš„çœä¼šæ˜¯éƒ‘å·ï¼Œæ¹–åŒ—çš„çœä¼šæ˜¯æ­¦æ±‰ï¼Œæ¹–å—çš„çœä¼šæ˜¯é•¿æ²™ï¼Œæ±Ÿè¥¿çš„çœä¼šæ˜¯å—æ˜Œï¼Œå®‰å¾½çš„çœä¼šæ˜¯åˆè‚¥ï¼Œæ±Ÿè‹çš„çœä¼šæ˜¯å—äº¬ï¼Œæµ™æ±Ÿçš„çœä¼šæ˜¯æ­å·ï¼Œç¦å»ºçš„çœä¼šæ˜¯ç¦å·ï¼Œå¹¿ä¸œçš„çœä¼šæ˜¯å¹¿å·ï¼Œå¹¿è¥¿çš„çœä¼šæ˜¯å—å®ï¼Œæµ·å—çš„çœä¼šæ˜¯æµ·å£ï¼Œå››å·çš„çœä¼šæ˜¯æˆéƒ½ï¼Œè´µå·çš„çœä¼šæ˜¯è´µé˜³ï¼Œäº‘å—çš„çœä¼šæ˜¯æ˜†æ˜ï¼Œè¥¿è—çš„çœä¼šæ˜¯æ‹‰è¨ï¼Œé’æµ·çš„çœä¼šæ˜¯è¥¿å®ï¼Œå®å¤çš„çœä¼šæ˜¯é“¶å·ï¼Œæ–°ç–†çš„çœä¼šæ˜¯ä¹Œé²æœ¨é½ã€‚


```

# æ¨¡å‹å¾®è°ƒï¼ˆFine-tuningï¼‰
## å…¨é‡å¾®è°ƒï¼ˆFull-parameter Fine-tuningï¼‰
ä½¿ç”¨Skywork-13B-Baseæ¨¡å‹è¿›è¡Œé¢„è®­ç»ƒå¾®è°ƒ
```bash
## preprocess continue pretraining data
## Because pre-training data is usually large, we use a script to process the training data separately.
python train/pt_data_preprocess.py \
    -t $MODEL_PATH \
    -i data/pt_train.jsonl \
    -o data_cache/pt_train_demo 

## launch training
export WANDB_API_KEY=YOUR_WANDB_KEY
export WANDB_ENTITY=skywork
export WANDB_PROJECT=skywork-13b-opensource

export MODEL_PATH=skywork-13b-models/skywork-13b-base
export DATA_CACHE_DIR=data_cache/pt_train_demo/pt_train
bash bash_scripts/skywork_13b_pt.sh
 
```
ä½¿ç”¨Skywork-13B-Baseæ¨¡å‹è¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFT, Supevise Fine-tuningï¼‰

```bash 
## preprocess data and launch training
export WANDB_API_KEY=YOUR_WANDB_KEY
export WANDB_ENTITY=skywork
export WANDB_PROJECT=skywork-13b-opensource

export SFT_DATA_DIR=data/sft_data
export DATA_CACHE_DIR=data_cache/sft_train_demo
bash bash_scripts/skywork_13b_sft.sh


```

## LoRAå¾®è°ƒï¼ˆPEFTï¼‰
ä½¿ç”¨Skywork-13B-Baseæ¨¡å‹ä»¥åŠLoRAè¿›è¡Œé¢„è®­ç»ƒå¾®è°ƒ
```bash 
## preprocess continue pretraining data
## Because pre-training data is usually large, we use a script to process the training data separately.
python train/pt_data_preprocess.py \
    -t $MODEL_PATH \
    -i data/pt_train.jsonl \
    -o data_cache/pt_train_demo 


export WANDB_API_KEY=YOUR_WANDB_KEY
export WANDB_ENTITY=skywork
export WANDB_PROJECT=skywork-13b-opensource

export MODEL_PATH=skywork-13b-models/skywork-13b-base
export DATA_CACHE_DIR=data_cache/pt_train_demo/pt_train
bash bash_scripts/skywork_13b_pt_lora.sh
 
```

ä½¿ç”¨Skywork-13B-Baseæ¨¡å‹ä»¥åŠLoRAè¿›è¡Œæœ‰ç›‘ç£å¾®è°ƒï¼ˆSFT, Supevise Fine-tuningï¼‰

```bash 


export WANDB_API_KEY=YOUR_WANDB_KEY
export WANDB_ENTITY=skywork
export WANDB_PROJECT=skywork-13b-opensource

export SFT_DATA_DIR=data/sft_data
export DATA_CACHE_DIR=data_cache/sft_train_demo
bash bash_scripts/skywork_13b_sft_lora.sh
 
```

# é‡åŒ–éƒ¨ç½²ï¼ˆQuantizationï¼‰

## 8bité‡åŒ–ï¼ˆInt8 Quantizationï¼‰

skywork é‡‡ç”¨ä¸»æµ8bitsé‡åŒ–æ–¹æ³•ï¼š[BitsAndBytes](https://github.com/TimDettmers/bitsandbytes)ã€‚è¯¥æ–¹æ³•é‡åŒ–åæ€§èƒ½åŸºæœ¬æ— æŸï¼Œä¸”å·²ç»é›†æˆåˆ°transformersåº“ä¸­ï¼ŒåŸºäºBitsAndBytesï¼Œæˆ‘ä»¬æä¾›åœ¨çº¿é‡åŒ–å’Œç¦»çº¿8bitsæ¨¡å‹ä¸¤ç§æ–¹å¼ã€‚

ä»¥ä¸‹æˆ‘ä»¬æä¾›ç¤ºä¾‹è¯´æ˜å¦‚ä½•ä½¿ç”¨int8é‡åŒ–æ¨¡å‹ï¼Œåœ¨å¼€å§‹ä½¿ç”¨ä¹‹å‰ï¼Œè¯·å…ˆå®‰è£…BitsAndBytesåº“å¹¶å®‰è£…æ‰€éœ€ä¾èµ–åŒ…ï¼Œå…·ä½“å®‰è£…æ–¹å¼è§[BitsAndBytes](https://github.com/TimDettmers/bitsandbytes)åº“ã€‚

### åœ¨çº¿é‡åŒ–ï¼ˆOnline Quantizationï¼‰

```python
model = AutoModelForCausalLM.from_pretrained("skywork-13B-Base", torch_dtype=torch.bfloat16,load_in_8bit=True, trust_remote_code=True).eval()
```

### ç¦»çº¿é‡åŒ–ï¼ˆOffline Quantizationï¼‰

```python
model = AutoModelForCausalLM.from_pretrained("skywork-13B-Base-8bits", device_map="auto", torch_dtype=torch.bfloat16,trust_remote_code=True).eval()
```



### é‡åŒ–æ•ˆæœï¼ˆEvaluationï¼‰

æˆ‘ä»¬å¯¹é‡åŒ–æ¨¡å‹åœ¨åŸºå‡†è¯„æµ‹æ•°æ®é›†ä¸Šåšäº†æµ‹è¯•ï¼Œç»“æœå¦‚ä¸‹æ‰€ç¤ºï¼š

| Precision | C-Eval | MMLU  | CMMLU |
| --------- | ------ | ----- | ----- | 
| bf16      | 59.5  | 61.6 | 61.6 |
| 8bits     | 58.5  | 61.8 | 61.0 |

### æ˜¾å­˜å ç”¨ï¼ˆGPU Mem in GBï¼‰

| Precision | Skywork-13B |
| --------- | ----------- |
| bf16      | 25.91       |
| 8bits     | 13.57       |



# å£°æ˜å’Œåè®®ï¼ˆDeclaration and License Aggrementï¼‰


## å£°æ˜ï¼ˆDeclarationï¼‰

æˆ‘ä»¬åœ¨æ­¤å£°æ˜ï¼Œä¸è¦åˆ©ç”¨Skyworkæ¨¡å‹è¿›è¡Œä»»ä½•å±å®³å›½å®¶ç¤¾ä¼šå®‰å…¨æˆ–è¿æ³•çš„æ´»åŠ¨ã€‚å¦å¤–ï¼Œæˆ‘ä»¬ä¹Ÿè¦æ±‚ä½¿ç”¨è€…ä¸è¦å°† Skywork æ¨¡å‹ç”¨äºæœªç»é€‚å½“å®‰å…¨å®¡æŸ¥å’Œå¤‡æ¡ˆçš„äº’è”ç½‘æœåŠ¡ã€‚æˆ‘ä»¬å¸Œæœ›æ‰€æœ‰çš„ä½¿ç”¨è€…éƒ½èƒ½éµå®ˆè¿™ä¸ªåŸåˆ™ï¼Œç¡®ä¿ç§‘æŠ€çš„å‘å±•èƒ½åœ¨è§„èŒƒå’Œåˆæ³•çš„ç¯å¢ƒä¸‹è¿›è¡Œã€‚

æˆ‘ä»¬å·²ç»å°½æˆ‘ä»¬æ‰€èƒ½ï¼Œæ¥ç¡®ä¿æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨çš„æ•°æ®çš„åˆè§„æ€§ã€‚ç„¶è€Œï¼Œå°½ç®¡æˆ‘ä»¬å·²ç»åšå‡ºäº†å·¨å¤§çš„åŠªåŠ›ï¼Œä½†ç”±äºæ¨¡å‹å’Œæ•°æ®çš„å¤æ‚æ€§ï¼Œä»æœ‰å¯èƒ½å­˜åœ¨ä¸€äº›æ— æ³•é¢„è§çš„é—®é¢˜ã€‚å› æ­¤ï¼Œå¦‚æœç”±äºä½¿ç”¨skyworkå¼€æºæ¨¡å‹è€Œå¯¼è‡´çš„ä»»ä½•é—®é¢˜ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºæ•°æ®å®‰å…¨é—®é¢˜ã€å…¬å…±èˆ†è®ºé£é™©ï¼Œæˆ–æ¨¡å‹è¢«è¯¯å¯¼ã€æ»¥ç”¨ã€ä¼ æ’­æˆ–ä¸å½“åˆ©ç”¨æ‰€å¸¦æ¥çš„ä»»ä½•é£é™©å’Œé—®é¢˜ï¼Œæˆ‘ä»¬å°†ä¸æ‰¿æ‹…ä»»ä½•è´£ä»»ã€‚

We hereby declare that the Skywork model should not be used for any activities that pose a threat to national or societal security or engage in unlawful actions. Additionally, we request users not to deploy the Skywork model for internet services without appropriate security reviews and records. We hope that all users will adhere to this principle to ensure that technological advancements occur in a regulated and lawful environment.

We have done our utmost to ensure the compliance of the data used during the model's training process. However, despite our extensive efforts, due to the complexity of the model and data, there may still be unpredictable risks and issues. Therefore, if any problems arise as a result of using the Skywork open-source model, including but not limited to data security issues, public opinion risks, or any risks and problems arising from the model being misled, abused, disseminated, or improperly utilized, we will not assume any responsibility.

## åè®®ï¼ˆLicense Aggrementï¼‰

ç¤¾åŒºä½¿ç”¨Skyworkæ¨¡å‹éœ€è¦éµå¾ª[ã€ŠSkywork æ¨¡å‹ç¤¾åŒºè®¸å¯åè®®ã€‹](https://github.com/SkyworkAI/Skywork/blob/main/Skywork%20æ¨¡å‹ç¤¾åŒºè®¸å¯åè®®.pdf)ã€‚Skyworkæ¨¡å‹æ”¯æŒå•†ä¸šç”¨é€”ï¼Œå¦‚æœæ‚¨è®¡åˆ’å°†Skyworkæ¨¡å‹æˆ–å…¶è¡ç”Ÿå“ç”¨äºå•†ä¸šç›®çš„ï¼Œæ— éœ€å†æ¬¡ç”³è¯·ï¼Œ ä½†è¯·æ‚¨ä»”ç»†é˜…è¯»[ã€ŠSkywork æ¨¡å‹ç¤¾åŒºè®¸å¯åè®®ã€‹](https://github.com/SkyworkAI/Skywork/blob/main/Skywork%20æ¨¡å‹ç¤¾åŒºè®¸å¯åè®®.pdf)å¹¶ä¸¥æ ¼éµå®ˆç›¸å…³æ¡æ¬¾ã€‚ 


The community usage of Skywork model requires [Skywork Community License](https://github.com/SkyworkAI/Skywork/blob/main/Skywork%20Community%20License.pdf). The Skywork model supports commercial use. If you plan to use the Skywork model or its derivatives for commercial purposes, you must abide by terms and conditions within [Skywork Community License](https://github.com/SkyworkAI/Skywork/blob/main/Skywork%20Community%20License.pdf).

  

[ã€ŠSkywork æ¨¡å‹ç¤¾åŒºè®¸å¯åè®®ã€‹ã€‹]:https://github.com/SkyworkAI/Skywork/blob/main/Skywork%20æ¨¡å‹ç¤¾åŒºè®¸å¯åè®®.pdf


[skywork-opensource@kunlun-inc.com]: mailto:skywork-opensource@kunlun-inc.com

# å¼•ç”¨å’Œè”ç³»æˆ‘ä»¬ï¼ˆContact Us and Citationï¼‰
å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œå¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œæ¬¢è¿å¼•ç”¨æˆ‘ä»¬çš„è®ºæ–‡~

If you find our work helpful, please feel free to cite our paper~
```
@article{skyworktechreport,
  title={},
  author={},
  journal={arXiv preprint arXiv:},
  year={2023}
}
```

```
@article{skyworkmath,
  title={},
  author={},
  journal={arXiv preprint arXiv:},
  year={2023}
}
```
